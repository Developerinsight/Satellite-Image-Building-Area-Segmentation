{"cells":[{"cell_type":"markdown","metadata":{"id":"D5y90GLVXafO"},"source":["## Import"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19739,"status":"ok","timestamp":1690357959050,"user":{"displayName":"조영민","userId":"10749653391039175177"},"user_tz":-540},"id":"7RHXehZ2dMvD","outputId":"c532f2e8-1bea-4f50-8325-141b66b99d22"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n","/content/drive/MyDrive/sw_contest\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd \"/content/drive/MyDrive/sw_contest\""]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5948,"status":"ok","timestamp":1690357964994,"user":{"displayName":"조영민","userId":"10749653391039175177"},"user_tz":-540},"id":"MpkJuYhFdOND","outputId":"c1de9af1-1f22-4278-a50c-8ea89d2527fe"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/729.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/729.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m419.8/729.2 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m727.0/729.2 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m729.2/729.2 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install torchmetrics --quiet"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9469,"status":"ok","timestamp":1690357974457,"user":{"displayName":"조영민","userId":"10749653391039175177"},"user_tz":-540},"id":"qvnD4ZZudPHs","outputId":"ab21b66a-64f0-4ac6-da6a-aba795c92620"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["\n","!pip install segmentation_models_pytorch --quiet"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11748,"status":"ok","timestamp":1690357986202,"user":{"displayName":"조영민","userId":"10749653391039175177"},"user_tz":-540},"id":"BJk9JjUedQJ7","outputId":"e1077e6f-ea01-49f2-a34e-8fcab0fb8c5e"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.7/70.7 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.7/65.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m596.7/596.7 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.8/69.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m722.8/722.8 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install lightning --quiet"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":14966,"status":"ok","timestamp":1690358001166,"user":{"displayName":"조영민","userId":"10749653391039175177"},"user_tz":-540},"id":"A1MO7tXvdSA9"},"outputs":[],"source":["import os\n","\n","import cv2\n","import pandas as pd\n","import numpy as np\n","\n","from sklearn.model_selection import train_test_split\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, Dataset\n","from torchmetrics import Dice\n","import segmentation_models_pytorch as smp\n","\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","\n","import lightning as L\n","from lightning.pytorch.utilities.types import TRAIN_DATALOADERS, EVAL_DATALOADERS\n","from lightning.pytorch.loggers import WandbLogger"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1690358001167,"user":{"displayName":"조영민","userId":"10749653391039175177"},"user_tz":-540},"id":"98fVt3W9W9Cg"},"outputs":[],"source":["import os\n","import cv2\n","import pandas as pd\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","\n","from tqdm import tqdm\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":5476,"status":"ok","timestamp":1690358006635,"user":{"displayName":"조영민","userId":"10749653391039175177"},"user_tz":-540},"id":"1J33A8-Jlw3i"},"outputs":[],"source":["!pip install -q -U segmentation-models-pytorch albumentations \u003e /dev/null"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gYR8dMIJYIV4"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/gdrive')\n","\n","\n","# folder1_path = '/content/gdrive/MyDrive/sw_contest'\n","# folder2_path = '/content/gdrive/MyDrive/sw_contest/train_img'\n","# # folder3_path = '/content/gdrive/Othercomputers/내 노트북/test_img'"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":315},"executionInfo":{"elapsed":12453,"status":"ok","timestamp":1690358019084,"user":{"displayName":"조영민","userId":"10749653391039175177"},"user_tz":-540},"id":"XS-Ucq5oZSOP","outputId":"437907ec-4f13-4e39-f649-7b8734f32a71"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.7.0.72)\n","Collecting opencv-python\n","  Downloading opencv_python-4.8.0.74-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy\u003e=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.22.4)\n","Installing collected packages: opencv-python\n","  Attempting uninstall: opencv-python\n","    Found existing installation: opencv-python 4.7.0.72\n","    Uninstalling opencv-python-4.7.0.72:\n","      Successfully uninstalled opencv-python-4.7.0.72\n","Successfully installed opencv-python-4.8.0.74\n"]},{"data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["cv2"]}}},"metadata":{},"output_type":"display_data"}],"source":["!pip install --upgrade opencv-python"]},{"cell_type":"markdown","metadata":{"id":"20x7C0trXd_p"},"source":["## Utils"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1690358019085,"user":{"displayName":"조영민","userId":"10749653391039175177"},"user_tz":-540},"id":"qZyppwFOXZxA"},"outputs":[],"source":["# RLE 디코딩 함수\n","def rle_decode(mask_rle, shape):\n","    s = mask_rle.split()\n","    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n","    starts -= 1\n","    ends = starts + lengths\n","    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n","    for lo, hi in zip(starts, ends):\n","        img[lo:hi] = 1\n","    return img.reshape(shape)\n","\n","# RLE 인코딩 함수\n","def rle_encode(mask):\n","    pixels = mask.flatten()\n","    pixels = np.concatenate([[0], pixels, [0]])\n","    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n","    runs[1::2] -= runs[::2]\n","    return ' '.join(str(x) for x in runs)"]},{"cell_type":"markdown","metadata":{"id":"UHjxwB11drEa"},"source":["#Validation augmentation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":110},"executionInfo":{"elapsed":3001,"status":"ok","timestamp":1690305153559,"user":{"displayName":"sehyun ban","userId":"01242021135040292581"},"user_tz":-540},"id":"ig9OOfaI70Ab","outputId":"73511195-11ec-45a3-9ff5-15a0712f5d56"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'9576 7 10590 17 11614 17 12638 17 13662 17 14686 17 15710 17 16734 17 17716 100 18740 100 19764 100 20788 100 21812 100 22836 100 23860 100 24884 100 25908 100 26932 100 27956 100 28980 100 30004 100 31028 100 32052 100 33076 100 34100 100 35124 100 36148 100 37172 100 38196 100 39220 100 40244 100 41268 100 42292 100 43317 99 44341 99 45365 99 46389 99 47413 99 48437 99 49461 99 50485 99 51509 99 52533 99 53557 99 54581 99 55605 99 56629 99 57653 99 58677 99 59701 99 60722 1 60725 99 61749 99 62773 99 63797 99 64821 99 65845 99 66481 45 66869 99 67505 55 67893 99 68529 55 68917 99 69553 55 69941 99 70577 55 70965 99 71601 55 71989 99 72625 55 73013 99 73649 55 74037 99 74673 55 75061 99 75697 55 76085 99 76721 55 77109 99 77745 55 78133 99 78769 55 79157 21 79793 55 80817 55 81251 1 81841 55 82865 55 83889 55 84913 55 85937 55 86961 55 87985 55 89009 55 90033 55 91057 55 92081 55 93105 55 94129 55 95153 55 96177 57 97201 57 98225 57 99249 57 100273 57 101297 57 102321 57 103345 57 104369 57 105393 57 106417 57 107440 58 108464 58 109488 58 110512 58 111536 58 112560 58 113584 58 114608 58 115632 58 116656 58 117680 58 118704 58 119728 58 120752 58 121776 58 122800 58 123824 58 124848 58 125872 58 126896 58 127920 58 128944 58 129968 58 130992 58 132016 58 133040 58 134064 58 135088 58 136112 58 137136 58 138194 24 140233 1 159671 52 160695 52 161719 52 162742 53 163766 53 164790 53 165814 53 166838 53 167862 53 168886 53 169910 53 170934 53 171958 53 172982 53 174006 53 175030 53 176054 53 177078 53 178102 53 179126 53 180150 53 181174 53 182198 53 183222 53 184246 53 185270 53 186294 53 187318 53 188342 53 189366 53 190390 53 191414 53 192438 53 193440 75 194464 75 195488 75 196512 75 197536 75 198560 75 199584 75 200608 75 201632 75 202656 75 203680 75 204704 75 205728 75 206751 76 207775 76 208799 76 209823 85 210847 98 211871 98 212895 98 213919 98 214943 98 215967 98 216991 98 218015 98 219039 98 220063 98 221087 98 222111 98 223135 98 224159 98 225183 98 226207 98 227231 98 228255 98 229279 98 230303 98 231327 98 232351 98 233375 98 234159 29 234399 98 235174 38 235423 98 236198 38 236447 98 237222 38 237471 98 238246 38 238495 98 239270 38 239519 98 240294 38 240543 98 241318 38 241567 98 242342 38 242591 98 243366 38 243615 98 244390 38 244639 98 245414 38 245663 98 246438 38 246687 98 247462 38 247711 98 248486 38 248735 98 249510 38 249759 98 250534 38 250783 98 251558 38 251807 98 252582 38 252831 98 253606 38 253855 98 254630 38 254879 98 255654 38 255903 98 256678 38 256927 98 257702 38 257951 98 258726 38 258975 98 259751 36 259999 98 261023 98 262047 98 263071 98 264095 98 265119 98 266143 98 267167 98 268191 98 269215 98 270239 98 271263 98 272287 98 273311 98 274335 98 275359 98 276383 98 277407 98 278431 98 279455 98 280479 98 281503 98 282527 98 283551 98 284575 98 285599 98 286623 98 287647 98 288671 98 289695 98 290814 3 358257 29 359281 29 360305 29 361329 29 362353 29 363377 29 364401 29 365425 29 366449 29 367473 29 368497 29 369521 29 370545 29 371569 29 372593 29 373617 29 727281 1 728303 3 729325 6 730347 8 731369 11 732391 13 733412 17 734434 19 735456 22 736478 24 737500 27 738521 30 739543 33 740565 35 741587 38 742609 40 743633 41 744658 38 745682 36 746706 34 747731 30 748755 28 749780 25 750804 23 751829 20 752853 18 753878 14 754902 12 755927 9 756951 7 757976 4 759000 2 855061 2 856083 4 857106 6 858128 8 859150 11 860172 13 861194 16 862216 18 863238 21 864262 21 865287 21 866311 21 867336 21 868360 21 869385 21 870409 22 871434 21 872458 22 873483 21 874507 22 875532 21 876556 22 877580 22 878604 23 879626 25 880650 26 881675 25 882700 25 883724 25 884749 25 885773 2 885777 21 886802 21 887826 21 888851 21 889875 21 890900 21 891924 22 892949 21 893973 22 894998 21 896022 22 897047 21 898072 21 899096 21 900121 20 901145 18 902170 15 903194 13 904219 10 905243 8 906268 5 907292 3 921888 2 922911 4 923934 6 924957 8 925980 10 927003 12 928026 15 929049 17 930073 18 931096 20 932119 22 933142 24 934167 24 935192 24 936217 24 937242 24 938266 25 939289 25 940312 25 941335 25 942358 25 943381 25 944404 25 944432 1 945427 25 946450 25 947474 25 948497 25 949522 23 950547 21 951572 19 952597 17 953622 15 954647 13 955672 11 956698 8 957723 6 958748 4 959773 2 961819 1'"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["train_df = pd.read_csv(\"./train.csv\")\n","train_df['mask_rle'][0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1200,"status":"ok","timestamp":1690305154751,"user":{"displayName":"sehyun ban","userId":"01242021135040292581"},"user_tz":-540},"id":"BtMlhZIodt0p","outputId":"cbd98464-e439-4c4f-a1aa-d56f5e2c8ab0"},"outputs":[{"name":"stdout","output_type":"stream","text":["4998\n"]}],"source":["train_df = pd.read_csv(\"./train.csv\")\n","train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=0, shuffle=True)\n","print(len(train_df))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1690209473975,"user":{"displayName":"sehyun ban","userId":"01242021135040292581"},"user_tz":-540},"id":"9mWvGD0XdtxB","outputId":"f3e4132a-2332-4af6-cbfe-223ce2698fb8"},"outputs":[{"name":"stdout","output_type":"stream","text":["4998\n","2142\n"]}],"source":["print(len(train_df))\n","print(len(val_df))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ngsV7CJxd7M4"},"outputs":[],"source":["def crop_img(img, img_size=256):\n","    img_list = []\n","\n","    y_cnt = 0\n","    while True:\n","        start_y = y_cnt * img_size\n","        end_y = (y_cnt + 1) * img_size\n","\n","        if end_y \u003e 1024:\n","            break\n","\n","        x_cnt = 0\n","        while True:\n","            start_x = x_cnt * img_size\n","            end_x = (x_cnt + 1) * img_size\n","\n","            if end_x \u003e 1024:\n","                break\n","\n","            temp_img = img[start_x:end_x, start_y:end_y, :]\n","            x_cnt += 1\n","            img_list.append(temp_img)\n","\n","        y_cnt += 1\n","\n","    return img_list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HUTrNKe5d7LF"},"outputs":[],"source":["new_data_list = []\n","for idx, row in val_df.iterrows():\n","    img_name = row[\"img_path\"].split(\"/\")[-1]\n","    img_path = os.path.join(\"./train_img\", img_name)\n","    mask_path = os.path.join(\"./train_mask\", img_name)\n","    img = cv2.imread(img_path)\n","    mask = cv2.imread(mask_path)\n","\n","    images = crop_img(img)\n","    masks = crop_img(mask)\n","\n","    for idx, (img, mask) in enumerate(zip(images, masks)):\n","        new_img_name = img_name[:-4] + \"_\" + str(idx).zfill(2) + \".png\"\n","\n","\n","        new_data_list.append({\"img_id\": new_img_name[:-4]})\n","\n","\n","        cv2.imwrite(os.path.join(\"./train_img3\", new_img_name), img)\n","        cv2.imwrite(os.path.join(\"./train_mask3\", new_img_name), mask)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":38920,"status":"ok","timestamp":1690310397156,"user":{"displayName":"sehyun ban","userId":"01242021135040292581"},"user_tz":-540},"id":"1ZwAo94X1SvY","outputId":"13a6dab8-69ed-4afe-a98b-2fe43a863cec"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'img_path': './train_img3/TRAIN_4972_00.png'}\n"]}],"source":["#./test_img/TEST_00008.png\n","new_data_path = []\n","\n","for filename in os.listdir('./train_img3'):\n","  new_data_path.append({'img_path': \"./train_img3/\" + filename})\n","\n","print(new_data_path[0])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lApTUjESkun_"},"outputs":[],"source":["new_val_df = pd.DataFrame(new_data_list)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X9YSKnw-11M_"},"outputs":[],"source":["new_val_df2 = pd.DataFrame(new_data_path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1690310403016,"user":{"displayName":"sehyun ban","userId":"01242021135040292581"},"user_tz":-540},"id":"fCqZRDtz2tVv","outputId":"1e63382f-983f-4c3f-d6cc-6507229308cf"},"outputs":[{"name":"stdout","output_type":"stream","text":["              img_id                        img_path\n","0      TRAIN_4972_00  ./train_img3/TRAIN_4972_00.png\n","1      TRAIN_4972_01  ./train_img3/TRAIN_4972_01.png\n","2      TRAIN_4972_02  ./train_img3/TRAIN_4972_02.png\n","3      TRAIN_4972_03  ./train_img3/TRAIN_4972_03.png\n","4      TRAIN_4972_04  ./train_img3/TRAIN_4972_04.png\n","...              ...                             ...\n","34267  TRAIN_0552_11  ./train_img3/TRAIN_0552_11.png\n","34268  TRAIN_0552_12  ./train_img3/TRAIN_0552_12.png\n","34269  TRAIN_0552_13  ./train_img3/TRAIN_0552_13.png\n","34270  TRAIN_0552_14  ./train_img3/TRAIN_0552_14.png\n","34271  TRAIN_0552_15  ./train_img3/TRAIN_0552_15.png\n","\n","[34272 rows x 2 columns]\n"]}],"source":["val = pd.concat([new_val_df, new_val_df2], axis=1)\n","print(val)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DMV9Vux72yVG"},"outputs":[],"source":["val.to_csv(\"./val.csv\", index=False)"]},{"cell_type":"markdown","metadata":{"id":"_hrh9_OrdaNL"},"source":["이미지 보기"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":1727,"status":"ok","timestamp":1690312420346,"user":{"displayName":"sehyun ban","userId":"01242021135040292581"},"user_tz":-540},"id":"b3Y8MfgKdbS7","outputId":"70f11831-3d9b-48cf-d2ff-0f231be7d779"},"outputs":[{"data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAABAAAAAQACAAAAABadnRfAAAyU0lEQVR4nO3d7ZKkKBCFYZzY+79l90d1VamFiAjkSXifiJ3p3pnp4itTRMUQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAhKzWBUBPi3UBoOUV/wyLWdDT2PlOABgaM6CXsbM/A2B4jI4exs7PEgAjZGj/rAsAKSwBToYEAEzsP+sCTE5+zU20WKiEBKDinQpMI44zgNmQANRIJALMggSg6i8RkAfQEouASCH/DI4EgC+WAKZDAgAmRgJAAmcAoyMBmLqccxOBaIqrAKb+4lvk3FukGOiIGYCChQM9bJAAkLAyKRgcpwBIWgMLESMjAeDt9GhPEhjMpqcnSQCbGjOSi5AEBrJ842GSBLCxKg5jTrVR7snzY/MtAirG/ylXhYVDcyQADrAZaKRaPLXkfKcAQGO35m3xbNFt6kcCQK7hz0c+sVhS00f/+Pyn3flxJctbc5wC4JqneSuqGXgGwIiGf62nXfPNAMgLhYY/A5jSwDOAM4KvvyQp4bbIoDn8r5wxPt8M4EUq/k91LCU5aE6zJgDAibapecJTABSRnjPZ76H+DdN12Qft00K1rRQJAO49OkbG/vGdVyT//ntXZ1OcAiAEZ4MW9ZAAMDfpU5v2OAVACCEsV3OAO3ekFv0rmCABCJCYfy9BpCDoiQQgYDkJvO7HT5LAdEgACt6RrhB8pklg5ayhp4UEoEUlEfwmgeZhaV3luXy785sASL7Wvo9zK3QEpwNT+CSANZADjOm1PUlgeMdTAHIA9jwlgWg5Gc8pkTWAQXKAl0HrQf54kNx0HefeCWAfLoPkAHTHsHHm9CoAb4JBme2xpODJuMMc4slLL3AteRmQiQCyxU+5Sk/E5jmBM46wvwSQei8kOQDtfe+GnCf2FWTcCOQ0B5zdXwsMoU5Q5t0J6DIHnJWYxOCR7eWFcS9uvBJARkyM0wTDVAQzqxSQPAuAjsY5jMR1rF6ljyIBAE84T2n/hZB3Vuy8nsCFOUc4MwBMbs7Af/sXQgjL3G0ABYxBE38zgOXqPIDuAQb03RZ8YR4AYdy+0cTuvQCkgC/GG2ZwWASU2Z/WFMFfgFuvXVK/CtD/zhHGMSZySAByo79v/B8fZQcGlzcDmCIW5JIf0NxxDUAsDtZOuUeq0rhtiiNUE9GWu/N69La6bEx2Fv2MqluykmiqTeu9nhTZoqcAx3mAafw3Phnn2I+Zna0BSJwLrO/fWqUA+yoiBwf/ZhKLgEJvhJh6Rb7XOoiieWvey7/0Hy/LYtcLm+yzWmQijey3GtVeAfHf3EUCCIb3Bx/eVTJpEKybX4HarhOAjAZBoH+EWQ+/A1Xp3gr8O+QnPBn+NoLTZZBXqe+mL591dUm2qeNjpnZxEyNToWU0LsZW8KmI50oMydEpQAj1VwLOxqPG5gg/b2wFKhM6BdhNchntv23g9CwgBA78unRmAOvusp9dOWTEFkH8G6EOI9FJACFrgj/PoSTWFgNEzzwd6INMf+zeA3860OsXV3ULJNVyYSxSM4AQXgN/gAPdU2dNQNOgKpUjSt7AblBazSOt+OVJDENkBmAW//7iiTkAKhJJAHJswyz56WQA1FOcANaq9+RkHYfdHaxLXbQsGQDVFCWAtW70h2AZ3A7TChkAtdwf/pvRVzd2roZ1322BTPMC90Ogk3sD6TAwK49Co9eTKl4HIAOgj/xnAdrPOxfmtm/X79ly/GAAhOQOo8Q8ea14ODof9w3Hu+A5AHMA9PHsacDqR+zTScB0o/1yDjBdi6AFoceBXyTOAxSCa1mTe7MrFBH+PTsFuPtTij+r7XDff6BWaDEjQkvZA6nrEn3nd5N9P04xrqItr1hQOKR5K/BxePcZ7hr7gP2IFUqyoHBIcwZw+LwOrweVjiidl7ViNJozgNB5jGse+j9sJkSYQf5Y6n+f3m6ToLkV7Q8uPq+BAtkZQPgclxnF+za4ceFm1vepIZtyAiD2v5bIVznIAEjSTgBhWUgCL8vh90t/oc8kACk3wsvoYT38WUMoXLOhb3CmTgJghPVw66mrYV4riKaqnAIwvrooPx3iPAAn7gwpHkzx47ev6CREiC8CokwkVzMLQAQJYB5kAPyokgAYWWJOOuQ1CaC38HUnAXAW6d5ac/82DIBTgAGljvEc/7FVJwEwqpSke4PFQGxU2ROQSaUrPCWIj+ePljCYxGQd4ek1hBCezgAYRoBr5QmA4NfEKT5uKEwARL9v9B9e7iWAZQ0MHmAcRPN4eK0gsjEURnSRAuh0vHEn4IguI5yVQrxwMBgU+zdNqOBJD7m3A6OOy9eLYyyF/S12NOAu1WrYv2keD96jpzUDWEkBwC0FR/5tjElF219dpMrkFROAPtb+7fn9yLJnc9bt35MaDp/6SJXKp7MEQNMOoryDd2EmMB4+Ca3rG8EH951MHQYKDdvC+Ty8SXsnp/23rgEvCmsAsVkUSwE1LOF4OYBWrar/pZbnn7j7Cat9Avgs/B2qZnByNabl7qBh30BVDfKNfQIIIcRrxiTggd1jW98UkP9qcdp+Dua3Aic3sORmlmLLLoLvLPm+Gn2l8Z266LfDH4vMAM5wHlDukAHux/Pw0wA2T7OfAVg/t7ZOc6xbCt4tOkfL+FEhHI49apwAzAfYq0nNi9FF3vDZt8Uk2dGFJj1hPQNI6zH9er0ui2F+irYRkb+Ce4PtGoD1CcCmCKw2vPi+HnMyoJoV390zlz/lFV8EbO7bIGSABO0U4C0KldieAizJYdV5zDHVDYlYYjFAQf2QsF4DSKeA5tbEdzj4SwG0krrzHvr9E+sEkNAhNRzbY/qj3NVNJGwLPxrhBGBi9gxwhTOBwWQkAKMeN5gAhMAk4Bot9GbQEM+iIlLg66sAa9unwww3rjj56IkvB2SOaO1rAv1ItkKqUJEbwjMvA474hOj5gvdwVcUcrgfuTwrIvw+gTQ5g5yohzO0d+24UmI6dwyZRlwlg3X1dOy4Nh1znXZyGQguZObn3cIl+efYzNqP/5lWAXovA7UcY8f+DCcA8PoP8/q3ANU8FFIfctPGfv2dAvyb6Fki0W5YQJIZx8brVVQKI1q35KrDlBGBmJduG1EOf9PNp67KHgSoFqN0KICcAJ3JSwFkTraf/eOpGrXxR6bsIUCVdXySAQZMy8X+ufFjRdlFNmuX1Q3crgncSzXeT2KIZgPsJAFIuUsA43RNfUZevX7SAa8ljdYvtfgBmZ5xMAC642+ei0Le7HVW4xhj9XghMJ4DWzRIdaD02Aj1B/D/jKIxqnpvbDZsHmfqv0CUzgIr1NZkEzHJ8K5dqoJLe372OUoVSWR4rroz5lmA/KaBDv5xlgKGGhCT2E2jgUYMmE0Cvu/52ryvt/5FdP9qD3e03/fNzBT5KWcHjihbMAOo3rsGcPPKR0wyafK8XyMufMdFz5VLPAvTr+O8ljG59Gfsg+ZHeyaFttheYnvcP0SrFfA3gT//jzPETF8bmR6RtSI5jSo353pfLuu860n3twZOf3shYwMvJEokfUfviA64lTgES3dHmoeDufWy8J7m2n8ap1lpMJoRI7QrcPyD7rz048tMmlfpHsa2nTUqFCUCxD4v8jelh6oMy0w6A8wQwS06ctuuB0hnAUEHD8j/mdXoZcJYJQCD8MTGpRUAAfRUlAA6Z6G2iGWlXZwmA9gYmoHIrMBDHdLMp4w1BmuH9lQ7RZf2dJIABzgC8pwD5d2JgBKd7vJ/9bR/vCX4V30FBQ0ay9VGNPfZddOK8O9zumbN9P4WHAg+ZAHZ2NXRfm7GcXwZcfh/98NF321IOcCozAB/jZk7JRUCv+0Cs2y8ZfSLoCEVXNwJ5fGR+TXwHYOP6MuB3GuAwF4TAJMAeOVhX1q3AkeUAYb/DTXsAempbjCb3RiA/ozQW7UwCgKhZngbUngQARtomAIOwO/vINvuYPidaLEyi5cNAFmPbXTxxbgJLzRLAKxLrvYIZfjEIdDVKAFYrcYkJAKMQ+NFmDWD9/arLOTjxD9zSJgEco219hb+7E3RgcD12BOoW90wAgHuaJ4BdULZdEyT+gZsa3QdAwAEeDHQnIBMA4K7OCaDhcgDxD9w20AzgFPEPnGiUAE4Px82mAFxiBO5rkQBWgwdvOAHYWje/AgnVo+Nq1DULx7MP1o//0y3Yn/xE/WpDQeVxknHQ6Z0BJowE31u4oaeqNwLZzjkX5rx77IOES3WHSFYENhyV0c+fLgq2rTBd5XHPWJcBo5uXqu4F1MXMdUeGugnA/ngTLcFcUcCLuJDPYAbQNhyZBAD5xjoFCCHED3oWB0KyDvRVTgASM06Z15iYpADOAHBD/xlAj/A8foRZHDALgLbOdwL2i0Tza2GfAvT9dCYAuKPHlmBvXYejzm1B3JcLXf1OAbqfmS8/X/S03wpNYVtE4Ff94FC6G2+1++yfZuhTjMPHMvVAWpcEYDcMV6tPj6XBDgX5/VhSAFLarwEYRv8SlulfTsYTQUhpMDpYhxaaAPT6ZDjVdAbAwNPALABnWoyMtdlPdsNoISR1DWDq/sCZVjOAuYeb0cW45McyDUBEkwQw50hLLzfat4l9CVBLvWzudVToLe5vrjhqLQF2LMHsus2y6p1l97wVuCrJGa1kof7olmwYa+cDU427XJwOC8Nb/E7YH37tSzC5fovfFe/39DkDeL/4gkENGX9RaXBy+mQm4DOEvhlQpfz2h1/7Esxp/Vn4ad/UFW/4crkl2Kb+7LgBU+vve9jMhmTJB7tMADsSKUDg8Jt+3JoJQBvr5r/d/2z9obV4HBiCj7wJJIAQQqIc5i00JpsXUtb8VP8zgBDkt8HoF31n0wDiv4nTcSc+IDccJoBY4xqfBwj1t8yWyBNIdLvQiEhzmADiJJYC4jqHZCQFkBRauHj0wuZz7/KXAHQDXQazgC7Srdx/nBb1ur8EcMZy0C9qIbcvkFjhhqGWAUr4SgCrZrOuIXXUNQo/tZw0ogEygLtRYnPhJU+8bIYlY2uW1gzeg1P3Uq+vGYB4Vo0edG1PTQw/fApCc4CyvnaWANRvdBFfDEBdlwEufbwKIbhLAA4cIs4+/uxLMKzLptVve18JQH0C8MJBdx791wAqTyp8JQAvvimAXDA0lXdhh1CaGVwlAB8TgBCC01mA8N2UmpTiv7D7fO4I5MHiYQloZ2WTpXu04r+Mj1K+OJoAuMRdA7fYPAJe+90vrk4BTjBkq3jvaeds3mJFZQuIhxwlAAZmU2vkK5TyEv+eEsApBmxlNGiGQXZg81PS5Kj0Uw1VFbean8X5iGzXetVf/zrCDIDh+txxYDEJuKbz/OdSfse3p9DhKkAzRm8z9K7/45/VP9HTDMDlzTV+MQm4FB2Q/Ufpk0/0FlMcqlpQ3mRBW+ct6utPOTzNAEJgFtDEALtbW/kZjt7Gp7fyhsCCdXUsrjywbzyT9wLOdArwsmsEn1UQYnE5ayDb5uvUYBWTjtMu3rSA0xroIP4fsnlXdaVPddvHn/q7rYEKTgCeerdg9war8PiW407m4bUqmAA8ZzkUHz7B7bqT1+C8AgISC/00bTa/I9HbZcCdZXHa6jpSF/q4CHiH05HoIQFwkDJCBsjl90jkIAGsDMRmLlp2xs1B1s8vd3iNfwcFd77WJ73L3uU4Fy57Q+uu4tI9+Jh+3f4GqX5BY9YQtIs+wr6Wte06ze/yXh75utncZVGJg+TFPisHuz5z0IEPqVfN4D7Lapzcq1R+I8CAk+P98cZJDz6hXjO/d/37SV3FDwOPNz2e77lo8ZodO0S8uF+9nxF7puwxc+fLs79mvN78Lyhf7v0pmW5R99bkt3KKdrYZ7j0CU94U9S8E3T6MFEu1qDsuCrkTCfb8Y5672kb567QqFuWZXP89F2uIllq90Pe3WfGzxpHjOvr91zFqEV7qdPmcqt9lpHvLFmNty5Rz9HdexRP/tq+E0poEnZdGq5w7wkW7sttu8e5od1xvuYHf1f5ZAC8tIVtMl0nrY4l8deJ3edZDBU/kZTvHFUz4d1yvlqlmsiA6xdxzPk3M3nM51vyiXZLDebc98fs0oEhsXZVCo5Q/zoeSaIEPls2v5+J1ERk5zQxZvdjjwD460kUht3wUeFkeHBB9VDFi3pOA+H4A9inAvACl/M8mr2uQWOlw22+zOtsQxLgnMz7dX6S5CI619AQg5w91yU0Ben3U+Y5Aq2EO8Bz/sgXL87j4ZIA6OoVfckswJnQlvK8DXrlcnh2jmjH93ky5hk7D5WJPQJu+9DwBGF5G57jMANcjqvuLaXuk0osEYBJn3uN/9CnANZeTgKsxZfHer/YNqbgr8MCvAPcYGHuZNfBf0YOug3LTeq1TQDoBWEWiUjIuIF68J7KHo8NJQKrbDLu0bUMqzgDCVb51HGD+wmLnTvGdV3Wr85y05+35yQRgGWiJz9aPf/0SIuKk2+xPSRumANEZQHAeRGeFtx9L/WhXNXqbS9HWaNV1fdLqv8SfGffg4nRHoATHRX8565PoXxWWH0/9q3HypFWjkujOAIJKQi4SK+UQh/8R6vAXYxlTAKUua3MekEgAAnVXav+nRqlLZjWEa/uJpMsMYFGJRJi3yADSM4AQ6QLhgbXzcywxKUULWZlMt7r5B1K9lN1gErDo77y5K6FMqa6NtW3ujt89dK/3MjXeI/eyaSsX69/tn9f9Dg/VsZRP71Dy0GWFVCt8Y/FPNP5rnwfsXoF4/IPTz+/+VMT7C9WBFfVXaldlzpYch6JVzrvwt8b/dx/d51b/wr1kd7p+2tRy+N0Tj2XOkBo0mlWOTl3jOUE4/mt7V/X4ySdNYHYoNpl4PJWxu45n+stHWyel1Spr/8WV789bT/7/1nr1FxoaPJpc8nOn1nlkKZXWYHH1exlw2U7qnmwM2cgi1VUIwc+dWk4eTbQo5e4+gKvFgPX0mx4EB9b0fPTJOLuY1m/vw41AS+rFEMenFKsXBgOQywlXh3+NcbwaFeTnTsDzWcBP+TRaDoZ+h4Ba/DuZ/ee1W4PGzf+Rt+4WwBRyn6i1kxX+GmW2ub0y+1mAs4cUgQ2NWPpwND6NipqbAM6K56iFUZv+CUBeeZwM4iaNm5kAEm+Dq1QQuKMf/44ygFUR8hJAqnQCjTcm9eUr8eINx3BHoHRXMxCasLou9IDeBCCvTC6e1mxUxqwfezUQPbSfN/KPEno4AQgh+Bm9F+VsVMwqOwJ5O1I5InseoFqu21Ti/6IgrYpZaZFEphVHYfnYVZ6fMSFazvTo1Sp0oqS2CcDxHlBOedgHLef5UQU+ngR8OStrs5JmngIIPh04tH1rip4H6IVP3Gk5BStwUqR2Jc3+ycwBOnK4wCZawBcvt7GvZzv0CSQAN4upI+g7Bp54l1SzdG/6zyzs9Fxeub0dYIUfhAsudq96c7FZm58VyxBC9gZ9Ndy4DJgshXaD+uLruQuPPa9+58+hfC1Le+dnG1ykmJK7dl6NinXjc/29paXT8sqNGYC7cbnjYl09hODxwQujd2isZS/5dTBYQ+hWzCqLgB6adLepsPR5q80tod68Wun+8PXUfHfrWKLGIqCHNt23pfKN9sT/tZLpsXTSP9P+LUUVFgEdNeq6f7GR4okA8X+pvNvctd7SvMy3fryX+yl+7Q8ZyitCJIC0Q/vcmcFO33Yx9xrF2Q0VX35uBx/4bov1efHd3CLpxn+3/vbi7IaKEEII6+Io/kOqsK49r9agDWPr5n4AcvGSwVuZE3epeKvKxvr5pfQHsC91C3c3BFmS34pyNAEIIQyx3Hq07n4r4bjyym7vCLQ7PvnoFH8HCR/tmu979PbXF4Mr2BJsiXyFDtw2d52oP6k+KeWRkj0Bl8Pv4pK31jJ82ltPv4G5siB2dVeVw40M6t1wUeHS20O1Lhx5W8g5tUqVuGxX4CVIVSLp8pCjd0yqGf/Wk5xKnz7OVG0NUkOucFvwJfOFAkI1PeejlCX2dz5bFCDy2SXFSf4bT90nN9ZK3wuQF/8CnZNTALXJTKw8T8po1gnVLt2r9VChT/grRMZLlReDnHh870etMlzQG12/JSo9Afj7wqYXfG1t1J7iXu8NE8Dzez/mVWXPqvXka4+S7eGjcqtkf7RLABL3fvicAIRwLNXzMlpMAk5LLTP8+zm2v0oTtDwFeFOpqy+PJwHHZlfqhsplUapanGwJmx3+VM53LltecwLw8mQfK4lHtytevE92pHInBumNNFrNANbEd11pNHOhZffbLbEm738a4Lr1a5G79LfVqIekNm7wfOwo3hXupNLdq1tvJ0mv3ai9l2abGUCl2z8qUX8NRNKSec/Vgf4VOJ2SNJU4/Eu0QJPYiNfMNAzPGttzbkhIja3OVa62DOBxszT9MreYAZzU2jThnbS1Qhc0oHTnbLUmTv0g0UneRVMrTAEaJIDTatlmAM0hYkFlUapaMUT79rqdBTqiftMJzT6PPG5pWuJ6XHWteNuTANU+zApu+8JXnwEozT6PRI8UtYnFf+LTno8H2S7Nq5r9FKDHnYAb1hXejxfVwdNa73o3+zzZ8Dcf6Nk6JwD7hpEdMvVchYVQE9wbDsctqYUqcpRbtBHjQX4Lric32DqhdtNMpWWAzY9R77zMyDavRosCyGeAvxLal6MhsS30Kt0O5yh3O3kQteONQI0/9J41rxT2e2oWE7v1qU4G8JS5fTyG1mQN4Kpm5ic+uZsarseLuQIlzzXkrU9LCIvyqf8dGtXo9jBQhw+t7+eAkzlxkCB2Q7ZYcTpQW4iJalUO4duB8n0qsez+h4/yywVcrEA+mrJQIgZ06t3qMmCihjqVz+bwPEAu/mOf7XAo3HBeO6F6tyuK2CJUgX0Nlu3/0K+EXgL4KZJ+Iz4l2AlHDQvjoPZJrtcxJBv/J6MObV08nPU0LY7EtnTFHFzMPKf53IyjG3nqkNoZK6rprcBqlb3FxZn+qW/Tfy+b2XfHEvlqKnqXMNs+C/BTXbn6lxOvyrZ4esNOrkCN7Oup1w/te8LtaZ/rE4CXNYRPOVVuYNiWyb/1uibqZz2tnwb0+vit7xOAlyV8m7xsa9H6tmXyb4CatK+Cp2tnXwNMANBYxvE/yI//9vsBLD9fODDCBABNrevdUSI5qDpsCLLsfhvDUJVBgexoFh8qPXYEWj6/eCGZq6Hj797wmxt/Kg6r/3p8yJJ5uuTGWLXBTYqBXIqhHMEKIM7tRof7dcDem4J6MFKCR21r4rub/1gBCeCXo1110VnRW5WUB4xy2SwNsaEJKit+um+98Xf76rII6FD8Uc6/P8KUns7fJQcOpwBnJLsLdp689Fr3DmitGYDKIyshhNNJgE4B0dGQh/+gVi65Z8X0N3RADy42+C0iVXjF977Mt5Edjka+L0RyDaDoWksjS/JbTEBoNNanNKDv32LVx7ZcQsVCF07e8VdKqeiHphYq2s8bQjCJvKO/42EhVHTpBTfdOznQzijP/CZIrgG8Ca0FLJtf0/YvEbrz3CjE5Pea3/7VSV0nbahUwOy7PpftN0JVwC03wtptH+sUfJBbb3fnCorXNZHtznHdaxfLlHuUp29264Xqe0IjbYIpgEy5Xb+J78vFK6GRaYIpgMoi4EVT+11k+RqhDpNxGtR3qCSAC156giCfltOuF0kATlvvFi9JDDMRSQAXvMTODHlsJhOsAWjsBzBD4DgdIBObIPxVEsAFL607Qx6bxhThL5IAZggcx2NkRpOEv481AC8NPEMem8WNMedleMZJJADfTZiH7OBL7pi8eomEOpXiD3AH3WWIe6lIQuYDUSO4+eJPryTWAEJyH36ImKuHluvq+g9/rTrEW1yphCmDj5f5NkWaYUYnsgbwx3GDDr5xnNLWLN1cvSHSc39+idVCeluwpIEnAPuqua3GfQMsTF1SWQP4M/BSgNMh89Mfq9OK3JWq50BNIFgV1d3B04Y8Y5z93WibHZ1G3RtebAYQgtdZwFWp/Q0aj73QxNDvhhGtjc/N9J5tayb1ZtRRdmh7aLPD66AbwwvOAEIYcBaQN25E7rPx2PatLInvRiBcI6kjYrbi3c1VJj0Z0W9dRBs+x+MV6SrlLDivS+Zf7CYWQTkVufW32yEBTEXpRqAfWWGzqmXmQe4QwRSkE0AmuVPWpSAHyFUiwVNZkeY8Afy9eU9rRK7hOA24lw9sZxDMX6biPAG8SaWAvxeJ3ookpfIPjqbe8p0ANn0plQJePinA0wQgi15bZxMcJ6Z8J4CdV9dK9a/L9UCPZc62BrEhYs11b7t4djDr5h6pO83HvRCocq+FkIFmACFIJve7swD7wXkzYbnB7D9C9FbgLLH+FLmZ9qZeI3PQ+9mzbNtY6sYxW54TQJzPFNBW7QTjr4U5+J/w15Uf4+zXcqhJxdIXbrOYDBdnjRtCiFTIYyWa8DsDSA1S37OAeqWvf9xz2a4c/s+57NAQwmWveqpY4eNDZT8454cXP9KoyPdm0625bYfB4z9UqUJpAoj9Q08tuvFwh4bh+T0FGJ7UU446JbmFyf8Vpx07wQTg5UFFniyStluW7Gq4xcz6mAFoK5gG1DjqZbwXS98IdWjO6Z2AA00ALt28gY1h/3LZbDRUCMwA7A38SiFDRHcmnzOAmSYAd9Wp/P6neAunvEmTt1o14TMBpLmKf5NROPjQH7x6VflMAC6fsy8lUFeBIuTLXzMhUXhNAMkU4Gu0Whcgbns3cskWpz6MWq87/C4C+nx50H0mo3T5+cKP3EuYDqvWgN8EcNbT9Gsdi+8nqq6MXLc7vJ4ChBBGWApQnsS4bd2ccnutW3WeZwAhdh4wWs+OVh8JNOrHAE2xTwHeKnQ1ByioT860wlsz3ZNugbHrfpPzGUAIh1mAu85d0sPVXX3k0aI7rtcA3hbPi9aOT7ZlJa4R09Z7QySAm4Gvt/LGsKzsrD1p56NxWmQNmbXJ/oudxTcGK9jBmjWAkzYYvdIlBpkBhJtHUcV3RMQrwKAtEmk2plkx8zXKJ/Qlq17jkgYzgPDbCMNXuNA4M4DbFGcBVQ5TjPVwbASO/memSwDrydcyli7XNCSr3gzhf2q6pvGx3eWrlKWF48aCEA7nerwN8MRs7eLijeIhPLtW0exFI67M/CbUfLM1zxSx4fvm6GpUr/dKmWwNIDo5llwNrGbeEFhmrnyuAZ4FqGDsR9+nRadee88Ahj4IfpzXcuxZAHDinSSlXkTXyjS7ie8qOk61UN9mDWD6g+D0DYD5/CWA19AfPAKuazd4AwBHh0XA2VfDhqu9/wrNPiQbeyWAzYFv3Pae6ug+SCdO1WcmuAy4NUjYDFIPor+DJYRJ7hzLGE6D1tylbXfRL+38zgBobVjj2N/Nf2GS5mYC4MYU41HFzwyAKIClWPTzLG87/5Fw3xhl5hiK3R3fSzFwFKwhJN/DMXDVXWAbEwvHl+sN3dLvexzYM1rONI9pqDmsAYzd0Mvnd+aaQAgh/JszFH53iR079cm7aP45R2kPk+0I9EXAAz8J4O9xuOET7nqcBJAPMKXFzTa5LXwrP1GlRbEMaCLyMNAUmwO9fFYDZ6kwsBdfA5hnY4zMd0bN0yCYS+QU4P0nPYth6noLhPc9RGiIcwAL5/sBjLszyFFmPedpEMwjdRmQee+fdzvQIC2RXi2k7wNgxB/QIGZo+SaubgRixB+GHg2CkVzeCcjE7IgU0Ai3AxuY9lbgfL8DjxSAUZAAipACMIarBMAZwEmokwHq4xygP2YApZgEZKOldF28GIQJQGLwcmdQDqJf2hKSXTT9CL8avtM30IVX+2W3EkOxt8QmeYFGzzl+0Uan7j9tzZMpvb0bNt7y0zd71vx1+laKK3q5F/u1dvZt20jT0/CZJ7A01NGx4cqnALRtU99FQLbK/UX8l6k2kmjZ1rZXAaZ6RwBaeRT92/0pGH8dHBuZtzLvsAZw19P15M+/p1W7ON4HsNADW5wX3VSpuRh7vcRamo0yd7gV4I7HF/K4vaqreGOvp38ypXQKoKG2uJLvS/xZgCVns9x5JsfJ1mBgw7HTh4HIAFuZu4fjfNxMNFo8KX8acJ2rT89SAJkBnhUngDWEyR6JJdSzMAVwpTQBrIffZxCbBJAV4NrjDUFmygAsBWA0hSN6F/ZzRcXEVc/ElUBHymYAa+K70W1nATOP6KnWf8ZVNIR/un62QHg3wGz1/khv9MMUwI+LPQEzzXb75jL3+4I59o+j5BQg1v+zjYmJlwOZ+4+kYByf9P+8ETGTvB0jOAdw4+YMIHH7HweG8dHHw7mfktm5eVKxjmcK4N3tNYDUMYDjw7g4+I/p7lWA9DCY7WrALAj+Yd0N2MuhQAYYTqrP750DMDjk3O4SMsBcLvr7RgJgYCi61yvrwsuyinncZ628s0vfDIK+SvqFSUARj3utVkoAruo8lzrPAlT5qaNz+cqL4q52WdsJlXUOpwEFDo3mo4UeTwF8VHNeZQ8D8b6M+44tts4QHOPX0LviHmKv/JucPkLxYAqgXjWEJ+Ov5OrwzJzeQs3Z3tie9B43fN9QeD3dHiu+Q3uyKSg9X8+6ur3Z3mu5EcLDXYF5V0ZNNBv6e7YteHRfHAbyUOjOoT3dE3CZfoPQPMPOk+lu3x6/GIQBMLrkq5HpfuceJ4DjGGBEFOrZcJXmI4S/fzW2BV+479uNYU9FUKZOwH6GFfEflxF3rZsubz/fjH9a+lOgqM6LQXg24LGWsUTn4EydBPA+DeCIIKZO6P9e6sEoaiWAsEzxdFshgwDq8pE87+NetQRA9MvgeI1szy8Dogbt9KldOjxAAphTpcWBKj8FhuqdAuSb7sxxlDn5ErZ1ma0Xx2QwA1iHCYhc/kNlCWHZ3/fHXYBj6N+Ns14vfGe9/WF08z+rf1LSo73gJuy+UXXvym0czGYXOw03zs/JAA8SwIxdNyyzBMAwCs0ao2kCwFB6jwOeGzpqMKVuew6AkXS+CrDuvmYMhhCyXrcItNH3KsCDJ9JGRkPAiuGNQAx7wFrXBMAbY/ugaZGrZwLgVBcQ03ERkPiXwRQBfyyeBXiZ7okADbQ6tvqNh8gEgMHYiNM3EaO/bmsAsTHJSUEjRDoy9UoA8Vh3+0JMYAx2awAvrAQ09Ne2ZFmc6hR/iTFIBmiN569wqs8pQOoYxGlAN7Q0jhT2BGRcAkb6JICL/aPIAICNXjOAiwxACgAsdDsFuFiAIgM0tyxsPYAjmbfSs0IN9NdzETAV48Q/YKDrVQCiHNBiuCnoFqkBsND7PoB4pBP/gInuNwLFYp34B2z0vxOQl8oBMixuBT5mADICYMQo+LZrgcQ/YMXoYaAl+iWAvqyeBiTsAQGGgbhaFwCYnvWrwYh/YFY8BQwAAAAAAAAAAAAAAAAAAAAAAACMxPEDQ6+i88gTdKk/jrsG/TImua8AhiY+Nr1vGvJ38PdbAQzOcEOQa+/J82EO7WVKvW4m/17KjMkoH5u+QbMp5apc4q1dyHspNGajOzLjAbQqF3njcMR3UWZMSHZkRiPIy5LAz4Rfv8iYk+oaQPScOb4kICdy3U+9yJjVf6vi0SkSQYuXIHJSTCCEEBbF69SXMaRW4K2TwisXGfP6F4LaQSvjzjmtAu8R6XDktQYgFVDXEeTxDeNSTQz8UVwEvApv8fAXLx6w8UoAnsasx8N/CEwBIElxBpDMRw7C30ERgZd/IQiO2PMCyRU1H1MA6PkXJINKsEg3+C49ZiJ5CpDg+TDquewY1D/R45VkoYDRyM4AzjKAi8PoSeFdlB1T+efuWEsUAdX8k41/2YLlcF14TOTqFMBwS1vPJwEnZfRQdEzlIgGsioNWr0RH+iUEQghXCWD9/GLB6zw6MWsiM0BLMsjWnL/UUjRexPNCOsbFC4/ZpGYA688XvcXCxXcIMQWAlEQCWKNfGtN/FlC+gMDX+XDdB73VsHa4vfZFtpStAm8xmtFpdx+HsUIGcDI2na4CaGR89HV2CvAziAXOAsYYkwINCbzduGvdKP7ML0Xc9/f6Il/7Azs82cJz8RlA6rUcvS2733xILlUyBYCO6Eg9HaImUaj45oI8rm5jYAYwpdgMQOxGtmWk0ah/GRNTiYzHZJQzfu/wdFT1VFZU8zsDSB/lOYEttiQWBgEbP4ne9Xv55HxbU77ZuA1gSscZgO/38sni1B+aCvYEJAPke8W9h/CnV+e0SwBr3tSPsXKLh/DHrHYJYAlkgNoIfyi7exnw/J/BpzWEEBauAk4qsgZA38/HcO9XmIotAi4hXExdyRHAEKJXAZaL+2+Jf2AMWZuC/v6blRwwiPO5P108g+R9APEhkHutAIC69I1A4+3KC2Dj4k5Aoh0Y2dWtwD8XA0gJwDiunwVYEt9hXPT0FDIeBlpOvgbgXc7TgEvkKwADyHoceDn8jmFwC/DkMhLA5yFh4h8YTOYaADf/ACPK3RFoEY5/HmUDCmVvCSYb/+vKmSxQ6D/rAjxE6Dcim/BRVcGmoEI+k3/yAFDCdQIg7IFnHCeA3dofuQAo4DYBHJf+yQAlaLXZOV0EZOACNdSbAawdL8fHPomcANxWbQbQMf4IdaCSWjOA9fNLeyeXqMkLwF11EsB79k8MjoL7gCZRJQGska9aYgoA1FEjAXA9HnCqQgIwiHmmAHXQYNN7nABsbsghAwA1PL0M+BtyvDXMj3dXkTin9TBaoyOnSwY4f28hyn1alXacxKOOPjtw9Bg9JADguSdrAKYTR1YBgOceJIDzWCMKAR/KE0AqyntkAKYAwGOl58xXccYyAOBA4Qzg8jjLgRhwoCwBaIQ3JwHAQyUJIGvnD8IQ0NduT8AOGYApAPBMSQLIXGUjDgF19xOAUlwzBQAeuZ8AFqULbWQA4ImypwGXvwhbQiLYdNIEgLjCRcDl++tJnC994p8sAzxQehVgCZ/gi8agaWB2Sj6Aew8uA76iLH4G0C8CI59E+AOZqgTLTxLoGYLHDyf8gWx1wmUfhJ1DcPfhhD9wQ6WA2QZh9xj8fjjhD9xS6VbgJfplb8Q/cE+tmHkfhU1icLX7aMC1Wg8DpS4JdsGlP+C+emGzVv1pNz+b6AeMZW0TAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAsMeOBcBOrS3BriiE3rpKFAPQUfZy0NsEAk+gCICaLpvprd0+KV2EEKyLAWjpEA+mO4bvi2BbCkBO+3gwP/Ye5v5kAOCjdTiYvjPsWAC7UgCa2oaD+cE3tvJHBgDeml4FsF54t/58QF3Dw2Ek/LoefE/DnykA8KfdjUDWh1/rzwccaHU0PAm/ngff8wzAFAB4aTMDOH1NIIdlQEmTBKAR5hzngSsNEkDyLcESuUGiEICA6pcBCS7Aj9ozgMv475ggzs8B1p8vgCn12g/gSyjmkicrwARqJwCplbf0FGBdg1Q6AvrrtCHI1qqQJAh8IDQ4BciI7n7BJ1UYQE/9NQCF4zuALP0XAQNTAEBFgwTwF3TLchp9TBIADa1mAOfRn/qjBuW4/itMATCvFglgecddNPo4/AMyGoejwI58Vwd4EhIm1vc+AL1g0ysR0FHPXYGNoi0xBSD8MbnmIfANP5H3AnwR/phev/sAzMLt5IO7XowANPV6M5BltPFyAOBEnwRgHG4C1yIASe1PARa5eGPyD/yZIxZ4OzAQZbAfgCXCH9iaKgEQ/sDeJDEhsBIJCJpmBkD4AwAAAAAAAAAAYB5cHUPEysiYhMl7ASCOtyZOY5obgZCN0J8IMwAcrIffMTLO9LCzDXsGx/gkZgAca2Ssp99gSAJrAAwzGXTFdMxnefZ7huLtN/7pl9GJbNfJQLMXPfzTMYMzXQNYP2OOuac5umBKhhle4K1heOP1SZMy69/jiFu4/9QQ70+cldFVgN8BxwzUkHXjk/rNmLQ7xxspF+Hf6QXS9LwJgxmA9eEGe1f9sTYMTcaCte4J4KLLW442RBiGINEvoHO8Xfc5CaCrrCBs0SfcdKRBLgEwDjrKPQbX7hPuOZIh8TAQjBBz0+ucADJGHGeGHWVmgNp9QuKRwQwAIsj8FnonAKYAWvKOxRyxh8UMYHIZsb3Uj//Tn7iu5P+u+ud2LgRoueyPJr3BzaAiFGcAjICeLlq7weH/AlOAnvongKsB1X/ETS7Z3q06g04WIbAn4A4DQwm9MTyDU4DEqOLob+G00Y16g3OAjpTWAAh/IyftTndMwCIBxAcW4a+lcX/Q2xpUZgCEv6VI49Mfc7Dp58iGgDB16JEOHZI602c8dKMwA+Dob29JfNfjE2HE5jLgskn/DAQ19MhErGcAHP1FLJGvzHAhsBur3uaVgHK6dwnPAwiwvBOQXtZDn0zG7s1ADDU1a+fRwHUAATQ0rJAABFgvAmJeBLkAEgAwMRIAFHEhsBMSAMxwDmBPbUMQgMzQETMAyCH++6Gtr6yBVmomfqpPc/fDKUACK1EYHQngxC74uW0RgyIB/OLAb4x82w8JYIfY72uhwY2RAN4YipgQCeBl7PB/187L1JpzgG5o6ZerBOC7nYQ3YeVCoC1mADPyNiNAMwyBPxdTAN/tlKycbdVOiua7vR3hVuBcY68SmCHSbXEKkIuR2g9t3Q0JIM+469JqFVMrz+BIABBC9PfGGsCfkYee9PLF5o0kvCWmP2YAUEDoGyEBZBp3EcAeTWuHBABjhL8l1gCAiZEA3jgQYUIkgMmR9+ZGAsglfS0NKEMCeBs3wMetGR7jKkAgQjCv6RMAwY+ZTZ0ACH7MjjWAbKQLjIcEAEyMBDA3bgOY3NRrAPmvpSBOMKapE0AW97HP2gXOkQAS3Mc+cIEEcILgxwxIAL+IfUxj7gTwswpI7GMucyeALWIfEyIBhJljf96a42X6BEAIYGaT3wn4txP9yJfKR64bHps8AfxhGoBJMfRnEZ0J0P2zYwTMZpcI6P7Z/Q/KLkVFhdsjfgAAAABJRU5ErkJggg==\n","text/plain":["\u003cPIL.Image.Image image mode=L size=1024x1024 at 0x7FC88F533070\u003e"]},"metadata":{},"output_type":"display_data"}],"source":["import cv2\n","import numpy as np\n","from google.colab.patches import cv2_imshow\n","\n","# Load the image\n","df = pd.read_csv(\"./train.csv\")\n","\n","image_path = df.iloc[1,1] # Replace with the path to the image file\n","image = cv2.imread(image_path)\n","\n","# Example mask_rle (Replace with the actual mask_rle for TRAIN_0000)\n","mask_rle = df.iloc[1,2]\n","\n","# Function to decode mask_rle into binary mask image\n","def rle_decode(mask_rle, shape):\n","    s = mask_rle.split()\n","    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n","    starts -= 1\n","    ends = starts + lengths\n","    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n","    for lo, hi in zip(starts, ends):\n","        img[lo:hi] = 1\n","    return img.reshape(shape)\n","\n","# Convert mask_rle to binary mask image\n","binary_mask_image = rle_decode(mask_rle, shape=(1024, 1024))\n","\n","# Convert binary_mask_image to grayscale\n","gray_mask_image = binary_mask_image * 255\n","\n","# Find contours in the binary mask image\n","contours, _ = cv2.findContours(binary_mask_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","\n","# Draw each contour as a line on the original image\n","for contour in contours:\n","    cv2.drawContours(image, [contour], -1, (0, 0, 255), 1)  # Red color for the contours\n","\n","# Display the grayscale mask image using cv2_imshow\n","cv2_imshow(gray_mask_image)\n"]},{"cell_type":"markdown","metadata":{"id":"qThRcQtbXhpr"},"source":["## Custom Dataset"]},{"cell_type":"markdown","metadata":{"id":"gQtTwrNKxeZu"},"source":["origin"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":277,"status":"ok","timestamp":1690358040104,"user":{"displayName":"조영민","userId":"10749653391039175177"},"user_tz":-540},"id":"WDMUvbgdxfXa"},"outputs":[],"source":["class SatelliteDataset(Dataset):\n","    def __init__(self, csv_file, transform=None, train=False, val=False, infer=False):\n","        self.data = pd.read_csv(csv_file)\n","        self.transform = transform\n","        self.train = train\n","        self.val = val\n","        self.infer = infer\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        img_name = self.data.iloc[idx, 0]\n","\n","        if self.train:\n","            image = cv2.imread(os.path.join(\"./train_img\", img_name + \".png\"))\n","            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","            mask = cv2.imread(os.path.join(\"./train_mask\", img_name + \".png\"), cv2.IMREAD_GRAYSCALE)\n","\n","            if self.transform:\n","                augmented = self.transform(image=image, mask=mask)\n","                image = augmented['image']\n","                mask = augmented['mask']\n","\n","\n","            return image, mask\n","\n","        elif self.val:\n","            image = cv2.imread(os.path.join(\"./train_img3\", img_name + \".png\"))\n","            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","            mask = cv2.imread(os.path.join(\"./train_mask3\", img_name + \".png\"), cv2.IMREAD_GRAYSCALE)\n","\n","            if self.transform:\n","                augmented = self.transform(image=image, mask=mask)\n","                image = augmented['image']\n","                mask = augmented['mask']\n","\n","            return image, mask\n","\n","        elif self.infer:\n","            image = cv2.imread(os.path.join(\"./test_img\", img_name + \".png\"))\n","            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","            if self.transform:\n","                image = self.transform(image=image)['image']\n","            return image\n","\n"]},{"cell_type":"markdown","metadata":{"id":"irjkSRUAxcRt"},"source":["one-hot"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":494,"status":"ok","timestamp":1690358048821,"user":{"displayName":"조영민","userId":"10749653391039175177"},"user_tz":-540},"id":"1-tXvlsMXaLV"},"outputs":[],"source":["import torch\n","\n","def one_hot_encode(label, num_classes):\n","    \"\"\"\n","    Convert a segmentation image label array to one-hot format\n","    by replacing each pixel value with a one-hot vector.\n","    # Arguments\n","        label: The 2D array segmentation image label\n","        num_classes: Number of classes in the segmentation task\n","\n","    # Returns\n","        A 3D tensor with one-hot encoding where each pixel value\n","        is represented as a one-hot vector of length num_classes.\n","    \"\"\"\n","    h, w = label.shape\n","    one_hot = torch.zeros((num_classes, h, w), dtype=torch.uint8)\n","    for class_idx in range(num_classes):\n","        one_hot[class_idx] = (label == class_idx).to(torch.uint8)\n","\n","    return one_hot\n","\n","\n","class SatelliteDataset(Dataset):\n","    def __init__(self, csv_file, transform=None, train=False, val=False, infer=False):\n","        self.data = pd.read_csv(csv_file)\n","        self.transform = transform\n","        self.train = train\n","        self.val = val\n","        self.infer = infer\n","        self.num_classes = 2\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        img_name = self.data.iloc[idx, 0]\n","\n","        if self.train:\n","            image = cv2.imread(os.path.join(\"./train_img\", img_name + \".png\"))\n","            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","            mask = cv2.imread(os.path.join(\"./train_mask\", img_name + \".png\"), cv2.IMREAD_GRAYSCALE)\n","\n","            if self.transform:\n","                augmented = self.transform(image=image, mask=mask)\n","                image = augmented['image']\n","                mask = augmented['mask']\n","\n","                mask = one_hot_encode(mask, self.num_classes)\n","\n","            return image, mask\n","\n","        elif self.val:\n","            image = cv2.imread(os.path.join(\"./train_img3\", img_name + \".png\"))\n","            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","            mask = cv2.imread(os.path.join(\"./train_mask3\", img_name + \".png\"), cv2.IMREAD_GRAYSCALE)\n","\n","            if self.transform:\n","                augmented = self.transform(image=image, mask=mask)\n","                image = augmented['image']\n","                mask = augmented['mask']\n","\n","                mask = one_hot_encode(mask, self.num_classes)\n","\n","            return image, mask\n","\n","        elif self.infer:\n","            image = cv2.imread(os.path.join(\"./test_img\", img_name + \".png\"))\n","            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","            if self.transform:\n","                image = self.transform(image=image)['image']\n","\n","            return image\n","\n"]},{"cell_type":"markdown","metadata":{"id":"IG8nK8pZrqoU"},"source":["Gray scale"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pg2jfTz4qBbP"},"outputs":[],"source":["class SatelliteDataset(Dataset):\n","    def __init__(self, csv_file, transform=None, train=False, val=False, infer=False):\n","        self.data = pd.read_csv(csv_file)\n","        self.transform = transform\n","        self.train = train\n","        self.val = val\n","        self.infer = infer\n","        self.num_classes = 2\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        img_name = self.data.iloc[idx, 0]\n","\n","        if self.train:\n","            image = cv2.imread(os.path.join(\"./train_img\", img_name + \".png\"), cv2.IMREAD_GRAYSCALE)\n","            image = np.expand_dims(image, axis=-1)\n","\n","            mask = cv2.imread(os.path.join(\"./train_mask\", img_name + \".png\"), cv2.IMREAD_GRAYSCALE)\n","            mask = np.expand_dims(mask, axis=-1)\n","\n","            if self.transform:\n","                augmented = self.transform(image=image, mask=mask)\n","                image = augmented['image']\n","                mask = augmented['mask']\n","\n","            return image, mask\n","\n","        elif self.val:\n","            image = cv2.imread(os.path.join(\"./train_img3\", img_name + \".png\"))\n","            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","            mask = cv2.imread(os.path.join(\"./train_mask3\", img_name + \".png\"), cv2.IMREAD_GRAYSCALE)\n","\n","            if self.transform:\n","                augmented = self.transform(image=image, mask=mask)\n","                image = augmented['image']\n","                mask = augmented['mask']\n","\n","            return image, mask\n","\n","        elif self.infer:\n","            image = cv2.imread(os.path.join(\"./test_img\", img_name + \".png\"))\n","            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","            if self.transform:\n","                image = self.transform(image=image)['image']\n","\n","            return image\n"]},{"cell_type":"markdown","metadata":{"id":"R7L-fVpTXnk8"},"source":["## Data Loader"]},{"cell_type":"markdown","metadata":{"id":"co7X7zYCMlfM"},"source":["one-hot encoding 추가"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":295,"status":"ok","timestamp":1690358051933,"user":{"displayName":"조영민","userId":"10749653391039175177"},"user_tz":-540},"id":"0lZveeJ39y__","outputId":"5c3cf733-371d-432a-f6ac-348bc8d5df0e"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/albumentations/augmentations/transforms.py:1613: FutureWarning: This class has been deprecated. Please use RandomBrightnessContrast\n","  self.max_value = max_value\n","/usr/local/lib/python3.10/dist-packages/albumentations/augmentations/dropout/cutout.py:50: FutureWarning: Cutout has been deprecated. Please use CoarseDropout\n","  f\"{self.__class__.__name__} has been deprecated. Please use CoarseDropout\",\n"]}],"source":["def train_transform():\n","        return A.Compose(\n","            [\n","                A.OneOf(\n","                    [\n","                        A.RandomBrightness(p=1),\n","                        A.RandomBrightnessContrast(p=1),\n","                        A.Emboss(p=1),\n","                        A.RandomShadow(p=1),\n","                        A.NoOp(),\n","                    ],\n","                    p=1,\n","                ),\n","\n","                A.OneOf(\n","                    [\n","                        A.NoOp(),\n","                        A.HorizontalFlip(p=0.5),\n","                        A.ShiftScaleRotate(p=0.5),\n","                        A.Rotate(limit=90, p=1, border_mode=cv2.BORDER_REPLICATE),\n","                        A.RandomRotate90(p=1)\n","                    ],\n","                    p=1,\n","                ),\n","                A.OneOf(\n","                    [\n","                        A.Cutout(num_holes=8, max_h_size=32, max_w_size=32, fill_value=0, p=0.5),\n","                        A.NoOp(),\n","                    ],\n","                    p=1,\n","                ),\n","\n","                A.RandomCrop(224, 224),\n","                A.Normalize(),\n","                ToTensorV2(transpose_mask=False)\n","            ]\n","        )\n","\n","def val_transform():\n","    return A.Compose(\n","        [\n","            A.Resize(224, 224),\n","            A.Normalize(),\n","            ToTensorV2(transpose_mask=True)\n","        ]\n","    )\n","\n","def test_transform():\n","        return A.Compose(\n","            [\n","                A.Normalize(),\n","                ToTensorV2(transpose_mask=True)\n","            ]\n","        )\n","\n","train_transform = train_transform()\n","val_transform = val_transform()\n","test_transform = test_transform()\n","\n"]},{"cell_type":"markdown","metadata":{"id":"3MCnYsZ6Xrj9"},"source":["## Define Model"]},{"cell_type":"markdown","metadata":{"id":"ieaEZlCGXwhS"},"source":["## Model Train"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":288},"executionInfo":{"elapsed":3583,"status":"error","timestamp":1689964368030,"user":{"displayName":"sehyun ban","userId":"01242021135040292581"},"user_tz":-540},"id":"19c0yQ9Pfc-7","outputId":"c16975c6-c9b8-4961-dacb-61064f873f0d"},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-5d3b4d8f.pth\n","100%|██████████| 170M/170M [00:02\u003c00:00, 86.9MB/s]\n"]},{"ename":"NameError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-18-198646eaa1a2\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 21\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 24\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'dataloader' is not defined"]}],"source":["# model 초기화\n","import segmentation_models_pytorch as smp\n","\n","lr = 1e-4\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","num_epochs = 30\n","\n","num_classes = 2\n","\n","# 전이 학습에 사용할 미리 학습된 DeepLabV3Plus 모델 가져오기\n","model = smp.DeepLabV3Plus(encoder_name=\"resnet101\", encoder_weights=\"imagenet\", in_channels=3, classes=num_classes)\n","\n","# 모델을 GPU로 이동\n","model.to(device)\n","\n","# loss function과 optimizer 정의\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr)\n","\n","# training loop\n","for epoch in range(num_epochs):  # 30 에폭 동안 학습합니다.\n","    model.train()\n","    epoch_loss = 0\n","    for images, masks in tqdm(dataloader):\n","        images = images.to(device)\n","        masks = masks.to(device)\n","        masks = masks.long()\n","\n","        optimizer.zero_grad()\n","        outputs = model(images)\n","        loss = criterion(outputs, masks)\n","        loss.backward()\n","        optimizer.step()\n","\n","        epoch_loss += loss.item()\n","\n","    if (epoch+1)%10 == 0:\n","      torch.save(model.state_dict(), './songweights/v3plus2_' + str(epoch+1) + '.pth')\n","\n","\n","    print(f'Epoch {epoch+1}, Loss: {epoch_loss/len(dataloader)}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qfbANWekXx3u"},"outputs":[],"source":["# # model 초기화\n","# model = DeepLabV3Plus(in_channels=3,num_classes=2).to(device)\n","\n","# # loss function과 optimizer 정의\n","# criterion = torch.nn.CrossEntropyLoss()\n","# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","# # training loop\n","# for epoch in range(10):  # 10 에폭 동안 학습합니다.\n","#     model.train()\n","#     epoch_loss = 0\n","#     for images, masks in tqdm(dataloader):\n","#         images = images.float().to(device)\n","#         masks = masks.float().to(device)\n","\n","#         optimizer.zero_grad()\n","#         outputs = model(images)\n","#         loss = criterion(outputs, masks.unsqueeze(1))\n","#         loss.backward()\n","#         optimizer.step()\n","\n","#         epoch_loss += loss.item()\n","\n","#     print(f'Epoch {epoch+1}, Loss: {epoch_loss/len(dataloader)}')"]},{"cell_type":"markdown","metadata":{"id":"FNbStL4b5shz"},"source":["#train val"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pwnlWLL4DinL"},"outputs":[],"source":["# class DiceLoss(nn.Module):\n","#     def __init__(self):\n","#         super(DiceLoss, self).__init__()\n","\n","#     def forward(self, inputs, targets, smooth=1):\n","\n","#         inputs = F.sigmoid(inputs) # sigmoid를 통과한 출력이면 주석처리\n","\n","#         inputs = inputs.view(-1)\n","#         targets = targets.view(-1)\n","\n","#         intersection = (inputs * targets).sum()\n","#         dice = (2.*intersection + smooth) / (inputs.sum() + targets.sum() + smooth)\n","\n","#         return 1 - dice"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"xb7F31Js5uOD"},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-5d3b4d8f.pth\n","100%|██████████| 170M/170M [00:03\u003c00:00, 59.2MB/s]\n","100%|██████████| 112/112 [13:59\u003c00:00,  7.49s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, Loss: 0.2510352331612791\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 357/357 [35:32\u003c00:00,  5.97s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Loss: 0.14481117595143678\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 112/112 [03:39\u003c00:00,  1.96s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2, Loss: 0.09928896384579795\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 357/357 [02:42\u003c00:00,  2.20it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Loss: 0.10047908156525855\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 112/112 [03:43\u003c00:00,  1.99s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3, Loss: 0.0811186885195119\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 357/357 [02:42\u003c00:00,  2.19it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Loss: 0.08673654751283448\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 112/112 [03:47\u003c00:00,  2.03s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4, Loss: 0.0744335065994944\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 357/357 [02:43\u003c00:00,  2.19it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Loss: 0.07985621173174776\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 112/112 [03:41\u003c00:00,  1.98s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5, Loss: 0.0707621042217527\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 357/357 [02:42\u003c00:00,  2.19it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Loss: 0.07577533264454006\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 112/112 [03:44\u003c00:00,  2.00s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 6, Loss: 0.06830341422132083\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 357/357 [02:43\u003c00:00,  2.19it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Loss: 0.0731043800586412\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 112/112 [03:46\u003c00:00,  2.02s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 7, Loss: 0.06731013687593597\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 357/357 [02:43\u003c00:00,  2.19it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Loss: 0.07129438050320837\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 112/112 [03:47\u003c00:00,  2.03s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 8, Loss: 0.0664108337036201\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 357/357 [02:43\u003c00:00,  2.18it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Loss: 0.06977056321643647\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 112/112 [03:46\u003c00:00,  2.02s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 9, Loss: 0.06560972120080676\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 357/357 [02:43\u003c00:00,  2.19it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Loss: 0.06873288358292994\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 112/112 [03:45\u003c00:00,  2.02s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 10, Loss: 0.06522300413676671\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 357/357 [02:44\u003c00:00,  2.18it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Loss: 0.06785599407361669\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 112/112 [03:49\u003c00:00,  2.05s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 11, Loss: 0.06416955590248108\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 357/357 [02:43\u003c00:00,  2.18it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Loss: 0.06713788415871415\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 112/112 [03:47\u003c00:00,  2.03s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 12, Loss: 0.06382654181548528\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 357/357 [02:43\u003c00:00,  2.18it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Loss: 0.06661094620782121\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 112/112 [03:50\u003c00:00,  2.06s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 13, Loss: 0.0646592378616333\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 357/357 [02:43\u003c00:00,  2.18it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Loss: 0.06604031488007191\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 112/112 [03:51\u003c00:00,  2.06s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 14, Loss: 0.06280505391103881\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 357/357 [02:43\u003c00:00,  2.18it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Loss: 0.06564762161559418\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 112/112 [03:51\u003c00:00,  2.07s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 15, Loss: 0.06286149312342916\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 357/357 [02:44\u003c00:00,  2.18it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Loss: 0.0653165376820818\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 112/112 [03:50\u003c00:00,  2.06s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 16, Loss: 0.06272720066564423\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 357/357 [02:43\u003c00:00,  2.18it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Loss: 0.0649959333136636\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 112/112 [03:48\u003c00:00,  2.04s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 17, Loss: 0.06194480508565903\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 357/357 [02:43\u003c00:00,  2.18it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Loss: 0.06473587908330752\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 112/112 [03:46\u003c00:00,  2.02s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 18, Loss: 0.06050722513880048\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 357/357 [02:44\u003c00:00,  2.18it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Loss: 0.06455557536678154\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 112/112 [03:50\u003c00:00,  2.06s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 19, Loss: 0.06234723489199366\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 357/357 [02:43\u003c00:00,  2.18it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Loss: 0.06430999755191535\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 112/112 [03:48\u003c00:00,  2.04s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 20, Loss: 0.06116338659610067\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 357/357 [02:43\u003c00:00,  2.18it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Loss: 0.06410342939093668\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 112/112 [03:44\u003c00:00,  2.01s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 21, Loss: 0.06202276476791927\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 357/357 [02:44\u003c00:00,  2.17it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Loss: 0.0639130705211009\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 112/112 [03:48\u003c00:00,  2.04s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 22, Loss: 0.06238456017204693\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 357/357 [02:44\u003c00:00,  2.17it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Loss: 0.06383113617322692\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 112/112 [03:43\u003c00:00,  2.00s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 23, Loss: 0.06179541988032205\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 357/357 [02:43\u003c00:00,  2.19it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Loss: 0.06368407014371301\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 112/112 [03:45\u003c00:00,  2.01s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 24, Loss: 0.06169680452772549\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 357/357 [02:44\u003c00:00,  2.18it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Loss: 0.06348265803494707\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 112/112 [03:46\u003c00:00,  2.02s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 25, Loss: 0.06022170292479651\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 357/357 [02:43\u003c00:00,  2.19it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Loss: 0.06341612806507185\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 112/112 [03:50\u003c00:00,  2.06s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 26, Loss: 0.06143828587872641\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 357/357 [02:43\u003c00:00,  2.18it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Loss: 0.06339017833982195\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 112/112 [03:46\u003c00:00,  2.02s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 27, Loss: 0.06106774881482124\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 357/357 [02:43\u003c00:00,  2.18it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Loss: 0.06328716147847536\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 112/112 [03:45\u003c00:00,  2.02s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 28, Loss: 0.060031487473419735\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 357/357 [02:43\u003c00:00,  2.18it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Loss: 0.06323126350798193\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 112/112 [03:44\u003c00:00,  2.00s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 29, Loss: 0.06130690925887653\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 357/357 [02:43\u003c00:00,  2.19it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Validation Loss: 0.06312404944449246\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 112/112 [03:40\u003c00:00,  1.97s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 30, Loss: 0.05962924233504704\n"]},{"name":"stderr","output_type":"stream","text":[" 18%|█▊        | 64/357 [00:30\u003c02:20,  2.09it/s]\n","ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n","ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n","ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n"]},{"name":"stdout","output_type":"stream","text":["Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"\u003cipython-input-13-6ff9565d7ed3\u003e\", line 68, in \u003ccell line: 43\u003e\n","    for images, masks in tqdm(val_dataloader):\n","  File \"/usr/local/lib/python3.10/dist-packages/tqdm/std.py\", line 1178, in __iter__\n","    for obj in iterable:\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 633, in __next__\n","    data = self._next_data()\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1345, in _next_data\n","    return self._process_data(data)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1371, in _process_data\n","    data.reraise()\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/_utils.py\", line 644, in reraise\n","    raise exception\n","TypeError: Caught TypeError in DataLoader worker process 0.\n","Original Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n","    data = fetcher.fetch(index)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n","    data = [self.dataset[idx] for idx in possibly_batched_index]\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in \u003clistcomp\u003e\n","    data = [self.dataset[idx] for idx in possibly_batched_index]\n","  File \"\u003cipython-input-11-3c46bb903e1b\u003e\", line 58, in __getitem__\n","    augmented = self.transform(image=image, mask=mask)\n","  File \"/usr/local/lib/python3.10/dist-packages/albumentations/core/composition.py\", line 190, in __call__\n","  File \"/usr/local/lib/python3.10/dist-packages/albumentations/core/composition.py\", line 267, in _check_args\n","    checked_multi = [\"masks\"]\n","TypeError: mask must be numpy array type\n","\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","  File \"/usr/lib/python3.10/inspect.py\", line 1667, in getinnerframes\n","    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n","  File \"/usr/lib/python3.10/inspect.py\", line 1625, in getframeinfo\n","    filename = getsourcefile(frame) or getfile(frame)\n","  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n","    module = getmodule(object, filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n","    file = getabsfile(object, _filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n","    return os.path.normcase(os.path.abspath(_filename))\n","  File \"/usr/lib/python3.10/posixpath.py\", line 383, in abspath\n","    cwd = os.getcwd()\n","OSError: [Errno 107] Transport endpoint is not connected\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"\u003cipython-input-13-6ff9565d7ed3\u003e\", line 68, in \u003ccell line: 43\u003e\n","    for images, masks in tqdm(val_dataloader):\n","  File \"/usr/local/lib/python3.10/dist-packages/tqdm/std.py\", line 1178, in __iter__\n","    for obj in iterable:\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 633, in __next__\n","    data = self._next_data()\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1345, in _next_data\n","    return self._process_data(data)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1371, in _process_data\n","    data.reraise()\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/_utils.py\", line 644, in reraise\n","    raise exception\n","TypeError: Caught TypeError in DataLoader worker process 0.\n","Original Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n","    data = fetcher.fetch(index)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n","    data = [self.dataset[idx] for idx in possibly_batched_index]\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in \u003clistcomp\u003e\n","    data = [self.dataset[idx] for idx in possibly_batched_index]\n","  File \"\u003cipython-input-11-3c46bb903e1b\u003e\", line 58, in __getitem__\n","    augmented = self.transform(image=image, mask=mask)\n","  File \"/usr/local/lib/python3.10/dist-packages/albumentations/core/composition.py\", line 190, in __call__\n","  File \"/usr/local/lib/python3.10/dist-packages/albumentations/core/composition.py\", line 267, in _check_args\n","    checked_multi = [\"masks\"]\n","TypeError: mask must be numpy array type\n","\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n","    if (await self.run_code(code, result,  async_=asy)):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n","    self.showtraceback(running_compiled_code=True)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n","    stb = self.InteractiveTB.structured_traceback(etype,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n","    return FormattedTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n","    return VerboseTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n","    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n","    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n","    return len(records), 0\n","TypeError: object of type 'NoneType' has no len()\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","  File \"/usr/lib/python3.10/inspect.py\", line 1667, in getinnerframes\n","    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n","  File \"/usr/lib/python3.10/inspect.py\", line 1625, in getframeinfo\n","    filename = getsourcefile(frame) or getfile(frame)\n","  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n","    module = getmodule(object, filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n","    file = getabsfile(object, _filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n","    return os.path.normcase(os.path.abspath(_filename))\n","  File \"/usr/lib/python3.10/posixpath.py\", line 383, in abspath\n","    cwd = os.getcwd()\n","OSError: [Errno 107] Transport endpoint is not connected\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"\u003cipython-input-13-6ff9565d7ed3\u003e\", line 68, in \u003ccell line: 43\u003e\n","    for images, masks in tqdm(val_dataloader):\n","  File \"/usr/local/lib/python3.10/dist-packages/tqdm/std.py\", line 1178, in __iter__\n","    for obj in iterable:\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 633, in __next__\n","    data = self._next_data()\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1345, in _next_data\n","    return self._process_data(data)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 1371, in _process_data\n","    data.reraise()\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/_utils.py\", line 644, in reraise\n","    raise exception\n","TypeError: Caught TypeError in DataLoader worker process 0.\n","Original Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n","    data = fetcher.fetch(index)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n","    data = [self.dataset[idx] for idx in possibly_batched_index]\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in \u003clistcomp\u003e\n","    data = [self.dataset[idx] for idx in possibly_batched_index]\n","  File \"\u003cipython-input-11-3c46bb903e1b\u003e\", line 58, in __getitem__\n","    augmented = self.transform(image=image, mask=mask)\n","  File \"/usr/local/lib/python3.10/dist-packages/albumentations/core/composition.py\", line 190, in __call__\n","  File \"/usr/local/lib/python3.10/dist-packages/albumentations/core/composition.py\", line 267, in _check_args\n","    checked_multi = [\"masks\"]\n","TypeError: mask must be numpy array type\n","\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n","    if (await self.run_code(code, result,  async_=asy)):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n","    self.showtraceback(running_compiled_code=True)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n","    stb = self.InteractiveTB.structured_traceback(etype,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n","    return FormattedTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n","    return VerboseTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n","    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n","    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n","    return len(records), 0\n","TypeError: object of type 'NoneType' has no len()\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n","    return runner(coro)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n","    coro.send(None)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n","    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3492, in run_ast_nodes\n","    self.showtraceback()\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n","    stb = self.InteractiveTB.structured_traceback(etype,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n","    return FormattedTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n","    return VerboseTB.structured_traceback(\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1142, in structured_traceback\n","    formatted_exceptions += self.format_exception_as_a_whole(etype, evalue, etb, lines_of_context,\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n","    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n","    return len(records), 0\n","TypeError: object of type 'NoneType' has no len()\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","  File \"/usr/lib/python3.10/inspect.py\", line 1667, in getinnerframes\n","    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n","  File \"/usr/lib/python3.10/inspect.py\", line 1625, in getframeinfo\n","    filename = getsourcefile(frame) or getfile(frame)\n","  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n","    module = getmodule(object, filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 861, in getmodule\n","    file = getabsfile(object, _filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 845, in getabsfile\n","    return os.path.normcase(os.path.abspath(_filename))\n","  File \"/usr/lib/python3.10/posixpath.py\", line 383, in abspath\n","    cwd = os.getcwd()\n","OSError: [Errno 107] Transport endpoint is not connected\n"]}],"source":["from sklearn.model_selection import train_test_split\n","\n","# 데이터셋 불러오기 및 전처리\n","train_dataset = SatelliteDataset(csv_file='./train.csv', transform=train_transform, train=True)\n","val_dataset = SatelliteDataset(csv_file='./val.csv', transform=val_transform, val=True)\n","\n","# 전체 데이터를 학습용과 검증용으로 분리\n","\n","# DataLoader 정의\n","train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n","val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4)\n","\n","# 모델 초기화\n","import segmentation_models_pytorch as smp\n","\n","lr = 1e-4\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","num_epochs = 50\n","num_classes = 2\n","\n","# 전이 학습에 사용할 미리 학습된 DeepLabV3Plus 모델 가져오기\n","model = smp.DeepLabV3Plus(encoder_name=\"resnet101\", encoder_weights=\"imagenet\", in_channels=3, classes=num_classes)\n","#model = smp.UnetPlusPlus(encoder_name=\"resnet34\", encoder_weights=\"imagenet\", in_channels=3, classes=num_classes)\n","#model = smp.Segformer(encoder_name=\"mit_b5\", encoder_weights=\"imagenet\", in_channels=3, classes=num_classes)\n","# 모델을 GPU로 이동\n","model.to(device)\n","\n","\n","#weight_init = torch.load(\"./JOweights/train_val_2_80.pth\")\n","#model.load_state_dict(weight_init, strict=False)\n","# loss function과 optimizer 정의\n","#criterion = torch.nn.CrossEntropyLoss()\n","#criterion = DiceLoss()\n","criterion = smp.losses.DiceLoss(mode='binary')\n","\n","\n","\n","#optimizer = torch.optim.Adam(model.parameters(), lr)\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=5e-4)\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=50)\n","\n","# training loop\n","for epoch in range(num_epochs):  # 30 에폭 동안 학습합니다.\n","    model.train()\n","    epoch_loss = 0\n","    for images, masks in tqdm(train_dataloader):\n","        images = images.to(device)\n","        masks = masks.to(device)\n","        masks = masks.long()\n","\n","        optimizer.zero_grad()\n","        outputs = model(images)\n","        loss = criterion(outputs, masks)\n","        loss.backward()\n","        optimizer.step()\n","\n","        epoch_loss += loss.item()\n","\n","    print(f'Epoch {epoch+1}, Loss: {epoch_loss/len(train_dataloader)}')\n","\n","    # learning rate scheduler 갱신\n","    scheduler.step()\n","\n","    # 검증 데이터를 사용하여 모델 평가\n","    model.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        for images, masks in tqdm(val_dataloader):\n","            images = images.to(device)\n","            masks = masks.to(device)\n","            masks = masks.long()\n","\n","            outputs = model(images)\n","            loss = criterion(outputs, masks)\n","            val_loss += loss.item()\n","\n","    print(f'Validation Loss: {val_loss/len(val_dataloader)}')\n","\n","    # 모델 저장\n","    if (epoch+1) % 2 == 0:\n","      torch.save(model.state_dict(), f'./JOweights/last_{epoch+1}.pth')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-YKWFMbv5uL2"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"FlOIJsvVXzY_"},"source":["## Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LN86qyD0X20V"},"outputs":[],"source":["# 예측\n","v3plus_weight = torch.load(\"./JOweights/last_100.pth\")\n","model.load_state_dict(v3plus_weight, strict=False)\n","\n","test_dataset = SatelliteDataset(csv_file='./test.csv', transform=test_transform, infer=True)\n","test_dataloader = DataLoader(test_dataset, batch_size=100, shuffle=False, num_workers=4)\n","\n","with torch.no_grad():\n","    model.eval()\n","    result = []\n","    for images, masks in test_dataloader:\n","        images = images.to(device)\n","\n","        outputs = model(images)\n","        masks = torch.sigmoid(outputs).cpu().numpy()\n","\n","        if masks.shape[1] != 1:\n","            masks = np.argmax(masks, axis=1)\n","\n","        masks = (masks \u003e 0.35).astype(np.uint8)\n","\n","        for i in range(len(images)):\n","            mask_rle = rle_encode(masks[i])\n","            if mask_rle == '':\n","                result.append(-1)\n","            else:\n","                result.append(mask_rle)\n","\n","submit = pd.read_csv('./sample_submission.csv')\n","\n","submit['mask_rle'] = result\n","\n","submit.to_csv('./submit_onehot.csv', index=False)\n"]},{"cell_type":"markdown","metadata":{"id":"bKDjSG_EWZu-"},"source":["#Dice Coefficient(Score)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"nRMqPbwJWC2y"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from typing import List, Union\n","from joblib import Parallel, delayed\n","\n","\n","def rle_decode(mask_rle: Union[str, int], shape=(224, 224)) -\u003e np.array:\n","    '''\n","    mask_rle: run-length as string formatted (start length)\n","    shape: (height,width) of array to return\n","    Returns numpy array, 1 - mask, 0 - background\n","    '''\n","    if mask_rle == -1:\n","        return np.zeros(shape)\n","\n","    s = mask_rle.split()\n","    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n","    starts -= 1\n","    ends = starts + lengths\n","    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n","    for lo, hi in zip(starts, ends):\n","        img[lo:hi] = 1\n","    return img.reshape(shape)\n","\n","\n","def dice_score(prediction: np.array, ground_truth: np.array, smooth=1e-7) -\u003e float:\n","    '''\n","    Calculate Dice Score between two binary masks.\n","    '''\n","    intersection = np.sum(prediction * ground_truth)\n","    return (2.0 * intersection + smooth) / (np.sum(prediction) + np.sum(ground_truth) + smooth)\n","\n","\n","def calculate_dice_scores(ground_truth_df, prediction_df, img_shape=(224, 224)) -\u003e List[float]:\n","    '''\n","    Calculate Dice scores for a dataset.\n","    '''\n","\n","\n","    # Keep only the rows in the prediction dataframe that have matching img_ids in the ground truth dataframe\n","    prediction_df = prediction_df[prediction_df.iloc[:, 0].isin(ground_truth_df.iloc[:, 0])]\n","    prediction_df.index = range(prediction_df.shape[0])\n","\n","    print(prediction_df)\n","    # Extract the mask_rle columns\n","    pred_mask_rle = prediction_df.iloc[:, 2]\n","    gt_mask_rle = ground_truth_df.iloc[:, 2]\n","\n","\n","    def calculate_dice(pred_rle, gt_rle):\n","        pred_mask = rle_decode(pred_rle, img_shape)\n","        gt_mask = rle_decode(gt_rle, img_shape)\n","\n","\n","        if np.sum(gt_mask) \u003e 0 or np.sum(pred_mask) \u003e 0:\n","            return dice_score(pred_mask, gt_mask)\n","        else:\n","            return None  # No valid masks found, return None\n","\n","\n","    dice_scores = Parallel(n_jobs=-1)(\n","        delayed(calculate_dice)(pred_rle, gt_rle) for pred_rle, gt_rle in zip(pred_mask_rle, gt_mask_rle)\n","    )\n","\n","\n","    dice_scores = [score for score in dice_scores if score is not None]  # Exclude None values\n","\n","\n","    return np.mean(dice_scores)\n","\n","train = pd.read_csv(\"./train.csv\")\n","calculate_dice_scores(train, pred)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"executionInfo":{"elapsed":674,"status":"ok","timestamp":1690265032943,"user":{"displayName":"sehyun ban","userId":"01242021135040292581"},"user_tz":-540},"id":"No7v8eTApYiz","outputId":"08df7edb-0da3-4bb5-b16d-3110758461a4"},"outputs":[{"data":{"text/html":["\n","\n","  \u003cdiv id=\"df-cf125ec3-44f2-4eb5-b7ce-944d38bd91b6\"\u003e\n","    \u003cdiv class=\"colab-df-container\"\u003e\n","      \u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003eimg_id\u003c/th\u003e\n","      \u003cth\u003eimg_path\u003c/th\u003e\n","      \u003cth\u003emask_rle\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e0\u003c/th\u003e\n","      \u003ctd\u003eTRAIN_0000\u003c/td\u003e\n","      \u003ctd\u003e./train_img/TRAIN_0000.png\u003c/td\u003e\n","      \u003ctd\u003e9576 7 10590 17 11614 17 12638 17 13662 17 146...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1\u003c/th\u003e\n","      \u003ctd\u003eTRAIN_0001\u003c/td\u003e\n","      \u003ctd\u003e./train_img/TRAIN_0001.png\u003c/td\u003e\n","      \u003ctd\u003e208402 1 209425 6 210449 10 211473 14 212497 1...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2\u003c/th\u003e\n","      \u003ctd\u003eTRAIN_0002\u003c/td\u003e\n","      \u003ctd\u003e./train_img/TRAIN_0002.png\u003c/td\u003e\n","      \u003ctd\u003e855 34 15654 9 16678 9 16742 8 17702 9 17766 9...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e3\u003c/th\u003e\n","      \u003ctd\u003eTRAIN_0003\u003c/td\u003e\n","      \u003ctd\u003e./train_img/TRAIN_0003.png\u003c/td\u003e\n","      \u003ctd\u003e362 6 745 15 798 22 900 25 1385 8 1828 16 1924...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e4\u003c/th\u003e\n","      \u003ctd\u003eTRAIN_0004\u003c/td\u003e\n","      \u003ctd\u003e./train_img/TRAIN_0004.png\u003c/td\u003e\n","      \u003ctd\u003e34 27 1058 27 2082 27 3105 27 4129 27 5153 27 ...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e...\u003c/th\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e7135\u003c/th\u003e\n","      \u003ctd\u003eTRAIN_7135\u003c/td\u003e\n","      \u003ctd\u003e./train_img/TRAIN_7135.png\u003c/td\u003e\n","      \u003ctd\u003e193 19 882 18 985 21 1217 17 1782 2 1906 18 20...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e7136\u003c/th\u003e\n","      \u003ctd\u003eTRAIN_7136\u003c/td\u003e\n","      \u003ctd\u003e./train_img/TRAIN_7136.png\u003c/td\u003e\n","      \u003ctd\u003e85938 13 86962 20 87986 20 89009 21 90033 21 9...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e7137\u003c/th\u003e\n","      \u003ctd\u003eTRAIN_7137\u003c/td\u003e\n","      \u003ctd\u003e./train_img/TRAIN_7137.png\u003c/td\u003e\n","      \u003ctd\u003e100 59 314 28 878 28 997 20 1124 59 1338 28 19...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e7138\u003c/th\u003e\n","      \u003ctd\u003eTRAIN_7138\u003c/td\u003e\n","      \u003ctd\u003e./train_img/TRAIN_7138.png\u003c/td\u003e\n","      \u003ctd\u003e789 18 975 17 1814 16 2000 14 2544 2 2839 14 3...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e7139\u003c/th\u003e\n","      \u003ctd\u003eTRAIN_7139\u003c/td\u003e\n","      \u003ctd\u003e./train_img/TRAIN_7139.png\u003c/td\u003e\n","      \u003ctd\u003e711 19 821 17 924 101 1740 14 1844 19 1948 101...\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003cp\u003e7140 rows × 3 columns\u003c/p\u003e\n","\u003c/div\u003e\n","      \u003cbutton class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cf125ec3-44f2-4eb5-b7ce-944d38bd91b6')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\"\u003e\n","\n","  \u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\"\u003e\n","    \u003cpath d=\"M0 0h24v24H0V0z\" fill=\"none\"/\u003e\n","    \u003cpath d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/\u003e\u003cpath d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/\u003e\n","  \u003c/svg\u003e\n","      \u003c/button\u003e\n","\n","\n","\n","    \u003cdiv id=\"df-4c6492ce-5972-40a6-a970-f4ec17402089\"\u003e\n","      \u003cbutton class=\"colab-df-quickchart\" onclick=\"quickchart('df-4c6492ce-5972-40a6-a970-f4ec17402089')\"\n","              title=\"Suggest charts.\"\n","              style=\"display:none;\"\u003e\n","\n","\u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\"\u003e\n","    \u003cg\u003e\n","        \u003cpath d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/\u003e\n","    \u003c/g\u003e\n","\u003c/svg\u003e\n","      \u003c/button\u003e\n","    \u003c/div\u003e\n","\n","\u003cstyle\u003e\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","\u003c/style\u003e\n","\n","    \u003cscript\u003e\n","      async function quickchart(key) {\n","        const containerElement = document.querySelector('#' + key);\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      }\n","    \u003c/script\u003e\n","\n","      \u003cscript\u003e\n","\n","function displayQuickchartButton(domScope) {\n","  let quickchartButtonEl =\n","    domScope.querySelector('#df-4c6492ce-5972-40a6-a970-f4ec17402089 button.colab-df-quickchart');\n","  quickchartButtonEl.style.display =\n","    google.colab.kernel.accessAllowed ? 'block' : 'none';\n","}\n","\n","        displayQuickchartButton(document);\n","      \u003c/script\u003e\n","      \u003cstyle\u003e\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  \u003c/style\u003e\n","\n","      \u003cscript\u003e\n","        const buttonEl =\n","          document.querySelector('#df-cf125ec3-44f2-4eb5-b7ce-944d38bd91b6 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-cf125ec3-44f2-4eb5-b7ce-944d38bd91b6');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '\u003ca target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb\u003edata table notebook\u003c/a\u003e'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      \u003c/script\u003e\n","    \u003c/div\u003e\n","  \u003c/div\u003e\n"],"text/plain":["          img_id                    img_path  \\\n","0     TRAIN_0000  ./train_img/TRAIN_0000.png   \n","1     TRAIN_0001  ./train_img/TRAIN_0001.png   \n","2     TRAIN_0002  ./train_img/TRAIN_0002.png   \n","3     TRAIN_0003  ./train_img/TRAIN_0003.png   \n","4     TRAIN_0004  ./train_img/TRAIN_0004.png   \n","...          ...                         ...   \n","7135  TRAIN_7135  ./train_img/TRAIN_7135.png   \n","7136  TRAIN_7136  ./train_img/TRAIN_7136.png   \n","7137  TRAIN_7137  ./train_img/TRAIN_7137.png   \n","7138  TRAIN_7138  ./train_img/TRAIN_7138.png   \n","7139  TRAIN_7139  ./train_img/TRAIN_7139.png   \n","\n","                                               mask_rle  \n","0     9576 7 10590 17 11614 17 12638 17 13662 17 146...  \n","1     208402 1 209425 6 210449 10 211473 14 212497 1...  \n","2     855 34 15654 9 16678 9 16742 8 17702 9 17766 9...  \n","3     362 6 745 15 798 22 900 25 1385 8 1828 16 1924...  \n","4     34 27 1058 27 2082 27 3105 27 4129 27 5153 27 ...  \n","...                                                 ...  \n","7135  193 19 882 18 985 21 1217 17 1782 2 1906 18 20...  \n","7136  85938 13 86962 20 87986 20 89009 21 90033 21 9...  \n","7137  100 59 314 28 878 28 997 20 1124 59 1338 28 19...  \n","7138  789 18 975 17 1814 16 2000 14 2544 2 2839 14 3...  \n","7139  711 19 821 17 924 101 1740 14 1844 19 1948 101...  \n","\n","[7140 rows x 3 columns]"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["train"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"executionInfo":{"elapsed":753,"status":"ok","timestamp":1690265041021,"user":{"displayName":"sehyun ban","userId":"01242021135040292581"},"user_tz":-540},"id":"ZSU1cVmSpaqD","outputId":"1b3eb282-0dd2-4d3f-88b1-c8f2bcea2132"},"outputs":[{"data":{"text/html":["\n","\n","  \u003cdiv id=\"df-deb86316-504b-4c9a-965d-3fb3967d94f8\"\u003e\n","    \u003cdiv class=\"colab-df-container\"\u003e\n","      \u003cdiv\u003e\n","\u003cstyle scoped\u003e\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","\u003c/style\u003e\n","\u003ctable border=\"1\" class=\"dataframe\"\u003e\n","  \u003cthead\u003e\n","    \u003ctr style=\"text-align: right;\"\u003e\n","      \u003cth\u003e\u003c/th\u003e\n","      \u003cth\u003eimg_id\u003c/th\u003e\n","      \u003cth\u003eimg_path\u003c/th\u003e\n","      \u003cth\u003emask_rle\u003c/th\u003e\n","    \u003c/tr\u003e\n","  \u003c/thead\u003e\n","  \u003ctbody\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e0\u003c/th\u003e\n","      \u003ctd\u003eTRAIN_0000\u003c/td\u003e\n","      \u003ctd\u003e./train_img/TRAIN_0000.png\u003c/td\u003e\n","      \u003ctd\u003e8549 5 9572 9 10596 9 11620 1 11626 2 12616 3 ...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e1\u003c/th\u003e\n","      \u003ctd\u003eTRAIN_0001\u003c/td\u003e\n","      \u003ctd\u003e./train_img/TRAIN_0001.png\u003c/td\u003e\n","      \u003ctd\u003e213522 7 214544 12 215567 17 216466 2 216470 1...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e2\u003c/th\u003e\n","      \u003ctd\u003eTRAIN_0002\u003c/td\u003e\n","      \u003ctd\u003e./train_img/TRAIN_0002.png\u003c/td\u003e\n","      \u003ctd\u003e34 22 865 6 1057 23 1887 8 2081 24 2310 1 2909...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e3\u003c/th\u003e\n","      \u003ctd\u003eTRAIN_0003\u003c/td\u003e\n","      \u003ctd\u003e./train_img/TRAIN_0003.png\u003c/td\u003e\n","      \u003ctd\u003e361 8 739 19 800 27 894 30 1150 3 1383 10 1761...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e4\u003c/th\u003e\n","      \u003ctd\u003eTRAIN_0004\u003c/td\u003e\n","      \u003ctd\u003e./train_img/TRAIN_0004.png\u003c/td\u003e\n","      \u003ctd\u003e30 28 1054 29 2079 28 3103 29 4127 29 5151 29 ...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e...\u003c/th\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","      \u003ctd\u003e...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e7135\u003c/th\u003e\n","      \u003ctd\u003eTRAIN_7135\u003c/td\u003e\n","      \u003ctd\u003e./train_img/TRAIN_7135.png\u003c/td\u003e\n","      \u003ctd\u003e131 7 877 24 993 18 1154 8 1901 24 2016 19 217...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e7136\u003c/th\u003e\n","      \u003ctd\u003eTRAIN_7136\u003c/td\u003e\n","      \u003ctd\u003e./train_img/TRAIN_7136.png\u003c/td\u003e\n","      \u003ctd\u003e83894 1 84917 6 85940 13 86964 15 87987 17 890...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e7137\u003c/th\u003e\n","      \u003ctd\u003eTRAIN_7137\u003c/td\u003e\n","      \u003ctd\u003e./train_img/TRAIN_7137.png\u003c/td\u003e\n","      \u003ctd\u003e108 51 314 28 879 28 998 17 1131 52 1338 28 19...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e7138\u003c/th\u003e\n","      \u003ctd\u003eTRAIN_7138\u003c/td\u003e\n","      \u003ctd\u003e./train_img/TRAIN_7138.png\u003c/td\u003e\n","      \u003ctd\u003e792 9 980 9 1816 8 2004 9 2840 9 3028 10 3859 ...\u003c/td\u003e\n","    \u003c/tr\u003e\n","    \u003ctr\u003e\n","      \u003cth\u003e7139\u003c/th\u003e\n","      \u003ctd\u003eTRAIN_7139\u003c/td\u003e\n","      \u003ctd\u003e./train_img/TRAIN_7139.png\u003c/td\u003e\n","      \u003ctd\u003e709 25 816 25 929 94 1731 28 1839 27 1952 95 2...\u003c/td\u003e\n","    \u003c/tr\u003e\n","  \u003c/tbody\u003e\n","\u003c/table\u003e\n","\u003cp\u003e7140 rows × 3 columns\u003c/p\u003e\n","\u003c/div\u003e\n","      \u003cbutton class=\"colab-df-convert\" onclick=\"convertToInteractive('df-deb86316-504b-4c9a-965d-3fb3967d94f8')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\"\u003e\n","\n","  \u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\"\u003e\n","    \u003cpath d=\"M0 0h24v24H0V0z\" fill=\"none\"/\u003e\n","    \u003cpath d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/\u003e\u003cpath d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/\u003e\n","  \u003c/svg\u003e\n","      \u003c/button\u003e\n","\n","\n","\n","    \u003cdiv id=\"df-96e7a407-97c7-4eed-a327-727c9c4c2799\"\u003e\n","      \u003cbutton class=\"colab-df-quickchart\" onclick=\"quickchart('df-96e7a407-97c7-4eed-a327-727c9c4c2799')\"\n","              title=\"Suggest charts.\"\n","              style=\"display:none;\"\u003e\n","\n","\u003csvg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\"\u003e\n","    \u003cg\u003e\n","        \u003cpath d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/\u003e\n","    \u003c/g\u003e\n","\u003c/svg\u003e\n","      \u003c/button\u003e\n","    \u003c/div\u003e\n","\n","\u003cstyle\u003e\n","  .colab-df-quickchart {\n","    background-color: #E8F0FE;\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: #1967D2;\n","    height: 32px;\n","    padding: 0 0 0 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: #E2EBFA;\n","    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: #174EA6;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","    background-color: #3B4455;\n","    fill: #D2E3FC;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart:hover {\n","    background-color: #434B5C;\n","    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","    fill: #FFFFFF;\n","  }\n","\u003c/style\u003e\n","\n","    \u003cscript\u003e\n","      async function quickchart(key) {\n","        const containerElement = document.querySelector('#' + key);\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      }\n","    \u003c/script\u003e\n","\n","      \u003cscript\u003e\n","\n","function displayQuickchartButton(domScope) {\n","  let quickchartButtonEl =\n","    domScope.querySelector('#df-96e7a407-97c7-4eed-a327-727c9c4c2799 button.colab-df-quickchart');\n","  quickchartButtonEl.style.display =\n","    google.colab.kernel.accessAllowed ? 'block' : 'none';\n","}\n","\n","        displayQuickchartButton(document);\n","      \u003c/script\u003e\n","      \u003cstyle\u003e\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  \u003c/style\u003e\n","\n","      \u003cscript\u003e\n","        const buttonEl =\n","          document.querySelector('#df-deb86316-504b-4c9a-965d-3fb3967d94f8 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-deb86316-504b-4c9a-965d-3fb3967d94f8');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '\u003ca target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb\u003edata table notebook\u003c/a\u003e'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      \u003c/script\u003e\n","    \u003c/div\u003e\n","  \u003c/div\u003e\n"],"text/plain":["          img_id                    img_path  \\\n","0     TRAIN_0000  ./train_img/TRAIN_0000.png   \n","1     TRAIN_0001  ./train_img/TRAIN_0001.png   \n","2     TRAIN_0002  ./train_img/TRAIN_0002.png   \n","3     TRAIN_0003  ./train_img/TRAIN_0003.png   \n","4     TRAIN_0004  ./train_img/TRAIN_0004.png   \n","...          ...                         ...   \n","7135  TRAIN_7135  ./train_img/TRAIN_7135.png   \n","7136  TRAIN_7136  ./train_img/TRAIN_7136.png   \n","7137  TRAIN_7137  ./train_img/TRAIN_7137.png   \n","7138  TRAIN_7138  ./train_img/TRAIN_7138.png   \n","7139  TRAIN_7139  ./train_img/TRAIN_7139.png   \n","\n","                                               mask_rle  \n","0     8549 5 9572 9 10596 9 11620 1 11626 2 12616 3 ...  \n","1     213522 7 214544 12 215567 17 216466 2 216470 1...  \n","2     34 22 865 6 1057 23 1887 8 2081 24 2310 1 2909...  \n","3     361 8 739 19 800 27 894 30 1150 3 1383 10 1761...  \n","4     30 28 1054 29 2079 28 3103 29 4127 29 5151 29 ...  \n","...                                                 ...  \n","7135  131 7 877 24 993 18 1154 8 1901 24 2016 19 217...  \n","7136  83894 1 84917 6 85940 13 86964 15 87987 17 890...  \n","7137  108 51 314 28 879 28 998 17 1131 52 1338 28 19...  \n","7138  792 9 980 9 1816 8 2004 9 2840 9 3028 10 3859 ...  \n","7139  709 25 816 25 929 94 1731 28 1839 27 1952 95 2...  \n","\n","[7140 rows x 3 columns]"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["pred"]},{"cell_type":"markdown","metadata":{"id":"EFUSJ285X4ne"},"source":["## Submission"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZZcUbNV8X5-W"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-ke8fygMX7i4"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"NbJe2msPZSHA"},"source":["#ensemble"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P5iTCqLVaMER"},"outputs":[],"source":["class SatelliteDataset(Dataset):\n","    def __init__(self, csv_file, transform=None, infer=False):\n","        self.data = pd.read_csv(csv_file)\n","        self.transform = transform\n","        self.infer = infer\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.data.iloc[idx, 1]\n","        image = cv2.imread(img_path)\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","        if self.infer:\n","            if self.transform:\n","                image = self.transform(image=image)['image']\n","            return image\n","\n","        mask_rle = self.data.iloc[idx, 2]\n","        mask = rle_decode(mask_rle, (image.shape[0], image.shape[1]))\n","\n","        if self.transform:\n","            augmented = self.transform(image=image, mask=mask)\n","            image = augmented['image']\n","            mask = augmented['mask']\n","\n","        return image, mask"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vm8BRVsIZUtP"},"outputs":[],"source":["def train_transform(train = True):\n","        return A.Compose(\n","            [\n","                A.OneOf(\n","                    [\n","                        A.RandomBrightness(p=1),\n","                        A.RandomBrightnessContrast(p=1),\n","                        A.Emboss(p=1),\n","                        A.RandomShadow(p=1),\n","                        A.NoOp(),\n","                    ],\n","                    p=1,\n","                ),\n","                A.OneOf(\n","                    [\n","                        A.Blur(p=1),\n","                        A.AdvancedBlur(p=1),\n","                        A.MotionBlur(p=1),\n","                    ],\n","                    p=0.6,\n","                ),\n","                A.OneOf(\n","                    [\n","                        A.NoOp(),\n","                        A.HorizontalFlip(p=0.5),\n","                        A.VerticalFlip(p=0.5),\n","                        A.ShiftScaleRotate(p=0.5),\n","                        A.Rotate(limit=90, p=1, border_mode=cv2.BORDER_REPLICATE),\n","                        A.RandomRotate90(p=1)\n","                    ],\n","                    p=1,\n","                ),\n","                A.RandomCrop(224, 224),\n","                A.Normalize(),\n","                ToTensorV2(transpose_mask=True)\n","            ]\n","        )\n","\n","\n","def test_transform(train = False):\n","        return A.Compose(\n","            [\n","                A.Normalize(),\n","                ToTensorV2(transpose_mask=True)\n","            ]\n","        )\n","\n","train_transform = train_transform()\n","test_transform = test_transform()\n","\n","dataset = SatelliteDataset(csv_file='./train.csv', transform=train_transform)\n","dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4)"]},{"cell_type":"markdown","metadata":{"id":"oguoJriVJDRc"},"source":["# ensemble 만들기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6zEOZjKEJbKu"},"outputs":[],"source":["import os\n","import cv2\n","import pandas as pd\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","\n","from tqdm import tqdm\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s-vnnu8nJbHI"},"outputs":[],"source":["!pip install -q -U segmentation-models-pytorch albumentations \u003e /dev/null"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5444,"status":"ok","timestamp":1689847685216,"user":{"displayName":"조영민","userId":"10749653391039175177"},"user_tz":-540},"id":"qWi25jSbJbBd","outputId":"458ac67d-c69e-4f2d-ce10-3cf276d26d79"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.74)\n","Requirement already satisfied: numpy\u003e=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.22.4)\n"]}],"source":["!pip install --upgrade opencv-python"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_9Bu0ZY5Jptw"},"outputs":[],"source":["class SatelliteDataset(Dataset):\n","    def __init__(self, csv_file, transform=None, infer=False):\n","        self.data = pd.read_csv(csv_file)\n","        self.transform = transform\n","        self.infer = infer\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.data.iloc[idx, 1]\n","        image = cv2.imread(img_path)\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","        if self.infer:\n","            if self.transform:\n","                image = self.transform(image=image)['image']\n","            return image\n","\n","        mask_rle = self.data.iloc[idx, 2]\n","        mask = rle_decode(mask_rle, (image.shape[0], image.shape[1]))\n","\n","        if self.transform:\n","            augmented = self.transform(image=image, mask=mask)\n","            image = augmented['image']\n","            mask = augmented['mask']\n","\n","        return image, mask"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"53TE0dNEJpqP"},"outputs":[],"source":["train_df = pd.read_csv(\"./train.csv\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X-oyWTlJJpoh"},"outputs":[],"source":["def crop_img(img, img_size=256):\n","    img_list = []\n","\n","    y_cnt = 0\n","    while True:\n","        start_y = y_cnt * img_size\n","        end_y = (y_cnt + 1) * img_size\n","\n","        if end_y \u003e 1024:\n","            break\n","\n","        x_cnt = 0\n","        while True:\n","            start_x = x_cnt * img_size\n","            end_x = (x_cnt + 1) * img_size\n","\n","            if end_x \u003e 1024:\n","                break\n","\n","            temp_img = img[start_x:end_x, start_y:end_y, :]\n","            x_cnt += 1\n","            img_list.append(temp_img)\n","\n","        y_cnt += 1\n","\n","    return img_list"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1597,"status":"ok","timestamp":1689847688699,"user":{"displayName":"조영민","userId":"10749653391039175177"},"user_tz":-540},"id":"eqp9MYQ0Jpmr","outputId":"b0102f15-1495-4294-94c0-34424af0135d"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/albumentations/augmentations/transforms.py:1613: FutureWarning: This class has been deprecated. Please use RandomBrightnessContrast\n","  self.max_value = max_value\n"]}],"source":["def train_transform(train = True):\n","        return A.Compose(\n","            [\n","                A.OneOf(\n","                    [\n","                        A.RandomBrightness(p=1),\n","                        A.RandomBrightnessContrast(p=1),\n","                        A.Emboss(p=1),\n","                        A.RandomShadow(p=1),\n","                        A.NoOp(),\n","                    ],\n","                    p=1,\n","                ),\n","                A.OneOf(\n","                    [\n","                        A.Blur(p=1),\n","                        A.AdvancedBlur(p=1),\n","                        A.MotionBlur(p=1),\n","                    ],\n","                    p=0.6,\n","                ),\n","                A.OneOf(\n","                    [\n","                        A.NoOp(),\n","                        A.HorizontalFlip(p=0.5),\n","                        A.VerticalFlip(p=0.5),\n","                        A.ShiftScaleRotate(p=0.5),\n","                        A.Rotate(limit=90, p=1, border_mode=cv2.BORDER_REPLICATE),\n","                        A.RandomRotate90(p=1)\n","                    ],\n","                    p=1,\n","                ),\n","                A.RandomCrop(224, 224),\n","                A.Normalize(),\n","                ToTensorV2(transpose_mask=True)\n","            ]\n","        )\n","\n","\n","def test_transform(train = False):\n","        return A.Compose(\n","            [\n","                A.Normalize(),\n","                ToTensorV2(transpose_mask=True)\n","            ]\n","        )\n","\n","train_transform = train_transform()\n","test_transform = test_transform()\n","\n","dataset = SatelliteDataset(csv_file='./train.csv', transform=train_transform)\n","dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kDR7qzC2Jpki"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KNNMKT6sJa-F"},"outputs":[],"source":["# RLE 디코딩 함수\n","def rle_decode(mask_rle, shape):\n","    s = mask_rle.split()\n","    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n","    starts -= 1\n","    ends = starts + lengths\n","    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n","    for lo, hi in zip(starts, ends):\n","        img[lo:hi] = 1\n","    return img.reshape(shape)\n","\n","# RLE 인코딩 함수\n","def rle_encode(mask):\n","    pixels = mask.flatten()\n","    pixels = np.concatenate([[0], pixels, [0]])\n","    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n","    runs[1::2] -= runs[::2]\n","    return ' '.join(str(x) for x in runs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KzsHabYgJa73"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FgE4K6c1Ja5m"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2461,"status":"ok","timestamp":1689847692897,"user":{"displayName":"조영민","userId":"10749653391039175177"},"user_tz":-540},"id":"521KPOnzZUlV","outputId":"5ee6c387-2d15-4607-e9c4-cc706baf8d7e"},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-5d3b4d8f.pth\n","100%|██████████| 170M/170M [00:00\u003c00:00, 234MB/s]\n"]},{"data":{"text/plain":["DeepLabV3Plus(\n","  (encoder): ResNetEncoder(\n","    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (relu): ReLU(inplace=True)\n","    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","    (layer1): Sequential(\n","      (0): Bottleneck(\n","        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): Bottleneck(\n","        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (2): Bottleneck(\n","        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","    )\n","    (layer2): Sequential(\n","      (0): Bottleneck(\n","        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): Bottleneck(\n","        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (2): Bottleneck(\n","        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (3): Bottleneck(\n","        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","    )\n","    (layer3): Sequential(\n","      (0): Bottleneck(\n","        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (2): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (3): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (4): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (5): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (6): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (7): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (8): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (9): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (10): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (11): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (12): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (13): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (14): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (15): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (16): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (17): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (18): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (19): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (20): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (21): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (22): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","    )\n","    (layer4): Sequential(\n","      (0): Bottleneck(\n","        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), dilation=(2, 2), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), dilation=(2, 2), bias=False)\n","        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), dilation=(2, 2), bias=False)\n","          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): Bottleneck(\n","        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), dilation=(2, 2), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), dilation=(2, 2), bias=False)\n","        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (2): Bottleneck(\n","        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), dilation=(2, 2), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), dilation=(2, 2), bias=False)\n","        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","    )\n","  )\n","  (decoder): DeepLabV3PlusDecoder(\n","    (aspp): Sequential(\n","      (0): ASPP(\n","        (convs): ModuleList(\n","          (0): Sequential(\n","            (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (2): ReLU()\n","          )\n","          (1): ASPPSeparableConv(\n","            (0): SeparableConv2d(\n","              (0): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12), groups=2048, bias=False)\n","              (1): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            )\n","            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (2): ReLU()\n","          )\n","          (2): ASPPSeparableConv(\n","            (0): SeparableConv2d(\n","              (0): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(24, 24), dilation=(24, 24), groups=2048, bias=False)\n","              (1): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            )\n","            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (2): ReLU()\n","          )\n","          (3): ASPPSeparableConv(\n","            (0): SeparableConv2d(\n","              (0): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(36, 36), dilation=(36, 36), groups=2048, bias=False)\n","              (1): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            )\n","            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (2): ReLU()\n","          )\n","          (4): ASPPPooling(\n","            (0): AdaptiveAvgPool2d(output_size=1)\n","            (1): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","            (3): ReLU()\n","          )\n","        )\n","        (project): Sequential(\n","          (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU()\n","          (3): Dropout(p=0.5, inplace=False)\n","        )\n","      )\n","      (1): SeparableConv2d(\n","        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n","        (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","      (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (3): ReLU()\n","    )\n","    (up): UpsamplingBilinear2d(scale_factor=4.0, mode='bilinear')\n","    (block1): Sequential(\n","      (0): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): ReLU()\n","    )\n","    (block2): Sequential(\n","      (0): SeparableConv2d(\n","        (0): Conv2d(304, 304, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=304, bias=False)\n","        (1): Conv2d(304, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      )\n","      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): ReLU()\n","    )\n","  )\n","  (segmentation_head): SegmentationHead(\n","    (0): Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1))\n","    (1): UpsamplingBilinear2d(scale_factor=4.0, mode='bilinear')\n","    (2): Activation(\n","      (activation): Identity()\n","    )\n","  )\n",")"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["import segmentation_models_pytorch as smp\n","\n","model = smp.DeepLabV3Plus(encoder_name=\"resnet101\", encoder_weights=\"imagenet\", in_channels=3, classes=2)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","\n","# 모델을 GPU로 이동\n","model.to(device)\n","\n","# loss function과 optimizer 정의\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","# training loop\n","for epoch in range(10):  # 10 에폭 동안 학습합니다.\n","    model.train()\n","    epoch_loss = 0\n","    for images, masks in tqdm(dataloader):\n","        images = images.float().to(device)\n","        masks = masks.float().to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(images)\n","        loss = criterion(outputs, masks.unsqueeze(1))\n","        loss.backward()\n","        optimizer.step()\n","\n","        epoch_loss += loss.item()\n","\n","    print(f'Epoch {epoch+1}, Loss: {epoch_loss/len(dataloader)}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1601,"status":"ok","timestamp":1689847730547,"user":{"displayName":"조영민","userId":"10749653391039175177"},"user_tz":-540},"id":"dK7fW7jkZUjR","outputId":"0e887d64-187d-49ae-fbdc-77f47d7162c1"},"outputs":[{"data":{"text/plain":["UnetPlusPlus(\n","  (encoder): ResNetEncoder(\n","    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (relu): ReLU(inplace=True)\n","    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","    (layer1): Sequential(\n","      (0): Bottleneck(\n","        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): Bottleneck(\n","        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (2): Bottleneck(\n","        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","    )\n","    (layer2): Sequential(\n","      (0): Bottleneck(\n","        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): Bottleneck(\n","        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (2): Bottleneck(\n","        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (3): Bottleneck(\n","        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","    )\n","    (layer3): Sequential(\n","      (0): Bottleneck(\n","        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (2): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (3): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (4): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (5): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (6): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (7): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (8): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (9): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (10): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (11): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (12): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (13): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (14): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (15): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (16): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (17): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (18): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (19): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (20): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (21): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (22): Bottleneck(\n","        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","    )\n","    (layer4): Sequential(\n","      (0): Bottleneck(\n","        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): Bottleneck(\n","        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","      (2): Bottleneck(\n","        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","      )\n","    )\n","  )\n","  (decoder): UnetPlusPlusDecoder(\n","    (center): Identity()\n","    (blocks): ModuleDict(\n","      (x_0_0): DecoderBlock(\n","        (conv1): Conv2dReLU(\n","          (0): Conv2d(3072, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (attention1): Attention(\n","          (attention): Identity()\n","        )\n","        (conv2): Conv2dReLU(\n","          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (attention2): Attention(\n","          (attention): Identity()\n","        )\n","      )\n","      (x_0_1): DecoderBlock(\n","        (conv1): Conv2dReLU(\n","          (0): Conv2d(1280, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (attention1): Attention(\n","          (attention): Identity()\n","        )\n","        (conv2): Conv2dReLU(\n","          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (attention2): Attention(\n","          (attention): Identity()\n","        )\n","      )\n","      (x_1_1): DecoderBlock(\n","        (conv1): Conv2dReLU(\n","          (0): Conv2d(1536, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (attention1): Attention(\n","          (attention): Identity()\n","        )\n","        (conv2): Conv2dReLU(\n","          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (attention2): Attention(\n","          (attention): Identity()\n","        )\n","      )\n","      (x_0_2): DecoderBlock(\n","        (conv1): Conv2dReLU(\n","          (0): Conv2d(896, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (attention1): Attention(\n","          (attention): Identity()\n","        )\n","        (conv2): Conv2dReLU(\n","          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (attention2): Attention(\n","          (attention): Identity()\n","        )\n","      )\n","      (x_1_2): DecoderBlock(\n","        (conv1): Conv2dReLU(\n","          (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (attention1): Attention(\n","          (attention): Identity()\n","        )\n","        (conv2): Conv2dReLU(\n","          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (attention2): Attention(\n","          (attention): Identity()\n","        )\n","      )\n","      (x_2_2): DecoderBlock(\n","        (conv1): Conv2dReLU(\n","          (0): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (attention1): Attention(\n","          (attention): Identity()\n","        )\n","        (conv2): Conv2dReLU(\n","          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (attention2): Attention(\n","          (attention): Identity()\n","        )\n","      )\n","      (x_0_3): DecoderBlock(\n","        (conv1): Conv2dReLU(\n","          (0): Conv2d(320, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (attention1): Attention(\n","          (attention): Identity()\n","        )\n","        (conv2): Conv2dReLU(\n","          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (attention2): Attention(\n","          (attention): Identity()\n","        )\n","      )\n","      (x_1_3): DecoderBlock(\n","        (conv1): Conv2dReLU(\n","          (0): Conv2d(448, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (attention1): Attention(\n","          (attention): Identity()\n","        )\n","        (conv2): Conv2dReLU(\n","          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (attention2): Attention(\n","          (attention): Identity()\n","        )\n","      )\n","      (x_2_3): DecoderBlock(\n","        (conv1): Conv2dReLU(\n","          (0): Conv2d(384, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (attention1): Attention(\n","          (attention): Identity()\n","        )\n","        (conv2): Conv2dReLU(\n","          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (attention2): Attention(\n","          (attention): Identity()\n","        )\n","      )\n","      (x_3_3): DecoderBlock(\n","        (conv1): Conv2dReLU(\n","          (0): Conv2d(320, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (attention1): Attention(\n","          (attention): Identity()\n","        )\n","        (conv2): Conv2dReLU(\n","          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (attention2): Attention(\n","          (attention): Identity()\n","        )\n","      )\n","      (x_0_4): DecoderBlock(\n","        (conv1): Conv2dReLU(\n","          (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (attention1): Attention(\n","          (attention): Identity()\n","        )\n","        (conv2): Conv2dReLU(\n","          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","          (2): ReLU(inplace=True)\n","        )\n","        (attention2): Attention(\n","          (attention): Identity()\n","        )\n","      )\n","    )\n","  )\n","  (segmentation_head): SegmentationHead(\n","    (0): Conv2d(16, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): Identity()\n","    (2): Activation(\n","      (activation): Identity()\n","    )\n","  )\n",")"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["# model 초기화\n","import segmentation_models_pytorch as smp\n","\n","lr = 1e-4\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","num_epochs = 21\n","\n","num_classes = 2\n","\n","# 전이 학습에 사용할 미리 학습된 DeepLabV3Plus 모델 가져오기\n","model2 = smp.UnetPlusPlus(encoder_name=\"resnet101\", encoder_weights=\"imagenet\", in_channels=3, classes=num_classes)\n","\n","\n","# 모델을 GPU로 이동\n","model2.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FvK536yZGkiX"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","\n","test_df = pd.read_csv('./test.csv')\n","test_df1, test_df2 = train_test_split(test_df, test_size=0.5, random_state=0)\n","\n","test_df1.to_csv('./test_df1.csv')\n","test_df2.to_csv('./test_df2.csv')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sKt-I8NVHifi"},"outputs":[],"source":["test_df1 = pd.read_csv('./test_df1.csv')\n","test_df2 = pd.read_csv('./test_df2.csv')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":630},"executionInfo":{"elapsed":241658,"status":"error","timestamp":1689848145594,"user":{"displayName":"조영민","userId":"10749653391039175177"},"user_tz":-540},"id":"6rW8hv0FyGjk","outputId":"256f8c74-fe67-4f8a-8f14-f208b61c242c"},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/1895 [04:00\u003c?, ?it/s]\n"]},{"ename":"error","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-25-f3bb0e552f24\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 10\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 14\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1178\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 633\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1343\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1344\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1345\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1371\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1372\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 644\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31merror\u001b[0m: Caught error in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in \u003clistcomp\u003e\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"\u003cipython-input-14-55d3dff92454\u003e\", line 13, in __getitem__\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\ncv2.error: OpenCV(4.7.0) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n\n"]}],"source":["v3plus_weight = torch.load(\"./weights/v3plus2_21.pth\")\n","model.load_state_dict(v3plus_weight, strict=False)\n","\n","unetplus_weight = torch.load(\"./weights/unetplus_21.pth\")\n","model2.load_state_dict(unetplus_weight, strict=False)\n","\n","test_dataset = SatelliteDataset(csv_file='./test.csv', transform=test_transform, infer=True)\n","test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n","\n","with torch.no_grad():\n","    model.eval()\n","    model2.eval()\n","    result = []\n","    for images in tqdm(test_dataloader):\n","        images = images.float().to(device)\n","\n","        outputs1 = model(images)\n","        masks1 = torch.sigmoid(outputs1).cpu().numpy()\n","\n","        outputs2 = model2(images)\n","        masks2 = torch.sigmoid(outputs2).cpu().numpy()\n","\n","        # 예측 결과를 보팅 결과에 추가\n","        voting_results = []\n","        for i in range(len(images)):\n","            mask1 = (masks1[i] \u003e 0.35).astype(np.uint8)  # 임계값 = 0.35\n","            mask2 = (masks2[i] \u003e 0.35).astype(np.uint8)  # 임계값 = 0.35\n","\n","            # 두 모델의 예측을 모두 사용하여 보팅을 수행\n","            combined_votes = mask1 + mask2\n","            final_mask = (combined_votes \u003e= 2).astype(np.uint8)  # 두 모델이 동일하게 예측한 부분만 선택\n","            mask_rle = rle_encode(final_mask)\n","            result.append(mask_rle)\n","\n","submit = pd.read_csv('./sample_submission_song.csv')\n","submit['mask_rle'] = result\n","submit.to_csv(\"submission_ensemble.csv\", index=False)\n"]},{"cell_type":"markdown","metadata":{"id":"oRLBDaZ1H0it"},"source":["test_df1 돌리기\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":630},"executionInfo":{"elapsed":1239,"status":"error","timestamp":1689847737650,"user":{"displayName":"조영민","userId":"10749653391039175177"},"user_tz":-540},"id":"uggDrdg1ZUgv","outputId":"1e18e078-116f-4b14-ff65-23da852dc910"},"outputs":[{"name":"stderr","output_type":"stream","text":["  0%|          | 0/948 [00:00\u003c?, ?it/s]\n"]},{"ename":"error","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-24-1dfb874f92f4\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 10\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 14\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1178\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 633\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1343\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1344\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1345\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1371\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1372\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    642\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 644\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31merror\u001b[0m: Caught error in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 51, in \u003clistcomp\u003e\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"\u003cipython-input-14-55d3dff92454\u003e\", line 13, in __getitem__\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\ncv2.error: OpenCV(4.7.0) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n\n"]}],"source":["v3plus_weight = torch.load(\"./weights/v3plus2_21.pth\")\n","model.load_state_dict(v3plus_weight, strict=False)\n","\n","unetplus_weight = torch.load(\"./weights/unetplus_21.pth\")\n","model2.load_state_dict(unetplus_weight, strict=False)\n","\n","test_dataset = SatelliteDataset(csv_file='./test_df1.csv', transform=test_transform, infer=True)\n","test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n","\n","with torch.no_grad():\n","    model.eval()\n","    model2.eval()\n","    result = []\n","    for images in tqdm(test_dataloader):\n","        images = images.float().to(device)\n","\n","        outputs1 = model(images)\n","        masks1 = torch.sigmoid(outputs1).cpu().numpy()\n","\n","        outputs2 = model2(images)\n","        masks2 = torch.sigmoid(outputs2).cpu().numpy()\n","\n","        # 예측 결과를 보팅 결과에 추가\n","        voting_results = []\n","        for i in range(len(images)):\n","            mask1 = (masks1[i] \u003e 0.35).astype(np.uint8)  # 임계값 = 0.35\n","            mask2 = (masks2[i] \u003e 0.35).astype(np.uint8)  # 임계값 = 0.35\n","\n","            # 두 모델의 예측을 모두 사용하여 보팅을 수행\n","            combined_votes = mask1 + mask2\n","            final_mask = (combined_votes \u003e= 2).astype(np.uint8)  # 두 모델이 동일하게 예측한 부분만 선택\n","            mask_rle = rle_encode(final_mask)\n","            result.append(mask_rle)\n","\n","submit = pd.read_csv('./sample_submission_song.csv')\n","submit['mask_rle'] = result\n","#submit.to_csv(\"submission_ensemble.csv\", index=False)\n"]},{"cell_type":"markdown","metadata":{"id":"FsmUnEmwIO5T"},"source":["test_df2 돌리기"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P3wZtp5JaxFo"},"outputs":[],"source":["v3plus_weight = torch.load(\"./weights/v3plus2_21.pth\")\n","model.load_state_dict(v3plus_weight, strict=False)\n","\n","unetplus_weight = torch.load(\"./weights/unetplus_21.pth\")\n","model2.load_state_dict(unetplus_weight, strict=False)\n","\n","test_dataset = SatelliteDataset(csv_file='./test_df2.csv', transform=test_transform, infer=True)\n","test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n","\n","with torch.no_grad():\n","    model.eval()\n","    model2.eval()\n","    result = []\n","    for images in tqdm(test_dataloader):\n","        images = images.float().to(device)\n","\n","        outputs1 = model(images)\n","        masks1 = torch.sigmoid(outputs1).cpu().numpy()\n","\n","        outputs2 = model2(images)\n","        masks2 = torch.sigmoid(outputs2).cpu().numpy()\n","\n","        # 예측 결과를 보팅 결과에 추가\n","        voting_results = []\n","        for i in range(len(images)):\n","            mask1 = (masks1[i] \u003e 0.35).astype(np.uint8)  # 임계값 = 0.35\n","            mask2 = (masks2[i] \u003e 0.35).astype(np.uint8)  # 임계값 = 0.35\n","\n","            # 두 모델의 예측을 모두 사용하여 보팅을 수행\n","            combined_votes = mask1 + mask2\n","            final_mask = (combined_votes \u003e= 2).astype(np.uint8)  # 두 모델이 동일하게 예측한 부분만 선택\n","            mask_rle = rle_encode(final_mask)\n","            result.append(mask_rle)\n","\n","submit2 = pd.read_csv('./sample_submission_song.csv')\n","submit2['mask_rle'] = result\n","#submit.to_csv(\"submission_ensemble.csv\", index=False)\n"]},{"cell_type":"markdown","metadata":{"id":"lCfDB_7ir1ke"},"source":["# 다시 도전!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-Ys6ItaHr0zs"},"outputs":[],"source":["def rotate_augmentation(img, label):\n","    # rotate 0, 90, 180, or 270 degrees with 25% probability for each\n","    img = tf.image.rot90(img, tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32, seed=1111))\n","    return img, label\n","\n","def flip_augmentation(img, label):\n","    # flip with 50% probability for left-right and up-down\n","    img = tf.image.random_flip_left_right(img, seed=2222)\n","    img = tf.image.random_flip_up_down(img, seed=3333)\n","    return img, label"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["ieaEZlCGXwhS","EFUSJ285X4ne","NbJe2msPZSHA","oguoJriVJDRc"],"machine_shape":"hm","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}