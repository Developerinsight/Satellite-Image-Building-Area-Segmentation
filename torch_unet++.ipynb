{"cells":[{"cell_type":"markdown","metadata":{"id":"D5y90GLVXafO"},"source":["#  install"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":244,"status":"ok","timestamp":1704927521766,"user":{"displayName":"조영민","userId":"10749653391039175177"},"user_tz":-540},"id":"98fVt3W9W9Cg"},"outputs":[],"source":["import os\n","import cv2\n","import pandas as pd\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","\n","from tqdm import tqdm\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","\n","import segmentation_models_pytorch as smp\n","from sklearn.model_selection import KFold\n","from copy import deepcopy"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":12447,"status":"ok","timestamp":1704927535407,"user":{"displayName":"조영민","userId":"10749653391039175177"},"user_tz":-540},"id":"1J33A8-Jlw3i"},"outputs":[],"source":["!pip install -q -U segmentation-models-pytorch albumentations > /dev/null"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3385,"status":"ok","timestamp":1704927538785,"user":{"displayName":"조영민","userId":"10749653391039175177"},"user_tz":-540},"id":"gYR8dMIJYIV4","outputId":"ab65a37d-a90b-4e30-9862-b69b3c2b7319"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","/content/gdrive/MyDrive/sw_contest\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","%cd \"/content/gdrive/MyDrive/sw_contest\""]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7963,"status":"ok","timestamp":1704927546745,"user":{"displayName":"조영민","userId":"10749653391039175177"},"user_tz":-540},"id":"XS-Ucq5oZSOP","outputId":"184b93f5-73f4-4fc5-b031-4472fca97884"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.9.0.80)\n","Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.23.5)\n"]}],"source":["!pip install --upgrade opencv-python"]},{"cell_type":"markdown","metadata":{"id":"20x7C0trXd_p"},"source":["# Utils"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1704924086311,"user":{"displayName":"조영민","userId":"10749653391039175177"},"user_tz":-540},"id":"qZyppwFOXZxA"},"outputs":[],"source":["# RLE 디코딩 함수\n","def rle_decode(mask_rle, shape):\n","    s = mask_rle.split()\n","    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n","    starts -= 1\n","    ends = starts + lengths\n","    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n","    for lo, hi in zip(starts, ends):\n","        img[lo:hi] = 1\n","    return img.reshape(shape)\n","\n","# RLE 인코딩 함수\n","def rle_encode(mask):\n","    pixels = mask.flatten()\n","    pixels = np.concatenate([[0], pixels, [0]])\n","    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n","    runs[1::2] -= runs[::2]\n","    return ' '.join(str(x) for x in runs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yEycg-ju2pvW"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"qThRcQtbXhpr"},"source":["# Custom Dataset"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1704924086311,"user":{"displayName":"조영민","userId":"10749653391039175177"},"user_tz":-540},"id":"1-tXvlsMXaLV"},"outputs":[],"source":["class SatelliteDataset(Dataset):\n","    def __init__(self, csv_file, transform=None, infer=False):\n","        self.data = pd.read_csv(csv_file)\n","        self.transform = transform\n","        self.infer = infer\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.data.iloc[idx, 1]\n","        image = cv2.imread(img_path)\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","        if self.infer:\n","            if self.transform:\n","                image = self.transform(image=image)['image']\n","            return image\n","\n","        mask_rle = self.data.iloc[idx, 2]\n","        mask = rle_decode(mask_rle, (image.shape[0], image.shape[1]))\n","\n","        if self.transform:\n","\n","            augmented = self.transform(image=image, mask=mask)\n","            image = augmented['image']\n","            mask = augmented['mask']\n","\n","        return image, mask"]},{"cell_type":"markdown","metadata":{"id":"R7L-fVpTXnk8"},"source":["# Data augmentation\n","\n","* 우승팀 참조: https://github.com/drivendataorg/open-cities-ai-challenge/blob/main/1st%20Place/src/datasets/transforms.py\n","\n","* 3단계에 걸쳐 new dataset 생성. 이전 단계에 만들어진 new dataset은 원본 데이터와 합쳐짐.\n","따라서 각 단계 및 모델마다 적용되는 augmentation 다름\n","\n","* 여기서는 4개의 train augmentation 함수 생성"]},{"cell_type":"markdown","source":["Augmentation code"],"metadata":{"id":"-wcNywrLwK7-"}},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":455,"status":"ok","timestamp":1704924086761,"user":{"displayName":"조영민","userId":"10749653391039175177"},"user_tz":-540},"id":"8o17PL9OWcIP","outputId":"57457830-8698-4ea1-c0fa-04159196b262"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/albumentations/augmentations/transforms.py:1692: UserWarning: Using default interpolation INTER_NEAREST, which is sub-optimal.Please specify interpolation mode for downscale and upscale explicitly.For additional information see this PR https://github.com/albumentations-team/albumentations/pull/584\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/albumentations/imgaug/transforms.py:231: FutureWarning: IAASharpen is deprecated. Please use Sharpen instead\n","  warnings.warn(\"IAASharpen is deprecated. Please use Sharpen instead\", FutureWarning)\n","/usr/local/lib/python3.10/dist-packages/albumentations/imgaug/transforms.py:384: FutureWarning: This IAAPerspective is deprecated. Please use Perspective instead\n","  warnings.warn(\"This IAAPerspective is deprecated. Please use Perspective instead\", FutureWarning)\n"]}],"source":["def train_transform_1():\n","  return A.Compose([\n","    A.RandomCrop(512, 512, p=1.),\n","    A.Flip(p=0.75),\n","    A.RandomBrightnessContrast(p=0.5),\n","    A.Normalize(),\n","    ToTensorV2(transpose_mask=True)\n","])\n","\n","\n","def train_transform_2():\n","  return A.Compose([\n","\n","    A.RandomScale(scale_limit=0.3, p=0.5),\n","    A.PadIfNeeded(768, 768, p=1),\n","    A.RandomCrop(768, 768, p=1.),\n","    A.Flip(p=0.75),\n","    A.Downscale(scale_min=0.5, scale_max=0.75, p=0.05),\n","\n","    # color transforms\n","    A.OneOf(\n","        [\n","            A.RandomBrightnessContrast(p=1),\n","            A.RandomGamma(p=1),\n","            A.ChannelShuffle(p=0.2),\n","            A.HueSaturationValue(p=1),\n","            A.RGBShift(p=1),\n","        ],\n","        p=0.5,\n","    ),\n","\n","    # noise transforms\n","    A.OneOf(\n","        [\n","            A.GaussNoise(p=1),\n","            A.MultiplicativeNoise(p=1),\n","            A.IAASharpen(p=1),\n","            # A.ImageCompression(quality_lower=0.7, p=1),\n","            A.GaussianBlur(p=1),\n","        ],\n","        p=0.2,\n","    ),\n","    A.Normalize(),\n","    ToTensorV2(transpose_mask=True)\n","])\n","\n","def train_transform_3():\n","  return A.Compose([\n","          A.RandomScale(scale_limit=0.3, p=0.5),\n","          A.PadIfNeeded(1024, 1024, p=1),\n","          A.RandomCrop(1024, 1024, p=1.),\n","          A.Flip(p=0.75),\n","          A.Downscale(scale_min=0.5, scale_max=0.75, p=0.05),\n","\n","          # color transforms\n","          A.OneOf(\n","              [\n","                  A.RandomBrightnessContrast(p=1),\n","                  A.RandomGamma(p=1),\n","                  A.ChannelShuffle(p=0.2),\n","                  A.HueSaturationValue(p=1),\n","                  A.RGBShift(p=1),\n","              ],\n","              p=0.5,\n","          ),\n","\n","          # noise transforms\n","          A.OneOf(\n","              [\n","                  A.GaussNoise(p=1),\n","                  A.MultiplicativeNoise(p=1),\n","                  A.IAASharpen(p=1),\n","                  # A.ImageCompression(quality_lower=0.7, p=1),\n","                  A.GaussianBlur(p=1),\n","              ],\n","              p=0.2,\n","          ),\n","          A.Normalize(),\n","          ToTensorV2(transpose_mask=True)\n","      ])\n","\n","def train_transform_4():\n","  return A.Compose([\n","          A.ShiftScaleRotate(scale_limit=0.2, rotate_limit=45, border_mode=0, value=0, p=0.7),\n","          A.PadIfNeeded(768, 768, border_mode=0, value=0, p=1.),\n","          A.RandomCrop(768, 768, p=1.),\n","          A.Flip(p=0.75),\n","          A.Downscale(scale_min=0.5, scale_max=0.75, p=0.05),\n","          A.MaskDropout(max_objects=3, image_fill_value=0, mask_fill_value=0, p=0.1),\n","\n","          # color transforms\n","          A.OneOf(\n","              [\n","                  A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=1),\n","                  A.RandomGamma(gamma_limit=(70, 130), p=1),\n","                  A.ChannelShuffle(p=0.2),\n","                  A.HueSaturationValue(hue_shift_limit=30, sat_shift_limit=40, val_shift_limit=30, p=1),\n","                  A.RGBShift(r_shift_limit=30, g_shift_limit=30, b_shift_limit=30, p=1),\n","              ],\n","              p=0.8,\n","          ),\n","\n","          # distortion\n","          A.OneOf(\n","              [\n","                  A.ElasticTransform(p=1),\n","                  A.OpticalDistortion(p=1),\n","                  A.GridDistortion(p=1),\n","                  A.IAAPerspective(p=1),\n","              ],\n","              p=0.2,\n","          ),\n","\n","          # noise transforms\n","          A.OneOf(\n","              [\n","                  A.GaussNoise(p=1),\n","                  A.MultiplicativeNoise(p=1),\n","                  A.IAASharpen(p=1),\n","                  A.GaussianBlur(p=1),\n","              ],\n","              p=0.2,\n","          ),\n","          A.Normalize(),\n","          ToTensorV2(transpose_mask=True)\n","       ])\n","\n","def train_transform():\n","        return A.Compose(\n","            [\n","                A.Normalize(),\n","                ToTensorV2(transpose_mask=True)\n","            ]\n","        )\n","\n","def valid_transform():\n","        return A.Compose(\n","            [\n","                A.Normalize(),\n","                ToTensorV2(transpose_mask=True)\n","            ]\n","        )\n","\n","def test_transform():\n","        return A.Compose(\n","            [\n","                A.Normalize(),\n","                ToTensorV2(transpose_mask=True)\n","            ]\n","        )\n","train_transform = train_transform()\n","train_transform_1 = train_transform_1()\n","train_transform_2 = train_transform_2()\n","train_transform_3 = train_transform_3()\n","train_transform_4 = train_transform_4()\n","\n","valid_transform = valid_transform()\n","test_transform = test_transform()\n"]},{"cell_type":"markdown","source":["Augmentation Data"],"metadata":{"id":"x2xETFxhwW3z"}},{"cell_type":"code","source":["## train_data transform\n","# origin - no augmentation\n","dataset = SatelliteDataset(csv_file='./train.csv', transform=train_transform)\n","\n","# transform 1\n","dataset1 = SatelliteDataset(csv_file='./train.csv', transform=train_transform_1)\n","#dataloader1 = DataLoader(dataset1, batch_size=8, shuffle=True, num_workers=4)\n","\n","# transform 2\n","dataset2 = SatelliteDataset(csv_file='./train.csv', transform=train_transform_2)\n","#dataloader2 = DataLoader(dataset2, batch_size=8, shuffle=True, num_workers=4)\n","\n","# transform 3\n","dataset3 = SatelliteDataset(csv_file='./train.csv', transform=train_transform_3)\n","#dataloader3 = DataLoader(dataset3, batch_size=8, shuffle=True, num_workers=4)\n","\n","# transform 4\n","dataset4 = SatelliteDataset(csv_file='./train.csv', transform=train_transform_4)\n","#dataloader4 = DataLoader(dataset4, batch_size=8, shuffle=True, num_workers=4)\n","\n","## test_data transform\n","test_dataset = SatelliteDataset(csv_file='./test.csv', transform=test_transform, infer=True)\n","test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=1)"],"metadata":{"id":"DhzsP8ULwIz6","executionInfo":{"status":"ok","timestamp":1704924106320,"user_tz":-540,"elapsed":19561,"user":{"displayName":"조영민","userId":"10749653391039175177"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ieaEZlCGXwhS"},"source":["# Model Train\n","* Segmentation mdeol: https://github.com/qubvel/segmentation_models.pytorch/tree/master/segmentation_models_pytorch/encoders\n","\n","* Winner model: https://github.com/drivendataorg/open-cities-ai-challenge/tree/main/1st%20Place"]},{"cell_type":"markdown","source":["* Whole Pipeline of 3 stages:\n"," 1. Training 10 Unet++ models on that data. make prediction with ensemble of 10 models.\n"," 2. taka prediction from 1 and prepare as train_data. train 10 unet++ models with train_data and stage 1 data and prediction.\n"," 3. taka prediction from 2 and prepare as train_data. train 10 unet++ models with train_data and stage 2 data and prediction."],"metadata":{"id":"b8g5jS9ErBke"}},{"cell_type":"markdown","source":["## Stage 1\n","\n","* Step 1 - prepraring train data\n","* Step 2 - training 10 Unet++ models on that data\n","* Step 3 - make prediction with ensemble of 10 models"],"metadata":{"id":"8Yv1vnO0sFg4"}},{"cell_type":"markdown","source":["stage1-effb7-f0~4 -> make 5 models"],"metadata":{"id":"-yRpLgwqvtbe"}},{"cell_type":"code","source":["# KFold 인스턴스 생성\n","kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","\n","# 데이터셋 길이\n","dataset_size = len(dataset4)\n","indices = list(range(dataset_size))\n","\n","# 초기 설정\n","lr = 0.0001\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","num_epochs = 50\n","num_classes = 1\n","\n","# 손실 함수와 옵티마이저 정의\n","# 이진 분류를 위한 BCEWithLogitsLoss (Sigmoid 포함)\n","criterion = torch.nn.BCEWithLogitsLoss()\n","\n","# 교차 검증 루프\n","for fold, (train_idx, val_idx) in enumerate(kf.split(indices)):\n","    print(f'Fold {fold+1}')\n","\n","    train_subsample = torch.utils.data.SubsetRandomSampler(train_idx)\n","    val_subsample = torch.utils.data.SubsetRandomSampler(val_idx)\n","\n","    train_loader = DataLoader(dataset4, batch_size=8, sampler=train_subsample, num_workers=4)\n","    val_loader = DataLoader(dataset4, batch_size=8, sampler=val_subsample, num_workers=4)\n","\n","    # 모델 초기화 및 최적화(각 폴드마다)\n","    model = smp.UnetPlusPlus(encoder_name = \"efficientnet-b7\", encoder_weights='imagenet', in_channels=3, classes=num_classes)\n","    model.to(device)\n","    optimizer = torch.optim.Adam(model.parameters(), lr)\n","\n","    # 최고 모델 저장 위한 변수 초기화\n","    best_model_wts = deepcopy(model.state_dict())\n","    lowest_loss = float('inf')\n","\n","    for epoch in range(num_epochs):\n","      model.train() # 모델 훈련 모드로 설정\n","      train_loss = 0\n","      for images, masks in tqdm(train_loader):\n","          images = images.to(device)\n","          masks = masks.to(device).float()\n","\n","          optimizer.zero_grad() # gradient 초기화\n","          outputs = model(images)\n","          outputs = outputs.squeeze(1)\n","          loss = criterion(outputs, masks)\n","          loss.backward() # 역전파\n","          optimizer.step() # 가중치 업데이트\n","\n","          train_loss += loss.item() # 에폭별 총 손실 계산\n","      train_loss /= len(train_loader) # 평균 손실 계산\n","\n","      model.eval() # 모델을 평가 모드로 설정\n","      val_loss = 0\n","\n","      with torch.no_grad(): # 그레디언트 계산 비활성화\n","        for images, masks in val_loader:\n","          images = images.to(device)\n","          masks = masks.to(device).float()\n","\n","          outputs = model(images)\n","          outputs = outputs.squeeze(1)\n","          loss = criterion(outputs, masks)\n","\n","          val_loss += loss.item()\n","      val_loss /= len(val_loader)\n","\n","      if val_loss < lowest_loss:\n","        lowest_loss = val_loss\n","        best_model_wts = deepcopy(model.state_dict())\n","\n","    torch.save(best_model_wts, f'./weights/stage1-effb7-f{fold}-best.pth')"],"metadata":{"id":"Wwh3UT20vl7i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["stage1-srx101-f0~4 -> make 5 models"],"metadata":{"id":"WZVk4efP4M81"}},{"cell_type":"code","source":["import torch\n","from tqdm import tqdm\n","import segmentation_models_pytorch as smp\n","from sklearn.model_selection import KFold\n","from copy import deepcopy\n","\n","# KFold 인스턴스 생성\n","kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","\n","# 데이터셋 길이\n","dataset_size = len(dataset2)\n","indices = list(range(dataset_size))\n","\n","# 초기 설정\n","lr = 0.0001\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","num_epochs = 50\n","num_classes = 1\n","\n","# 손실 함수와 옵티마이저 정의\n","# 이진 분류를 위한 BCEWithLogitsLoss (Sigmoid 포함)\n","criterion = torch.nn.BCEWithLogitsLoss()\n","\n","# 교차 검증 루프\n","for fold, (train_idx, val_idx) in enumerate(kf.split(indices)):\n","    print(f'Fold {fold+1}')\n","\n","    train_subsample = torch.utils.data.SubsetRandomSampler(train_idx)\n","    val_subsample = torch.utils.data.SubsetRandomSampler(val_idx)\n","\n","    train_loader = DataLoader(dataset2, batch_size=8, sampler=train_subsample, num_workers=4)\n","    val_loader = DataLoader(dataset2, batch_size=8, sampler=val_subsample, num_workers=4)\n","\n","    # 모델 초기화 및 최적화(각 폴드마다)\n","    model = smp.UnetPlusPlus(encoder_name = \"se_resnext101_32x4d\", encoder_weights='imagenet', in_channels=3, classes=num_classes)\n","    model.to(device)\n","    optimizer = torch.optim.Adam(model.parameters(), lr)\n","\n","    # 최고 모델 저장 위한 변수 초기화\n","    best_model_wts = deepcopy(model.state_dict())\n","    lowest_loss = float('inf')\n","\n","    for epoch in range(num_epochs):\n","      model.train() # 모델 훈련 모드로 설정\n","      train_loss = 0\n","      for images, masks in tqdm(train_loader):\n","          images = images.to(device)\n","          masks = masks.to(device).float()\n","\n","          optimizer.zero_grad() # gradient 초기화\n","          outputs = model(images)\n","          outputs = outputs.squeeze(1)\n","          loss = criterion(outputs, masks)\n","          loss.backward() # 역전파\n","          optimizer.step() # 가중치 업데이트\n","\n","          train_loss += loss.item() # 에폭별 총 손실 계산\n","      train_loss /= len(train_loader) # 평균 손실 계산\n","\n","      model.eval() # 모델을 평가 모드로 설정\n","      val_loss = 0\n","\n","      with torch.no_grad(): # 그레디언트 계산 비활성화\n","        for images, masks in val_loader:\n","          images = images.to(device)\n","          masks = masks.to(device).float()\n","\n","          outputs = model(images)\n","          outputs = outputs.squeeze(1)\n","          loss = criterion(outputs, masks)\n","\n","          val_loss += loss.item()\n","      val_loss /= len(val_loader)\n","\n","      if val_loss < lowest_loss:\n","        lowest_loss = val_loss\n","        best_model_wts = deepcopy(model.state_dict())\n","\n","    torch.save(best_model_wts, f'./weights/stage1-srx101-f{fold}-best.pth')"],"metadata":{"id":"MqC5Ayk8y3Tw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Load weights & Ensemble & Prediction  "],"metadata":{"id":"Ta1Bvxpg7IoA"}},{"cell_type":"code","source":["# 가중치 로드\n","weight = torch.load('./weights/stage1-effb7-f0-best.pth')\n","model0 = smp.UnetPlusPlus(encoder_name=\"efficientnet-b7\", encoder_weights=None, in_channels=3, classes=num_classes)\n","model0.to(device)\n","model0.load_state_dict(weight, strict=False)\n","\n","weight = torch.load('./weights/stage1-effb7-f1-best.pth')\n","model1 = smp.UnetPlusPlus(encoder_name=\"efficientnet-b7\", encoder_weights=None, in_channels=3, classes=num_classes)\n","model1.to(device)\n","model1.load_state_dict(weight, strict=False)\n","\n","weight = torch.load('./weights/stage1-effb7-f2-best.pth')\n","model2 = smp.UnetPlusPlus(encoder_name=\"efficientnet-b7\", encoder_weights=None, in_channels=3, classes=num_classes)\n","model2.to(device)\n","model2.load_state_dict(weight, strict=False)\n","\n","weight = torch.load('./weights/stage1-effb7-f3-best.pth')\n","model3 = smp.UnetPlusPlus(encoder_name=\"efficientnet-b7\", encoder_weights=None, in_channels=3, classes=num_classes)\n","model3.to(device)\n","model3.load_state_dict(weight, strict=False)\n","\n","weight = torch.load('./weights/stage1-effb7-f4-best.pth')\n","model4 = smp.UnetPlusPlus(encoder_name=\"efficientnet-b7\", encoder_weights=None, in_channels=3, classes=num_classes)\n","model4.to(device)\n","model4.load_state_dict(weight, strict=False)\n","\n","\n","\n","weight = torch.load('./weights/stage1-srx101-f0-best.pth')\n","model5 = smp.UnetPlusPlus(encoder_name=\"se_resnext101_32x4d\", encoder_weights=None, in_channels=3, classes=num_classes)\n","model5.to(device)\n","model5.load_state_dict(weight, strict=False)\n","\n","weight = torch.load('./weights/stage1-srx101-f1-best.pth')\n","model6 = smp.UnetPlusPlus(encoder_name=\"se_resnext101_32x4d\", encoder_weights=None, in_channels=3, classes=num_classes)\n","model6.to(device)\n","model6.load_state_dict(weight, strict=False)\n","\n","weight = torch.load('./weights/stage1-srx101-f2-best.pth')\n","model7 = smp.UnetPlusPlus(encoder_name=\"se_resnext101_32x4d\", encoder_weights=None, in_channels=3, classes=num_classes)\n","model7.to(device)\n","model7.load_state_dict(weight, strict=False)\n","\n","weight = torch.load('./weights/stage1-srx101-f3-best.pth')\n","model8 = smp.UnetPlusPlus(encoder_name=\"se_resnext101_32x4d\", encoder_weights=None, in_channels=3, classes=num_classes)\n","model8.to(device)\n","model8.load_state_dict(weight, strict=False)\n","\n","weight = torch.load('./weights/stage1-srx101-f4-best.pth')\n","model9 = smp.UnetPlusPlus(encoder_name=\"se_resnext101_32x4d\", encoder_weights=None, in_channels=3, classes=num_classes)\n","model9.to(device)\n","model9.load_state_dict(weight, strict=False)\n"],"metadata":{"id":"Fht5y15ty3QU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 1단계: 예측 수행\n","final_predictions = []\n","with torch.no_grad():\n","    for images in tqdm(test_dataloader):\n","        images = images.float().to(device)\n","        masks = [model(images) for model in [model0, model1, model2, model3, model4, model5, model6, model7, model8, model9]]\n","        masks = [torch.sigmoid(mask).cpu().numpy() for mask in masks]\n","        masks = [np.squeeze(mask, axis=1) for mask in masks]\n","        averaged_mask = np.mean(masks, axis=0)\n","        final_mask = (averaged_mask > 0.5).astype(np.uint8)\n","        final_predictions.extend(final_mask)\n","\n","# 2단계: RLE 인코딩 적용\n","encoded_masks = [rle_encode(mask) for mask in final_predictions]\n","\n","# 3단계: 새로운 데이터셋 생성\n","stage1_dataset = []\n","for i, image in enumerate(test_dataset):\n","    # 원본 이미지를 가져옵니다 (변환 적용된 상태)\n","    original_image = image[0]\n","\n","    # 해당 이미지의 예측된 마스크(의사 레이블)를 가져옵니다\n","    predicted_mask = encoded_masks[i]\n","\n","    # 원본 이미지와 예측된 마스크를 쌍으로 새 데이터셋에 추가합니다\n","    stage1_dataset.append((original_image, predicted_mask))\n"],"metadata":{"id":"COQMlxYgy3OP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Stage2\n","\n","* Step 1 - take prediction from stage 1 and prepare as train data\n","* Step 2 - train 10 Unet++ models with train_data and stage_2 data\n","* Step 3 - make predictions with new models"],"metadata":{"id":"VCmW1EPa_0D0"}},{"cell_type":"markdown","source":["stage2-effb7-f0~4 -> make 5 models"],"metadata":{"id":"Gbxzi9KBaOsg"}},{"cell_type":"code","source":["import torch\n","from tqdm import tqdm\n","import segmentation_models_pytorch as smp\n","from sklearn.model_selection import KFold\n","from copy import deepcopy\n","\n","# KFold 인스턴스 생성\n","kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","\n","# 데이터셋 길이\n","dataset_size = len(dataset4 + stage1_dataset)\n","indices = list(range(dataset_size))\n","\n","# 초기 설정\n","lr = 0.0001\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","num_epochs = 50\n","num_classes = 1\n","\n","# 손실 함수와 옵티마이저 정의\n","# 이진 분류를 위한 BCEWithLogitsLoss (Sigmoid 포함)\n","criterion = torch.nn.BCEWithLogitsLoss()\n","\n","# 교차 검증 루프\n","for fold, (train_idx, val_idx) in enumerate(kf.split(indices)):\n","    print(f'Fold {fold+1}')\n","\n","    train_subsample = torch.utils.data.SubsetRandomSampler(train_idx)\n","    val_subsample = torch.utils.data.SubsetRandomSampler(val_idx)\n","\n","    train_loader = DataLoader(dataset4 + stage1_dataset, batch_size=8, sampler=train_subsample, num_workers=4)\n","    val_loader = DataLoader(dataset4 + stage1_dataset, batch_size=8, sampler=val_subsample, num_workers=4)\n","\n","    # 모델 초기화 및 최적화(각 폴드마다)\n","    model = smp.UnetPlusPlus(encoder_name = \"efficientnet-b7\", encoder_weights='imagenet', in_channels=3, classes=num_classes)\n","    model.to(device)\n","    optimizer = torch.optim.Adam(model.parameters(), lr)\n","\n","    # 최고 모델 저장 위한 변수 초기화\n","    best_model_wts = deepcopy(model.state_dict())\n","    lowest_loss = float('inf')\n","\n","    for epoch in range(num_epochs):\n","      model.train() # 모델 훈련 모드로 설정\n","      train_loss = 0\n","      for images, masks in tqdm(train_loader):\n","          images = images.to(device)\n","          masks = masks.to(device).float()\n","\n","          optimizer.zero_grad() # gradient 초기화\n","          outputs = model(images)\n","          outputs = outputs.squeeze(1)\n","          loss = criterion(outputs, masks)\n","          loss.backward() # 역전파\n","          optimizer.step() # 가중치 업데이트\n","\n","          train_loss += loss.item() # 에폭별 총 손실 계산\n","      train_loss /= len(train_loader) # 평균 손실 계산\n","\n","      model.eval() # 모델을 평가 모드로 설정\n","      val_loss = 0\n","\n","      with torch.no_grad(): # 그레디언트 계산 비활성화\n","        for images, masks in val_loader:\n","          images = images.to(device)\n","          masks = masks.to(device).float()\n","\n","          outputs = model(images)\n","          outputs = outputs.squeeze(1)\n","          loss = criterion(outputs, masks)\n","\n","          val_loss += loss.item()\n","      val_loss /= len(val_loader)\n","\n","      if val_loss < lowest_loss:\n","        lowest_loss = val_loss\n","        best_model_wts = deepcopy(model.state_dict())\n","\n","    torch.save(best_model_wts, f'./weights/stage2-effb7-f{fold}-best.pth')"],"metadata":{"id":"VHt9ny_O_4h2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["stage2-srx101-f0~4 -> make 5 models"],"metadata":{"id":"I8EzNrUPaWK3"}},{"cell_type":"code","source":["import torch\n","from tqdm import tqdm\n","import segmentation_models_pytorch as smp\n","from sklearn.model_selection import KFold\n","from copy import deepcopy\n","\n","# KFold 인스턴스 생성\n","kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","\n","# 데이터셋 길이\n","dataset_size = len(dataset2 + stage1_dataset)\n","indices = list(range(dataset_size))\n","\n","# 초기 설정\n","lr = 0.0001\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","num_epochs = 50\n","num_classes = 1\n","\n","# 손실 함수와 옵티마이저 정의\n","# 이진 분류를 위한 BCEWithLogitsLoss (Sigmoid 포함)\n","criterion = torch.nn.BCEWithLogitsLoss()\n","\n","# 교차 검증 루프\n","for fold, (train_idx, val_idx) in enumerate(kf.split(indices)):\n","    print(f'Fold {fold+1}')\n","\n","    train_subsample = torch.utils.data.SubsetRandomSampler(train_idx)\n","    val_subsample = torch.utils.data.SubsetRandomSampler(val_idx)\n","\n","    train_loader = DataLoader(dataset2 + stage1_dataset, batch_size=8, sampler=train_subsample, num_workers=4)\n","    val_loader = DataLoader(dataset2 + stage1_dataset, batch_size=8, sampler=val_subsample, num_workers=4)\n","\n","    # 모델 초기화 및 최적화(각 폴드마다)\n","    model = smp.UnetPlusPlus(encoder_name = \"se_resnext101_32x4d\", encoder_weights='imagenet', in_channels=3, classes=num_classes)\n","    model.to(device)\n","    optimizer = torch.optim.Adam(model.parameters(), lr)\n","\n","    # 최고 모델 저장 위한 변수 초기화\n","    best_model_wts = deepcopy(model.state_dict())\n","    lowest_loss = float('inf')\n","\n","    for epoch in range(num_epochs):\n","      model.train() # 모델 훈련 모드로 설정\n","      train_loss = 0\n","      for images, masks in tqdm(train_loader):\n","          images = images.to(device)\n","          masks = masks.to(device).float()\n","\n","          optimizer.zero_grad() # gradient 초기화\n","          outputs = model(images)\n","          outputs = outputs.squeeze(1)\n","          loss = criterion(outputs, masks)\n","          loss.backward() # 역전파\n","          optimizer.step() # 가중치 업데이트\n","\n","          train_loss += loss.item() # 에폭별 총 손실 계산\n","      train_loss /= len(train_loader) # 평균 손실 계산\n","\n","      model.eval() # 모델을 평가 모드로 설정\n","      val_loss = 0\n","\n","      with torch.no_grad(): # 그레디언트 계산 비활성화\n","        for images, masks in val_loader:\n","          images = images.to(device)\n","          masks = masks.to(device).float()\n","\n","          outputs = model(images)\n","          outputs = outputs.squeeze(1)\n","          loss = criterion(outputs, masks)\n","\n","          val_loss += loss.item()\n","      val_loss /= len(val_loader)\n","\n","      if val_loss < lowest_loss:\n","        lowest_loss = val_loss\n","        best_model_wts = deepcopy(model.state_dict())\n","\n","    torch.save(best_model_wts, f'./weights/stage2-srx101-f{fold}-best.pth')"],"metadata":{"id":"QMOHlz8T_4fv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Load weights & Ensemble & Prediction  "],"metadata":{"id":"YaknGDdjac8n"}},{"cell_type":"code","source":["# 가중치 로드\n","weight = torch.load('./weights/stage2-effb7-f0-best.pth')\n","model0 = smp.UnetPlusPlus(encoder_name=\"efficientnet-b7\", encoder_weights=None, in_channels=3, classes=num_classes)\n","model0.to(device)\n","model0.load_state_dict(weight, strict=False)\n","\n","weight = torch.load('./weights/stage2-effb7-f1-best.pth')\n","model1 = smp.UnetPlusPlus(encoder_name=\"efficientnet-b7\", encoder_weights=None, in_channels=3, classes=num_classes)\n","model1.to(device)\n","model1.load_state_dict(weight, strict=False)\n","\n","weight = torch.load('./weights/stage2-effb7-f2-best.pth')\n","model2 = smp.UnetPlusPlus(encoder_name=\"efficientnet-b7\", encoder_weights=None, in_channels=3, classes=num_classes)\n","model2.to(device)\n","model2.load_state_dict(weight, strict=False)\n","\n","weight = torch.load('./weights/stage2-effb7-f3-best.pth')\n","model3 = smp.UnetPlusPlus(encoder_name=\"efficientnet-b7\", encoder_weights=None, in_channels=3, classes=num_classes)\n","model3.to(device)\n","model3.load_state_dict(weight, strict=False)\n","\n","weight = torch.load('./weights/stage2-effb7-f4-best.pth')\n","model4 = smp.UnetPlusPlus(encoder_name=\"efficientnet-b7\", encoder_weights=None, in_channels=3, classes=num_classes)\n","model4.to(device)\n","model4.load_state_dict(weight, strict=False)\n","\n","\n","\n","weight = torch.load('./weights/stage2-srx101-f0-best.pth')\n","model5 = smp.UnetPlusPlus(encoder_name=\"se_resnext101_32x4d\", encoder_weights=None, in_channels=3, classes=num_classes)\n","model5.to(device)\n","model5.load_state_dict(weight, strict=False)\n","\n","weight = torch.load('./weights/stage2-srx101-f1-best.pth')\n","model6 = smp.UnetPlusPlus(encoder_name=\"se_resnext101_32x4d\", encoder_weights=None, in_channels=3, classes=num_classes)\n","model6.to(device)\n","model6.load_state_dict(weight, strict=False)\n","\n","weight = torch.load('./weights/stage2-srx101-f2-best.pth')\n","model7 = smp.UnetPlusPlus(encoder_name=\"se_resnext101_32x4d\", encoder_weights=None, in_channels=3, classes=num_classes)\n","model7.to(device)\n","model7.load_state_dict(weight, strict=False)\n","\n","weight = torch.load('./weights/stage2-srx101-f3-best.pth')\n","model8 = smp.UnetPlusPlus(encoder_name=\"se_resnext101_32x4d\", encoder_weights=None, in_channels=3, classes=num_classes)\n","model8.to(device)\n","model8.load_state_dict(weight, strict=False)\n","\n","weight = torch.load('./weights/stage2-srx101-f4-best.pth')\n","model9 = smp.UnetPlusPlus(encoder_name=\"se_resnext101_32x4d\", encoder_weights=None, in_channels=3, classes=num_classes)\n","model9.to(device)\n","model9.load_state_dict(weight, strict=False)\n"],"metadata":{"id":"fKStbOZKFqRS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 1단계: 예측 수행\n","final_predictions = []\n","with torch.no_grad():\n","    for images in tqdm(test_dataloader):\n","        images = images.float().to(device)\n","        masks = [model(images) for model in [model0, model1, model2, model3, model4, model5, model6, model7, model8, model9]]\n","        masks = [torch.sigmoid(mask).cpu().numpy() for mask in masks]\n","        masks = [np.squeeze(mask, axis=1) for mask in masks]\n","        averaged_mask = np.mean(masks, axis=0)\n","        final_mask = (averaged_mask > 0.5).astype(np.uint8)\n","        final_predictions.extend(final_mask)\n","\n","# 2단계: RLE 인코딩 적용\n","encoded_masks = [rle_encode(mask) for mask in final_predictions]\n","\n","# 3단계: 새로운 데이터셋 생성\n","stage2_dataset = []\n","for i, image in enumerate(test_dataset):\n","    # 원본 이미지를 가져옵니다 (변환 적용된 상태)\n","    original_image = image[0]\n","\n","    # 해당 이미지의 예측된 마스크(의사 레이블)를 가져옵니다\n","    predicted_mask = encoded_masks[i]\n","\n","    # 원본 이미지와 예측된 마스크를 쌍으로 새 데이터셋에 추가합니다\n","    stage2_dataset.append((original_image, predicted_mask))\n"],"metadata":{"id":"O-6mByWUFiRo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## stage3\n","\n","* Step 1 - take prediction from stage 2 and prepare as train data\n","* Step 2 - train 5 Unet++ models with train data and stage_3 data\n","* Step 3 - make prediction with new models"],"metadata":{"id":"hNlX8cZfGAzy"}},{"cell_type":"markdown","source":["stage3-effb7-f0~4"],"metadata":{"id":"uRC54qSDaR9S"}},{"cell_type":"code","source":["import torch\n","from tqdm import tqdm\n","import segmentation_models_pytorch as smp\n","from sklearn.model_selection import KFold\n","from copy import deepcopy\n","\n","# KFold 인스턴스 생성\n","kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","\n","# 데이터셋 길이\n","dataset_size = len(dataset4 + stage2_dataset)\n","indices = list(range(dataset_size))\n","\n","# 초기 설정\n","lr = 0.0001\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","num_epochs = 50\n","num_classes = 1\n","\n","# 손실 함수와 옵티마이저 정의\n","# 이진 분류를 위한 BCEWithLogitsLoss (Sigmoid 포함)\n","criterion = torch.nn.BCEWithLogitsLoss()\n","\n","# 교차 검증 루프\n","for fold, (train_idx, val_idx) in enumerate(kf.split(indices)):\n","    print(f'Fold {fold+1}')\n","\n","    train_subsample = torch.utils.data.SubsetRandomSampler(train_idx)\n","    val_subsample = torch.utils.data.SubsetRandomSampler(val_idx)\n","\n","    train_loader = DataLoader(dataset4 + stage2_dataset, batch_size=8, sampler=train_subsample, num_workers=4)\n","    val_loader = DataLoader(dataset4 + stage2_dataset, batch_size=8, sampler=val_subsample, num_workers=4)\n","\n","    # 모델 초기화 및 최적화(각 폴드마다)\n","    model = smp.UnetPlusPlus(encoder_name = \"efficientnet-b7\", encoder_weights='imagenet', in_channels=3, classes=num_classes)\n","    model.to(device)\n","    optimizer = torch.optim.Adam(model.parameters(), lr)\n","\n","    # 최고 모델 저장 위한 변수 초기화\n","    best_model_wts = deepcopy(model.state_dict())\n","    lowest_loss = float('inf')\n","\n","    for epoch in range(num_epochs):\n","      model.train() # 모델 훈련 모드로 설정\n","      train_loss = 0\n","      for images, masks in tqdm(train_loader):\n","          images = images.to(device)\n","          masks = masks.to(device).float()\n","\n","          optimizer.zero_grad() # gradient 초기화\n","          outputs = model(images)\n","          outputs = outputs.squeeze(1)\n","          loss = criterion(outputs, masks)\n","          loss.backward() # 역전파\n","          optimizer.step() # 가중치 업데이트\n","\n","          train_loss += loss.item() # 에폭별 총 손실 계산\n","      train_loss /= len(train_loader) # 평균 손실 계산\n","\n","      model.eval() # 모델을 평가 모드로 설정\n","      val_loss = 0\n","\n","      with torch.no_grad(): # 그레디언트 계산 비활성화\n","        for images, masks in val_loader:\n","          images = images.to(device)\n","          masks = masks.to(device).float()\n","\n","          outputs = model(images)\n","          outputs = outputs.squeeze(1)\n","          loss = criterion(outputs, masks)\n","\n","          val_loss += loss.item()\n","      val_loss /= len(val_loader)\n","\n","      if val_loss < lowest_loss:\n","        lowest_loss = val_loss\n","        best_model_wts = deepcopy(model.state_dict())\n","\n","    torch.save(best_model_wts, f'./weights/stage3-effb7-f{fold}-best.pth')"],"metadata":{"id":"177U4gjFGCiJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["stage3-srx101-f0~4"],"metadata":{"id":"eKsAdP0XaYWQ"}},{"cell_type":"code","source":["import torch\n","from tqdm import tqdm\n","import segmentation_models_pytorch as smp\n","from sklearn.model_selection import KFold\n","from copy import deepcopy\n","\n","# KFold 인스턴스 생성\n","kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","\n","# 데이터셋 길이\n","dataset_size = len(dataset2 + stage2_dataset)\n","indices = list(range(dataset_size))\n","\n","# 초기 설정\n","lr = 0.0001\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","num_epochs = 50\n","num_classes = 1\n","\n","# 손실 함수와 옵티마이저 정의\n","# 이진 분류를 위한 BCEWithLogitsLoss (Sigmoid 포함)\n","criterion = torch.nn.BCEWithLogitsLoss()\n","\n","# 교차 검증 루프\n","for fold, (train_idx, val_idx) in enumerate(kf.split(indices)):\n","    print(f'Fold {fold+1}')\n","\n","    train_subsample = torch.utils.data.SubsetRandomSampler(train_idx)\n","    val_subsample = torch.utils.data.SubsetRandomSampler(val_idx)\n","\n","    train_loader = DataLoader(dataset2 + stage2_dataset, batch_size=8, sampler=train_subsample, num_workers=4)\n","    val_loader = DataLoader(dataset2 + stage2_dataset, batch_size=8, sampler=val_subsample, num_workers=4)\n","\n","    # 모델 초기화 및 최적화(각 폴드마다)\n","    model = smp.UnetPlusPlus(encoder_name = \"se_resnext101_32x4d\", encoder_weights='imagenet', in_channels=3, classes=num_classes)\n","    model.to(device)\n","    optimizer = torch.optim.Adam(model.parameters(), lr)\n","\n","    # 최고 모델 저장 위한 변수 초기화\n","    best_model_wts = deepcopy(model.state_dict())\n","    lowest_loss = float('inf')\n","\n","    for epoch in range(num_epochs):\n","      model.train() # 모델 훈련 모드로 설정\n","      train_loss = 0\n","      for images, masks in tqdm(train_loader):\n","          images = images.to(device)\n","          masks = masks.to(device).float()\n","\n","          optimizer.zero_grad() # gradient 초기화\n","          outputs = model(images)\n","          outputs = outputs.squeeze(1)\n","          loss = criterion(outputs, masks)\n","          loss.backward() # 역전파\n","          optimizer.step() # 가중치 업데이트\n","\n","          train_loss += loss.item() # 에폭별 총 손실 계산\n","      train_loss /= len(train_loader) # 평균 손실 계산\n","\n","      model.eval() # 모델을 평가 모드로 설정\n","      val_loss = 0\n","\n","      with torch.no_grad(): # 그레디언트 계산 비활성화\n","        for images, masks in val_loader:\n","          images = images.to(device)\n","          masks = masks.to(device).float()\n","\n","          outputs = model(images)\n","          outputs = outputs.squeeze(1)\n","          loss = criterion(outputs, masks)\n","\n","          val_loss += loss.item()\n","      val_loss /= len(val_loader)\n","\n","      if val_loss < lowest_loss:\n","        lowest_loss = val_loss\n","        best_model_wts = deepcopy(model.state_dict())\n","\n","    torch.save(best_model_wts, f'./weights/stage3-srx101-f{fold}-best.pth')"],"metadata":{"id":"ZvYx4RsjGCep"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["stage3-inrv2-f0"],"metadata":{"id":"S4RjId9RagZd"}},{"cell_type":"code","source":["# SSL 인증서가 유효하지 않을 때 특히 임시로 문제를 해결하기 위한 방법으로 사용됩니다.\n","# 그러나 보안상의 이유로 인해 일반적으로 권장되지 않습니다.\n","\n","import ssl\n","ssl._create_default_https_context = ssl._create_unverified_context"],"metadata":{"id":"iIw1TEjva1ge"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from tqdm import tqdm\n","import segmentation_models_pytorch as smp\n","from sklearn.model_selection import KFold\n","from copy import deepcopy\n","\n","# KFold 인스턴스 생성\n","kf = KFold(n_splits=1, shuffle=True, random_state=42)\n","\n","# 데이터셋 길이\n","dataset_size = len(dataset2 + stage2_dataset)\n","indices = list(range(dataset_size))\n","\n","# 초기 설정\n","lr = 0.0001\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","num_epochs = 50\n","num_classes = 1\n","\n","# 손실 함수와 옵티마이저 정의\n","# 이진 분류를 위한 BCEWithLogitsLoss (Sigmoid 포함)\n","criterion = torch.nn.BCEWithLogitsLoss()\n","\n","# 교차 검증 루프\n","for fold, (train_idx, val_idx) in enumerate(kf.split(indices)):\n","    print(f'Fold {fold+1}')\n","\n","    train_subsample = torch.utils.data.SubsetRandomSampler(train_idx)\n","    val_subsample = torch.utils.data.SubsetRandomSampler(val_idx)\n","\n","    train_loader = DataLoader(dataset2 + stage2_dataset, batch_size=8, sampler=train_subsample, num_workers=4)\n","    val_loader = DataLoader(dataset2 + stage2_dataset, batch_size=8, sampler=val_subsample, num_workers=4)\n","\n","    # 모델 초기화 및 최적화(각 폴드마다)\n","    model = smp.UnetPlusPlus(encoder_name = \"inceptionresnetv2\", encoder_weights='imagenet', in_channels=3, classes=1)\n","    model.to(device)\n","    optimizer = torch.optim.Adam(model.parameters(), lr)\n","\n","    # 최고 모델 저장 위한 변수 초기화\n","    best_model_wts = deepcopy(model.state_dict())\n","    lowest_loss = float('inf')\n","\n","    for epoch in range(num_epochs):\n","      model.train() # 모델 훈련 모드로 설정\n","      train_loss = 0\n","      for images, masks in tqdm(train_loader):\n","          images = images.to(device)\n","          masks = masks.to(device).float()\n","\n","          optimizer.zero_grad() # gradient 초기화\n","          outputs = model(images)\n","          outputs = outputs.squeeze(1)\n","          loss = criterion(outputs, masks)\n","          loss.backward() # 역전파\n","          optimizer.step() # 가중치 업데이트\n","\n","          train_loss += loss.item() # 에폭별 총 손실 계산\n","      train_loss /= len(train_loader) # 평균 손실 계산\n","\n","      model.eval() # 모델을 평가 모드로 설정\n","      val_loss = 0\n","\n","      with torch.no_grad(): # 그레디언트 계산 비활성화\n","        for images, masks in val_loader:\n","          images = images.to(device)\n","          masks = masks.to(device).float()\n","\n","          outputs = model(images)\n","          outputs = outputs.squeeze(1)\n","          loss = criterion(outputs, masks)\n","\n","          val_loss += loss.item()\n","      val_loss /= len(val_loader)\n","\n","      if val_loss < lowest_loss:\n","        lowest_loss = val_loss\n","        best_model_wts = deepcopy(model.state_dict())\n","\n","    torch.save(best_model_wts, f'./weights/stage3-inrv2-f{fold}-best.pth')"],"metadata":{"id":"j9zcc7p2PDIn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Load weights & Ensemble & Prediction  "],"metadata":{"id":"VaMOl6WfbVcJ"}},{"cell_type":"code","source":["# 가중치 로드\n","weight = torch.load('./weights/stage3-effb7-f0-best.pth')\n","model0 = smp.UnetPlusPlus(encoder_name=\"efficientnet-b7\", encoder_weights=None, in_channels=3, classes=num_classes)\n","model0.to(device)\n","model0.load_state_dict(weight, strict=False)\n","\n","weight = torch.load('./weights/stage3-effb7-f5-best.pth')\n","model1 = smp.UnetPlusPlus(encoder_name=\"efficientnet-b7\", encoder_weights=None, in_channels=3, classes=num_classes)\n","model1.to(device)\n","model1.load_state_dict(weight, strict=False)\n","\n","\n","weight = torch.load('./weights/stage3-inrv2-f0-best.pth')\n","model2 = smp.UnetPlusPlus(encoder_name=\"inceptionresnetv2\", encoder_weights=None, in_channels=3, classes=num_classes)\n","model2.to(device)\n","model2.load_state_dict(weight, strict=False)\n","\n","weight = torch.load('./weights/stage3-srx101-f0-best.pth')\n","model3 = smp.UnetPlusPlus(encoder_name=\"se_resnext101_32x4d\", encoder_weights=None, in_channels=3, classes=num_classes)\n","model3.to(device)\n","model3.load_state_dict(weight, strict=False)\n","\n","weight = torch.load('./weights/stage3-srx101-f5-best.pth')\n","model4 = smp.UnetPlusPlus(encoder_name=\"se_resnext101_32x4d\", encoder_weights=None, in_channels=3, classes=num_classes)\n","model4.to(device)\n","model4.load_state_dict(weight, strict=False)\n"],"metadata":{"id":"sulAC5WmbU0B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_dataset = SatelliteDataset(csv_file='./test.csv', transform=test_transform, infer=True)\n","test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=2)\n","\n","model0.eval()\n","model1.eval()\n","model2.eval()\n","model3.eval()\n","model4.eval()\n","\n","\n","with torch.no_grad():\n","    result = []\n","    for images in tqdm(test_dataloader):\n","        images = images.float().to(device)\n","        masks = [model(images) for model in [model0, model1, model2, model3, model4]]\n","        masks = [torch.sigmoid(mask).cpu().numpy() for mask in masks]\n","        masks = [np.squeeze(mask, axis=1) for mask in masks]\n","        averaged_mask = np.mean(masks, axis=0)\n","        final_predictions = (averaged_mask > 0.5).astype(np.uint8)\n","\n","        for i in range(len(images)):\n","          mask_rle = rle_encode(final_predictions[i])\n","          if mask_rle == '': # 예측된 건물 픽셀이 없는 경우 -1\n","              result.append(-1)\n","          else:\n","              result.append(mask_rle)\n","\n","\n","\n"],"metadata":{"id":"xdK0cg3ubbJH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["submit = pd.read_csv('./sample_submission.csv')\n","submit['mask_rle'] = result\n","submit.to_csv(\"./submission/stage1-3.csv\", index=False)\n"],"metadata":{"id":"gg3JmuhAc9AK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4Zv_64V9GyMb"},"source":["# Pretrained Model load in Github\n","* last stage 3 weights"]},{"cell_type":"markdown","source":["## load model"],"metadata":{"id":"vBH8lipliehc"}},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":718,"status":"ok","timestamp":1704927576515,"user":{"displayName":"조영민","userId":"10749653391039175177"},"user_tz":-540},"id":"tPBqX8_nG1Dk","outputId":"d33012c6-1639-4988-d0f0-6062ec3f17ee"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["_IncompatibleKeys(missing_keys=['encoder._conv_stem.weight', 'encoder._bn0.weight', 'encoder._bn0.bias', 'encoder._bn0.running_mean', 'encoder._bn0.running_var', 'encoder._blocks.0._depthwise_conv.weight', 'encoder._blocks.0._bn1.weight', 'encoder._blocks.0._bn1.bias', 'encoder._blocks.0._bn1.running_mean', 'encoder._blocks.0._bn1.running_var', 'encoder._blocks.0._se_reduce.weight', 'encoder._blocks.0._se_reduce.bias', 'encoder._blocks.0._se_expand.weight', 'encoder._blocks.0._se_expand.bias', 'encoder._blocks.0._project_conv.weight', 'encoder._blocks.0._bn2.weight', 'encoder._blocks.0._bn2.bias', 'encoder._blocks.0._bn2.running_mean', 'encoder._blocks.0._bn2.running_var', 'encoder._blocks.1._depthwise_conv.weight', 'encoder._blocks.1._bn1.weight', 'encoder._blocks.1._bn1.bias', 'encoder._blocks.1._bn1.running_mean', 'encoder._blocks.1._bn1.running_var', 'encoder._blocks.1._se_reduce.weight', 'encoder._blocks.1._se_reduce.bias', 'encoder._blocks.1._se_expand.weight', 'encoder._blocks.1._se_expand.bias', 'encoder._blocks.1._project_conv.weight', 'encoder._blocks.1._bn2.weight', 'encoder._blocks.1._bn2.bias', 'encoder._blocks.1._bn2.running_mean', 'encoder._blocks.1._bn2.running_var', 'encoder._blocks.2._expand_conv.weight', 'encoder._blocks.2._bn0.weight', 'encoder._blocks.2._bn0.bias', 'encoder._blocks.2._bn0.running_mean', 'encoder._blocks.2._bn0.running_var', 'encoder._blocks.2._depthwise_conv.weight', 'encoder._blocks.2._bn1.weight', 'encoder._blocks.2._bn1.bias', 'encoder._blocks.2._bn1.running_mean', 'encoder._blocks.2._bn1.running_var', 'encoder._blocks.2._se_reduce.weight', 'encoder._blocks.2._se_reduce.bias', 'encoder._blocks.2._se_expand.weight', 'encoder._blocks.2._se_expand.bias', 'encoder._blocks.2._project_conv.weight', 'encoder._blocks.2._bn2.weight', 'encoder._blocks.2._bn2.bias', 'encoder._blocks.2._bn2.running_mean', 'encoder._blocks.2._bn2.running_var', 'encoder._blocks.3._expand_conv.weight', 'encoder._blocks.3._bn0.weight', 'encoder._blocks.3._bn0.bias', 'encoder._blocks.3._bn0.running_mean', 'encoder._blocks.3._bn0.running_var', 'encoder._blocks.3._depthwise_conv.weight', 'encoder._blocks.3._bn1.weight', 'encoder._blocks.3._bn1.bias', 'encoder._blocks.3._bn1.running_mean', 'encoder._blocks.3._bn1.running_var', 'encoder._blocks.3._se_reduce.weight', 'encoder._blocks.3._se_reduce.bias', 'encoder._blocks.3._se_expand.weight', 'encoder._blocks.3._se_expand.bias', 'encoder._blocks.3._project_conv.weight', 'encoder._blocks.3._bn2.weight', 'encoder._blocks.3._bn2.bias', 'encoder._blocks.3._bn2.running_mean', 'encoder._blocks.3._bn2.running_var', 'encoder._blocks.4._expand_conv.weight', 'encoder._blocks.4._bn0.weight', 'encoder._blocks.4._bn0.bias', 'encoder._blocks.4._bn0.running_mean', 'encoder._blocks.4._bn0.running_var', 'encoder._blocks.4._depthwise_conv.weight', 'encoder._blocks.4._bn1.weight', 'encoder._blocks.4._bn1.bias', 'encoder._blocks.4._bn1.running_mean', 'encoder._blocks.4._bn1.running_var', 'encoder._blocks.4._se_reduce.weight', 'encoder._blocks.4._se_reduce.bias', 'encoder._blocks.4._se_expand.weight', 'encoder._blocks.4._se_expand.bias', 'encoder._blocks.4._project_conv.weight', 'encoder._blocks.4._bn2.weight', 'encoder._blocks.4._bn2.bias', 'encoder._blocks.4._bn2.running_mean', 'encoder._blocks.4._bn2.running_var', 'encoder._blocks.5._expand_conv.weight', 'encoder._blocks.5._bn0.weight', 'encoder._blocks.5._bn0.bias', 'encoder._blocks.5._bn0.running_mean', 'encoder._blocks.5._bn0.running_var', 'encoder._blocks.5._depthwise_conv.weight', 'encoder._blocks.5._bn1.weight', 'encoder._blocks.5._bn1.bias', 'encoder._blocks.5._bn1.running_mean', 'encoder._blocks.5._bn1.running_var', 'encoder._blocks.5._se_reduce.weight', 'encoder._blocks.5._se_reduce.bias', 'encoder._blocks.5._se_expand.weight', 'encoder._blocks.5._se_expand.bias', 'encoder._blocks.5._project_conv.weight', 'encoder._blocks.5._bn2.weight', 'encoder._blocks.5._bn2.bias', 'encoder._blocks.5._bn2.running_mean', 'encoder._blocks.5._bn2.running_var', 'encoder._blocks.6._expand_conv.weight', 'encoder._blocks.6._bn0.weight', 'encoder._blocks.6._bn0.bias', 'encoder._blocks.6._bn0.running_mean', 'encoder._blocks.6._bn0.running_var', 'encoder._blocks.6._depthwise_conv.weight', 'encoder._blocks.6._bn1.weight', 'encoder._blocks.6._bn1.bias', 'encoder._blocks.6._bn1.running_mean', 'encoder._blocks.6._bn1.running_var', 'encoder._blocks.6._se_reduce.weight', 'encoder._blocks.6._se_reduce.bias', 'encoder._blocks.6._se_expand.weight', 'encoder._blocks.6._se_expand.bias', 'encoder._blocks.6._project_conv.weight', 'encoder._blocks.6._bn2.weight', 'encoder._blocks.6._bn2.bias', 'encoder._blocks.6._bn2.running_mean', 'encoder._blocks.6._bn2.running_var', 'encoder._blocks.7._expand_conv.weight', 'encoder._blocks.7._bn0.weight', 'encoder._blocks.7._bn0.bias', 'encoder._blocks.7._bn0.running_mean', 'encoder._blocks.7._bn0.running_var', 'encoder._blocks.7._depthwise_conv.weight', 'encoder._blocks.7._bn1.weight', 'encoder._blocks.7._bn1.bias', 'encoder._blocks.7._bn1.running_mean', 'encoder._blocks.7._bn1.running_var', 'encoder._blocks.7._se_reduce.weight', 'encoder._blocks.7._se_reduce.bias', 'encoder._blocks.7._se_expand.weight', 'encoder._blocks.7._se_expand.bias', 'encoder._blocks.7._project_conv.weight', 'encoder._blocks.7._bn2.weight', 'encoder._blocks.7._bn2.bias', 'encoder._blocks.7._bn2.running_mean', 'encoder._blocks.7._bn2.running_var', 'encoder._blocks.8._expand_conv.weight', 'encoder._blocks.8._bn0.weight', 'encoder._blocks.8._bn0.bias', 'encoder._blocks.8._bn0.running_mean', 'encoder._blocks.8._bn0.running_var', 'encoder._blocks.8._depthwise_conv.weight', 'encoder._blocks.8._bn1.weight', 'encoder._blocks.8._bn1.bias', 'encoder._blocks.8._bn1.running_mean', 'encoder._blocks.8._bn1.running_var', 'encoder._blocks.8._se_reduce.weight', 'encoder._blocks.8._se_reduce.bias', 'encoder._blocks.8._se_expand.weight', 'encoder._blocks.8._se_expand.bias', 'encoder._blocks.8._project_conv.weight', 'encoder._blocks.8._bn2.weight', 'encoder._blocks.8._bn2.bias', 'encoder._blocks.8._bn2.running_mean', 'encoder._blocks.8._bn2.running_var', 'encoder._blocks.9._expand_conv.weight', 'encoder._blocks.9._bn0.weight', 'encoder._blocks.9._bn0.bias', 'encoder._blocks.9._bn0.running_mean', 'encoder._blocks.9._bn0.running_var', 'encoder._blocks.9._depthwise_conv.weight', 'encoder._blocks.9._bn1.weight', 'encoder._blocks.9._bn1.bias', 'encoder._blocks.9._bn1.running_mean', 'encoder._blocks.9._bn1.running_var', 'encoder._blocks.9._se_reduce.weight', 'encoder._blocks.9._se_reduce.bias', 'encoder._blocks.9._se_expand.weight', 'encoder._blocks.9._se_expand.bias', 'encoder._blocks.9._project_conv.weight', 'encoder._blocks.9._bn2.weight', 'encoder._blocks.9._bn2.bias', 'encoder._blocks.9._bn2.running_mean', 'encoder._blocks.9._bn2.running_var', 'encoder._blocks.10._expand_conv.weight', 'encoder._blocks.10._bn0.weight', 'encoder._blocks.10._bn0.bias', 'encoder._blocks.10._bn0.running_mean', 'encoder._blocks.10._bn0.running_var', 'encoder._blocks.10._depthwise_conv.weight', 'encoder._blocks.10._bn1.weight', 'encoder._blocks.10._bn1.bias', 'encoder._blocks.10._bn1.running_mean', 'encoder._blocks.10._bn1.running_var', 'encoder._blocks.10._se_reduce.weight', 'encoder._blocks.10._se_reduce.bias', 'encoder._blocks.10._se_expand.weight', 'encoder._blocks.10._se_expand.bias', 'encoder._blocks.10._project_conv.weight', 'encoder._blocks.10._bn2.weight', 'encoder._blocks.10._bn2.bias', 'encoder._blocks.10._bn2.running_mean', 'encoder._blocks.10._bn2.running_var', 'encoder._blocks.11._expand_conv.weight', 'encoder._blocks.11._bn0.weight', 'encoder._blocks.11._bn0.bias', 'encoder._blocks.11._bn0.running_mean', 'encoder._blocks.11._bn0.running_var', 'encoder._blocks.11._depthwise_conv.weight', 'encoder._blocks.11._bn1.weight', 'encoder._blocks.11._bn1.bias', 'encoder._blocks.11._bn1.running_mean', 'encoder._blocks.11._bn1.running_var', 'encoder._blocks.11._se_reduce.weight', 'encoder._blocks.11._se_reduce.bias', 'encoder._blocks.11._se_expand.weight', 'encoder._blocks.11._se_expand.bias', 'encoder._blocks.11._project_conv.weight', 'encoder._blocks.11._bn2.weight', 'encoder._blocks.11._bn2.bias', 'encoder._blocks.11._bn2.running_mean', 'encoder._blocks.11._bn2.running_var', 'encoder._blocks.12._expand_conv.weight', 'encoder._blocks.12._bn0.weight', 'encoder._blocks.12._bn0.bias', 'encoder._blocks.12._bn0.running_mean', 'encoder._blocks.12._bn0.running_var', 'encoder._blocks.12._depthwise_conv.weight', 'encoder._blocks.12._bn1.weight', 'encoder._blocks.12._bn1.bias', 'encoder._blocks.12._bn1.running_mean', 'encoder._blocks.12._bn1.running_var', 'encoder._blocks.12._se_reduce.weight', 'encoder._blocks.12._se_reduce.bias', 'encoder._blocks.12._se_expand.weight', 'encoder._blocks.12._se_expand.bias', 'encoder._blocks.12._project_conv.weight', 'encoder._blocks.12._bn2.weight', 'encoder._blocks.12._bn2.bias', 'encoder._blocks.12._bn2.running_mean', 'encoder._blocks.12._bn2.running_var', 'encoder._blocks.13._expand_conv.weight', 'encoder._blocks.13._bn0.weight', 'encoder._blocks.13._bn0.bias', 'encoder._blocks.13._bn0.running_mean', 'encoder._blocks.13._bn0.running_var', 'encoder._blocks.13._depthwise_conv.weight', 'encoder._blocks.13._bn1.weight', 'encoder._blocks.13._bn1.bias', 'encoder._blocks.13._bn1.running_mean', 'encoder._blocks.13._bn1.running_var', 'encoder._blocks.13._se_reduce.weight', 'encoder._blocks.13._se_reduce.bias', 'encoder._blocks.13._se_expand.weight', 'encoder._blocks.13._se_expand.bias', 'encoder._blocks.13._project_conv.weight', 'encoder._blocks.13._bn2.weight', 'encoder._blocks.13._bn2.bias', 'encoder._blocks.13._bn2.running_mean', 'encoder._blocks.13._bn2.running_var', 'encoder._blocks.14._expand_conv.weight', 'encoder._blocks.14._bn0.weight', 'encoder._blocks.14._bn0.bias', 'encoder._blocks.14._bn0.running_mean', 'encoder._blocks.14._bn0.running_var', 'encoder._blocks.14._depthwise_conv.weight', 'encoder._blocks.14._bn1.weight', 'encoder._blocks.14._bn1.bias', 'encoder._blocks.14._bn1.running_mean', 'encoder._blocks.14._bn1.running_var', 'encoder._blocks.14._se_reduce.weight', 'encoder._blocks.14._se_reduce.bias', 'encoder._blocks.14._se_expand.weight', 'encoder._blocks.14._se_expand.bias', 'encoder._blocks.14._project_conv.weight', 'encoder._blocks.14._bn2.weight', 'encoder._blocks.14._bn2.bias', 'encoder._blocks.14._bn2.running_mean', 'encoder._blocks.14._bn2.running_var', 'encoder._blocks.15._expand_conv.weight', 'encoder._blocks.15._bn0.weight', 'encoder._blocks.15._bn0.bias', 'encoder._blocks.15._bn0.running_mean', 'encoder._blocks.15._bn0.running_var', 'encoder._blocks.15._depthwise_conv.weight', 'encoder._blocks.15._bn1.weight', 'encoder._blocks.15._bn1.bias', 'encoder._blocks.15._bn1.running_mean', 'encoder._blocks.15._bn1.running_var', 'encoder._blocks.15._se_reduce.weight', 'encoder._blocks.15._se_reduce.bias', 'encoder._blocks.15._se_expand.weight', 'encoder._blocks.15._se_expand.bias', 'encoder._blocks.15._project_conv.weight', 'encoder._blocks.15._bn2.weight', 'encoder._blocks.15._bn2.bias', 'encoder._blocks.15._bn2.running_mean', 'encoder._blocks.15._bn2.running_var', 'encoder._blocks.16._expand_conv.weight', 'encoder._blocks.16._bn0.weight', 'encoder._blocks.16._bn0.bias', 'encoder._blocks.16._bn0.running_mean', 'encoder._blocks.16._bn0.running_var', 'encoder._blocks.16._depthwise_conv.weight', 'encoder._blocks.16._bn1.weight', 'encoder._blocks.16._bn1.bias', 'encoder._blocks.16._bn1.running_mean', 'encoder._blocks.16._bn1.running_var', 'encoder._blocks.16._se_reduce.weight', 'encoder._blocks.16._se_reduce.bias', 'encoder._blocks.16._se_expand.weight', 'encoder._blocks.16._se_expand.bias', 'encoder._blocks.16._project_conv.weight', 'encoder._blocks.16._bn2.weight', 'encoder._blocks.16._bn2.bias', 'encoder._blocks.16._bn2.running_mean', 'encoder._blocks.16._bn2.running_var', 'encoder._blocks.17._expand_conv.weight', 'encoder._blocks.17._bn0.weight', 'encoder._blocks.17._bn0.bias', 'encoder._blocks.17._bn0.running_mean', 'encoder._blocks.17._bn0.running_var', 'encoder._blocks.17._depthwise_conv.weight', 'encoder._blocks.17._bn1.weight', 'encoder._blocks.17._bn1.bias', 'encoder._blocks.17._bn1.running_mean', 'encoder._blocks.17._bn1.running_var', 'encoder._blocks.17._se_reduce.weight', 'encoder._blocks.17._se_reduce.bias', 'encoder._blocks.17._se_expand.weight', 'encoder._blocks.17._se_expand.bias', 'encoder._blocks.17._project_conv.weight', 'encoder._blocks.17._bn2.weight', 'encoder._blocks.17._bn2.bias', 'encoder._blocks.17._bn2.running_mean', 'encoder._blocks.17._bn2.running_var', 'encoder._blocks.18._expand_conv.weight', 'encoder._blocks.18._bn0.weight', 'encoder._blocks.18._bn0.bias', 'encoder._blocks.18._bn0.running_mean', 'encoder._blocks.18._bn0.running_var', 'encoder._blocks.18._depthwise_conv.weight', 'encoder._blocks.18._bn1.weight', 'encoder._blocks.18._bn1.bias', 'encoder._blocks.18._bn1.running_mean', 'encoder._blocks.18._bn1.running_var', 'encoder._blocks.18._se_reduce.weight', 'encoder._blocks.18._se_reduce.bias', 'encoder._blocks.18._se_expand.weight', 'encoder._blocks.18._se_expand.bias', 'encoder._blocks.18._project_conv.weight', 'encoder._blocks.18._bn2.weight', 'encoder._blocks.18._bn2.bias', 'encoder._blocks.18._bn2.running_mean', 'encoder._blocks.18._bn2.running_var', 'encoder._blocks.19._expand_conv.weight', 'encoder._blocks.19._bn0.weight', 'encoder._blocks.19._bn0.bias', 'encoder._blocks.19._bn0.running_mean', 'encoder._blocks.19._bn0.running_var', 'encoder._blocks.19._depthwise_conv.weight', 'encoder._blocks.19._bn1.weight', 'encoder._blocks.19._bn1.bias', 'encoder._blocks.19._bn1.running_mean', 'encoder._blocks.19._bn1.running_var', 'encoder._blocks.19._se_reduce.weight', 'encoder._blocks.19._se_reduce.bias', 'encoder._blocks.19._se_expand.weight', 'encoder._blocks.19._se_expand.bias', 'encoder._blocks.19._project_conv.weight', 'encoder._blocks.19._bn2.weight', 'encoder._blocks.19._bn2.bias', 'encoder._blocks.19._bn2.running_mean', 'encoder._blocks.19._bn2.running_var', 'encoder._blocks.20._expand_conv.weight', 'encoder._blocks.20._bn0.weight', 'encoder._blocks.20._bn0.bias', 'encoder._blocks.20._bn0.running_mean', 'encoder._blocks.20._bn0.running_var', 'encoder._blocks.20._depthwise_conv.weight', 'encoder._blocks.20._bn1.weight', 'encoder._blocks.20._bn1.bias', 'encoder._blocks.20._bn1.running_mean', 'encoder._blocks.20._bn1.running_var', 'encoder._blocks.20._se_reduce.weight', 'encoder._blocks.20._se_reduce.bias', 'encoder._blocks.20._se_expand.weight', 'encoder._blocks.20._se_expand.bias', 'encoder._blocks.20._project_conv.weight', 'encoder._blocks.20._bn2.weight', 'encoder._blocks.20._bn2.bias', 'encoder._blocks.20._bn2.running_mean', 'encoder._blocks.20._bn2.running_var', 'encoder._blocks.21._expand_conv.weight', 'encoder._blocks.21._bn0.weight', 'encoder._blocks.21._bn0.bias', 'encoder._blocks.21._bn0.running_mean', 'encoder._blocks.21._bn0.running_var', 'encoder._blocks.21._depthwise_conv.weight', 'encoder._blocks.21._bn1.weight', 'encoder._blocks.21._bn1.bias', 'encoder._blocks.21._bn1.running_mean', 'encoder._blocks.21._bn1.running_var', 'encoder._blocks.21._se_reduce.weight', 'encoder._blocks.21._se_reduce.bias', 'encoder._blocks.21._se_expand.weight', 'encoder._blocks.21._se_expand.bias', 'encoder._blocks.21._project_conv.weight', 'encoder._blocks.21._bn2.weight', 'encoder._blocks.21._bn2.bias', 'encoder._blocks.21._bn2.running_mean', 'encoder._blocks.21._bn2.running_var', 'encoder._blocks.22._expand_conv.weight', 'encoder._blocks.22._bn0.weight', 'encoder._blocks.22._bn0.bias', 'encoder._blocks.22._bn0.running_mean', 'encoder._blocks.22._bn0.running_var', 'encoder._blocks.22._depthwise_conv.weight', 'encoder._blocks.22._bn1.weight', 'encoder._blocks.22._bn1.bias', 'encoder._blocks.22._bn1.running_mean', 'encoder._blocks.22._bn1.running_var', 'encoder._blocks.22._se_reduce.weight', 'encoder._blocks.22._se_reduce.bias', 'encoder._blocks.22._se_expand.weight', 'encoder._blocks.22._se_expand.bias', 'encoder._blocks.22._project_conv.weight', 'encoder._blocks.22._bn2.weight', 'encoder._blocks.22._bn2.bias', 'encoder._blocks.22._bn2.running_mean', 'encoder._blocks.22._bn2.running_var', 'encoder._conv_head.weight', 'encoder._bn1.weight', 'encoder._bn1.bias', 'encoder._bn1.running_mean', 'encoder._bn1.running_var', 'decoder.blocks.x_0_0.conv1.0.weight', 'decoder.blocks.x_0_0.conv1.1.weight', 'decoder.blocks.x_0_0.conv1.1.bias', 'decoder.blocks.x_0_0.conv1.1.running_mean', 'decoder.blocks.x_0_0.conv1.1.running_var', 'decoder.blocks.x_0_0.conv2.0.weight', 'decoder.blocks.x_0_0.conv2.1.weight', 'decoder.blocks.x_0_0.conv2.1.bias', 'decoder.blocks.x_0_0.conv2.1.running_mean', 'decoder.blocks.x_0_0.conv2.1.running_var', 'decoder.blocks.x_0_1.conv1.0.weight', 'decoder.blocks.x_0_1.conv1.1.weight', 'decoder.blocks.x_0_1.conv1.1.bias', 'decoder.blocks.x_0_1.conv1.1.running_mean', 'decoder.blocks.x_0_1.conv1.1.running_var', 'decoder.blocks.x_0_1.conv2.0.weight', 'decoder.blocks.x_0_1.conv2.1.weight', 'decoder.blocks.x_0_1.conv2.1.bias', 'decoder.blocks.x_0_1.conv2.1.running_mean', 'decoder.blocks.x_0_1.conv2.1.running_var', 'decoder.blocks.x_1_1.conv1.0.weight', 'decoder.blocks.x_1_1.conv1.1.weight', 'decoder.blocks.x_1_1.conv1.1.bias', 'decoder.blocks.x_1_1.conv1.1.running_mean', 'decoder.blocks.x_1_1.conv1.1.running_var', 'decoder.blocks.x_1_1.conv2.0.weight', 'decoder.blocks.x_1_1.conv2.1.weight', 'decoder.blocks.x_1_1.conv2.1.bias', 'decoder.blocks.x_1_1.conv2.1.running_mean', 'decoder.blocks.x_1_1.conv2.1.running_var', 'decoder.blocks.x_0_2.conv1.0.weight', 'decoder.blocks.x_0_2.conv1.1.weight', 'decoder.blocks.x_0_2.conv1.1.bias', 'decoder.blocks.x_0_2.conv1.1.running_mean', 'decoder.blocks.x_0_2.conv1.1.running_var', 'decoder.blocks.x_0_2.conv2.0.weight', 'decoder.blocks.x_0_2.conv2.1.weight', 'decoder.blocks.x_0_2.conv2.1.bias', 'decoder.blocks.x_0_2.conv2.1.running_mean', 'decoder.blocks.x_0_2.conv2.1.running_var', 'decoder.blocks.x_1_2.conv1.0.weight', 'decoder.blocks.x_1_2.conv1.1.weight', 'decoder.blocks.x_1_2.conv1.1.bias', 'decoder.blocks.x_1_2.conv1.1.running_mean', 'decoder.blocks.x_1_2.conv1.1.running_var', 'decoder.blocks.x_1_2.conv2.0.weight', 'decoder.blocks.x_1_2.conv2.1.weight', 'decoder.blocks.x_1_2.conv2.1.bias', 'decoder.blocks.x_1_2.conv2.1.running_mean', 'decoder.blocks.x_1_2.conv2.1.running_var', 'decoder.blocks.x_2_2.conv1.0.weight', 'decoder.blocks.x_2_2.conv1.1.weight', 'decoder.blocks.x_2_2.conv1.1.bias', 'decoder.blocks.x_2_2.conv1.1.running_mean', 'decoder.blocks.x_2_2.conv1.1.running_var', 'decoder.blocks.x_2_2.conv2.0.weight', 'decoder.blocks.x_2_2.conv2.1.weight', 'decoder.blocks.x_2_2.conv2.1.bias', 'decoder.blocks.x_2_2.conv2.1.running_mean', 'decoder.blocks.x_2_2.conv2.1.running_var', 'decoder.blocks.x_0_3.conv1.0.weight', 'decoder.blocks.x_0_3.conv1.1.weight', 'decoder.blocks.x_0_3.conv1.1.bias', 'decoder.blocks.x_0_3.conv1.1.running_mean', 'decoder.blocks.x_0_3.conv1.1.running_var', 'decoder.blocks.x_0_3.conv2.0.weight', 'decoder.blocks.x_0_3.conv2.1.weight', 'decoder.blocks.x_0_3.conv2.1.bias', 'decoder.blocks.x_0_3.conv2.1.running_mean', 'decoder.blocks.x_0_3.conv2.1.running_var', 'decoder.blocks.x_1_3.conv1.0.weight', 'decoder.blocks.x_1_3.conv1.1.weight', 'decoder.blocks.x_1_3.conv1.1.bias', 'decoder.blocks.x_1_3.conv1.1.running_mean', 'decoder.blocks.x_1_3.conv1.1.running_var', 'decoder.blocks.x_1_3.conv2.0.weight', 'decoder.blocks.x_1_3.conv2.1.weight', 'decoder.blocks.x_1_3.conv2.1.bias', 'decoder.blocks.x_1_3.conv2.1.running_mean', 'decoder.blocks.x_1_3.conv2.1.running_var', 'decoder.blocks.x_2_3.conv1.0.weight', 'decoder.blocks.x_2_3.conv1.1.weight', 'decoder.blocks.x_2_3.conv1.1.bias', 'decoder.blocks.x_2_3.conv1.1.running_mean', 'decoder.blocks.x_2_3.conv1.1.running_var', 'decoder.blocks.x_2_3.conv2.0.weight', 'decoder.blocks.x_2_3.conv2.1.weight', 'decoder.blocks.x_2_3.conv2.1.bias', 'decoder.blocks.x_2_3.conv2.1.running_mean', 'decoder.blocks.x_2_3.conv2.1.running_var', 'decoder.blocks.x_3_3.conv1.0.weight', 'decoder.blocks.x_3_3.conv1.1.weight', 'decoder.blocks.x_3_3.conv1.1.bias', 'decoder.blocks.x_3_3.conv1.1.running_mean', 'decoder.blocks.x_3_3.conv1.1.running_var', 'decoder.blocks.x_3_3.conv2.0.weight', 'decoder.blocks.x_3_3.conv2.1.weight', 'decoder.blocks.x_3_3.conv2.1.bias', 'decoder.blocks.x_3_3.conv2.1.running_mean', 'decoder.blocks.x_3_3.conv2.1.running_var', 'decoder.blocks.x_0_4.conv1.0.weight', 'decoder.blocks.x_0_4.conv1.1.weight', 'decoder.blocks.x_0_4.conv1.1.bias', 'decoder.blocks.x_0_4.conv1.1.running_mean', 'decoder.blocks.x_0_4.conv1.1.running_var', 'decoder.blocks.x_0_4.conv2.0.weight', 'decoder.blocks.x_0_4.conv2.1.weight', 'decoder.blocks.x_0_4.conv2.1.bias', 'decoder.blocks.x_0_4.conv2.1.running_mean', 'decoder.blocks.x_0_4.conv2.1.running_var', 'segmentation_head.0.weight', 'segmentation_head.0.bias'], unexpected_keys=['state_dict', 'logs', 'epoch'])"]},"metadata":{},"execution_count":23}],"source":["num_classes = 1\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","weight = torch.load('/content/gdrive/MyDrive/sw_contest/pretrained/models/stage3/effb1-f0/checkpoints/best.pth')\n","model1 = smp.UnetPlusPlus(encoder_name=\"efficientnet-b1\", encoder_weights=None, in_channels=3, classes=num_classes)\n","model1.to(device)\n","model1.load_state_dict(weight, strict=False)\n"]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":623,"status":"ok","timestamp":1704927577136,"user":{"displayName":"조영민","userId":"10749653391039175177"},"user_tz":-540},"id":"m8cIXY-rK0ye","outputId":"d0c107b5-b191-4f2e-af61-0076a6867c5c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["_IncompatibleKeys(missing_keys=['encoder._conv_stem.weight', 'encoder._bn0.weight', 'encoder._bn0.bias', 'encoder._bn0.running_mean', 'encoder._bn0.running_var', 'encoder._blocks.0._depthwise_conv.weight', 'encoder._blocks.0._bn1.weight', 'encoder._blocks.0._bn1.bias', 'encoder._blocks.0._bn1.running_mean', 'encoder._blocks.0._bn1.running_var', 'encoder._blocks.0._se_reduce.weight', 'encoder._blocks.0._se_reduce.bias', 'encoder._blocks.0._se_expand.weight', 'encoder._blocks.0._se_expand.bias', 'encoder._blocks.0._project_conv.weight', 'encoder._blocks.0._bn2.weight', 'encoder._blocks.0._bn2.bias', 'encoder._blocks.0._bn2.running_mean', 'encoder._blocks.0._bn2.running_var', 'encoder._blocks.1._depthwise_conv.weight', 'encoder._blocks.1._bn1.weight', 'encoder._blocks.1._bn1.bias', 'encoder._blocks.1._bn1.running_mean', 'encoder._blocks.1._bn1.running_var', 'encoder._blocks.1._se_reduce.weight', 'encoder._blocks.1._se_reduce.bias', 'encoder._blocks.1._se_expand.weight', 'encoder._blocks.1._se_expand.bias', 'encoder._blocks.1._project_conv.weight', 'encoder._blocks.1._bn2.weight', 'encoder._blocks.1._bn2.bias', 'encoder._blocks.1._bn2.running_mean', 'encoder._blocks.1._bn2.running_var', 'encoder._blocks.2._expand_conv.weight', 'encoder._blocks.2._bn0.weight', 'encoder._blocks.2._bn0.bias', 'encoder._blocks.2._bn0.running_mean', 'encoder._blocks.2._bn0.running_var', 'encoder._blocks.2._depthwise_conv.weight', 'encoder._blocks.2._bn1.weight', 'encoder._blocks.2._bn1.bias', 'encoder._blocks.2._bn1.running_mean', 'encoder._blocks.2._bn1.running_var', 'encoder._blocks.2._se_reduce.weight', 'encoder._blocks.2._se_reduce.bias', 'encoder._blocks.2._se_expand.weight', 'encoder._blocks.2._se_expand.bias', 'encoder._blocks.2._project_conv.weight', 'encoder._blocks.2._bn2.weight', 'encoder._blocks.2._bn2.bias', 'encoder._blocks.2._bn2.running_mean', 'encoder._blocks.2._bn2.running_var', 'encoder._blocks.3._expand_conv.weight', 'encoder._blocks.3._bn0.weight', 'encoder._blocks.3._bn0.bias', 'encoder._blocks.3._bn0.running_mean', 'encoder._blocks.3._bn0.running_var', 'encoder._blocks.3._depthwise_conv.weight', 'encoder._blocks.3._bn1.weight', 'encoder._blocks.3._bn1.bias', 'encoder._blocks.3._bn1.running_mean', 'encoder._blocks.3._bn1.running_var', 'encoder._blocks.3._se_reduce.weight', 'encoder._blocks.3._se_reduce.bias', 'encoder._blocks.3._se_expand.weight', 'encoder._blocks.3._se_expand.bias', 'encoder._blocks.3._project_conv.weight', 'encoder._blocks.3._bn2.weight', 'encoder._blocks.3._bn2.bias', 'encoder._blocks.3._bn2.running_mean', 'encoder._blocks.3._bn2.running_var', 'encoder._blocks.4._expand_conv.weight', 'encoder._blocks.4._bn0.weight', 'encoder._blocks.4._bn0.bias', 'encoder._blocks.4._bn0.running_mean', 'encoder._blocks.4._bn0.running_var', 'encoder._blocks.4._depthwise_conv.weight', 'encoder._blocks.4._bn1.weight', 'encoder._blocks.4._bn1.bias', 'encoder._blocks.4._bn1.running_mean', 'encoder._blocks.4._bn1.running_var', 'encoder._blocks.4._se_reduce.weight', 'encoder._blocks.4._se_reduce.bias', 'encoder._blocks.4._se_expand.weight', 'encoder._blocks.4._se_expand.bias', 'encoder._blocks.4._project_conv.weight', 'encoder._blocks.4._bn2.weight', 'encoder._blocks.4._bn2.bias', 'encoder._blocks.4._bn2.running_mean', 'encoder._blocks.4._bn2.running_var', 'encoder._blocks.5._expand_conv.weight', 'encoder._blocks.5._bn0.weight', 'encoder._blocks.5._bn0.bias', 'encoder._blocks.5._bn0.running_mean', 'encoder._blocks.5._bn0.running_var', 'encoder._blocks.5._depthwise_conv.weight', 'encoder._blocks.5._bn1.weight', 'encoder._blocks.5._bn1.bias', 'encoder._blocks.5._bn1.running_mean', 'encoder._blocks.5._bn1.running_var', 'encoder._blocks.5._se_reduce.weight', 'encoder._blocks.5._se_reduce.bias', 'encoder._blocks.5._se_expand.weight', 'encoder._blocks.5._se_expand.bias', 'encoder._blocks.5._project_conv.weight', 'encoder._blocks.5._bn2.weight', 'encoder._blocks.5._bn2.bias', 'encoder._blocks.5._bn2.running_mean', 'encoder._blocks.5._bn2.running_var', 'encoder._blocks.6._expand_conv.weight', 'encoder._blocks.6._bn0.weight', 'encoder._blocks.6._bn0.bias', 'encoder._blocks.6._bn0.running_mean', 'encoder._blocks.6._bn0.running_var', 'encoder._blocks.6._depthwise_conv.weight', 'encoder._blocks.6._bn1.weight', 'encoder._blocks.6._bn1.bias', 'encoder._blocks.6._bn1.running_mean', 'encoder._blocks.6._bn1.running_var', 'encoder._blocks.6._se_reduce.weight', 'encoder._blocks.6._se_reduce.bias', 'encoder._blocks.6._se_expand.weight', 'encoder._blocks.6._se_expand.bias', 'encoder._blocks.6._project_conv.weight', 'encoder._blocks.6._bn2.weight', 'encoder._blocks.6._bn2.bias', 'encoder._blocks.6._bn2.running_mean', 'encoder._blocks.6._bn2.running_var', 'encoder._blocks.7._expand_conv.weight', 'encoder._blocks.7._bn0.weight', 'encoder._blocks.7._bn0.bias', 'encoder._blocks.7._bn0.running_mean', 'encoder._blocks.7._bn0.running_var', 'encoder._blocks.7._depthwise_conv.weight', 'encoder._blocks.7._bn1.weight', 'encoder._blocks.7._bn1.bias', 'encoder._blocks.7._bn1.running_mean', 'encoder._blocks.7._bn1.running_var', 'encoder._blocks.7._se_reduce.weight', 'encoder._blocks.7._se_reduce.bias', 'encoder._blocks.7._se_expand.weight', 'encoder._blocks.7._se_expand.bias', 'encoder._blocks.7._project_conv.weight', 'encoder._blocks.7._bn2.weight', 'encoder._blocks.7._bn2.bias', 'encoder._blocks.7._bn2.running_mean', 'encoder._blocks.7._bn2.running_var', 'encoder._blocks.8._expand_conv.weight', 'encoder._blocks.8._bn0.weight', 'encoder._blocks.8._bn0.bias', 'encoder._blocks.8._bn0.running_mean', 'encoder._blocks.8._bn0.running_var', 'encoder._blocks.8._depthwise_conv.weight', 'encoder._blocks.8._bn1.weight', 'encoder._blocks.8._bn1.bias', 'encoder._blocks.8._bn1.running_mean', 'encoder._blocks.8._bn1.running_var', 'encoder._blocks.8._se_reduce.weight', 'encoder._blocks.8._se_reduce.bias', 'encoder._blocks.8._se_expand.weight', 'encoder._blocks.8._se_expand.bias', 'encoder._blocks.8._project_conv.weight', 'encoder._blocks.8._bn2.weight', 'encoder._blocks.8._bn2.bias', 'encoder._blocks.8._bn2.running_mean', 'encoder._blocks.8._bn2.running_var', 'encoder._blocks.9._expand_conv.weight', 'encoder._blocks.9._bn0.weight', 'encoder._blocks.9._bn0.bias', 'encoder._blocks.9._bn0.running_mean', 'encoder._blocks.9._bn0.running_var', 'encoder._blocks.9._depthwise_conv.weight', 'encoder._blocks.9._bn1.weight', 'encoder._blocks.9._bn1.bias', 'encoder._blocks.9._bn1.running_mean', 'encoder._blocks.9._bn1.running_var', 'encoder._blocks.9._se_reduce.weight', 'encoder._blocks.9._se_reduce.bias', 'encoder._blocks.9._se_expand.weight', 'encoder._blocks.9._se_expand.bias', 'encoder._blocks.9._project_conv.weight', 'encoder._blocks.9._bn2.weight', 'encoder._blocks.9._bn2.bias', 'encoder._blocks.9._bn2.running_mean', 'encoder._blocks.9._bn2.running_var', 'encoder._blocks.10._expand_conv.weight', 'encoder._blocks.10._bn0.weight', 'encoder._blocks.10._bn0.bias', 'encoder._blocks.10._bn0.running_mean', 'encoder._blocks.10._bn0.running_var', 'encoder._blocks.10._depthwise_conv.weight', 'encoder._blocks.10._bn1.weight', 'encoder._blocks.10._bn1.bias', 'encoder._blocks.10._bn1.running_mean', 'encoder._blocks.10._bn1.running_var', 'encoder._blocks.10._se_reduce.weight', 'encoder._blocks.10._se_reduce.bias', 'encoder._blocks.10._se_expand.weight', 'encoder._blocks.10._se_expand.bias', 'encoder._blocks.10._project_conv.weight', 'encoder._blocks.10._bn2.weight', 'encoder._blocks.10._bn2.bias', 'encoder._blocks.10._bn2.running_mean', 'encoder._blocks.10._bn2.running_var', 'encoder._blocks.11._expand_conv.weight', 'encoder._blocks.11._bn0.weight', 'encoder._blocks.11._bn0.bias', 'encoder._blocks.11._bn0.running_mean', 'encoder._blocks.11._bn0.running_var', 'encoder._blocks.11._depthwise_conv.weight', 'encoder._blocks.11._bn1.weight', 'encoder._blocks.11._bn1.bias', 'encoder._blocks.11._bn1.running_mean', 'encoder._blocks.11._bn1.running_var', 'encoder._blocks.11._se_reduce.weight', 'encoder._blocks.11._se_reduce.bias', 'encoder._blocks.11._se_expand.weight', 'encoder._blocks.11._se_expand.bias', 'encoder._blocks.11._project_conv.weight', 'encoder._blocks.11._bn2.weight', 'encoder._blocks.11._bn2.bias', 'encoder._blocks.11._bn2.running_mean', 'encoder._blocks.11._bn2.running_var', 'encoder._blocks.12._expand_conv.weight', 'encoder._blocks.12._bn0.weight', 'encoder._blocks.12._bn0.bias', 'encoder._blocks.12._bn0.running_mean', 'encoder._blocks.12._bn0.running_var', 'encoder._blocks.12._depthwise_conv.weight', 'encoder._blocks.12._bn1.weight', 'encoder._blocks.12._bn1.bias', 'encoder._blocks.12._bn1.running_mean', 'encoder._blocks.12._bn1.running_var', 'encoder._blocks.12._se_reduce.weight', 'encoder._blocks.12._se_reduce.bias', 'encoder._blocks.12._se_expand.weight', 'encoder._blocks.12._se_expand.bias', 'encoder._blocks.12._project_conv.weight', 'encoder._blocks.12._bn2.weight', 'encoder._blocks.12._bn2.bias', 'encoder._blocks.12._bn2.running_mean', 'encoder._blocks.12._bn2.running_var', 'encoder._blocks.13._expand_conv.weight', 'encoder._blocks.13._bn0.weight', 'encoder._blocks.13._bn0.bias', 'encoder._blocks.13._bn0.running_mean', 'encoder._blocks.13._bn0.running_var', 'encoder._blocks.13._depthwise_conv.weight', 'encoder._blocks.13._bn1.weight', 'encoder._blocks.13._bn1.bias', 'encoder._blocks.13._bn1.running_mean', 'encoder._blocks.13._bn1.running_var', 'encoder._blocks.13._se_reduce.weight', 'encoder._blocks.13._se_reduce.bias', 'encoder._blocks.13._se_expand.weight', 'encoder._blocks.13._se_expand.bias', 'encoder._blocks.13._project_conv.weight', 'encoder._blocks.13._bn2.weight', 'encoder._blocks.13._bn2.bias', 'encoder._blocks.13._bn2.running_mean', 'encoder._blocks.13._bn2.running_var', 'encoder._blocks.14._expand_conv.weight', 'encoder._blocks.14._bn0.weight', 'encoder._blocks.14._bn0.bias', 'encoder._blocks.14._bn0.running_mean', 'encoder._blocks.14._bn0.running_var', 'encoder._blocks.14._depthwise_conv.weight', 'encoder._blocks.14._bn1.weight', 'encoder._blocks.14._bn1.bias', 'encoder._blocks.14._bn1.running_mean', 'encoder._blocks.14._bn1.running_var', 'encoder._blocks.14._se_reduce.weight', 'encoder._blocks.14._se_reduce.bias', 'encoder._blocks.14._se_expand.weight', 'encoder._blocks.14._se_expand.bias', 'encoder._blocks.14._project_conv.weight', 'encoder._blocks.14._bn2.weight', 'encoder._blocks.14._bn2.bias', 'encoder._blocks.14._bn2.running_mean', 'encoder._blocks.14._bn2.running_var', 'encoder._blocks.15._expand_conv.weight', 'encoder._blocks.15._bn0.weight', 'encoder._blocks.15._bn0.bias', 'encoder._blocks.15._bn0.running_mean', 'encoder._blocks.15._bn0.running_var', 'encoder._blocks.15._depthwise_conv.weight', 'encoder._blocks.15._bn1.weight', 'encoder._blocks.15._bn1.bias', 'encoder._blocks.15._bn1.running_mean', 'encoder._blocks.15._bn1.running_var', 'encoder._blocks.15._se_reduce.weight', 'encoder._blocks.15._se_reduce.bias', 'encoder._blocks.15._se_expand.weight', 'encoder._blocks.15._se_expand.bias', 'encoder._blocks.15._project_conv.weight', 'encoder._blocks.15._bn2.weight', 'encoder._blocks.15._bn2.bias', 'encoder._blocks.15._bn2.running_mean', 'encoder._blocks.15._bn2.running_var', 'encoder._blocks.16._expand_conv.weight', 'encoder._blocks.16._bn0.weight', 'encoder._blocks.16._bn0.bias', 'encoder._blocks.16._bn0.running_mean', 'encoder._blocks.16._bn0.running_var', 'encoder._blocks.16._depthwise_conv.weight', 'encoder._blocks.16._bn1.weight', 'encoder._blocks.16._bn1.bias', 'encoder._blocks.16._bn1.running_mean', 'encoder._blocks.16._bn1.running_var', 'encoder._blocks.16._se_reduce.weight', 'encoder._blocks.16._se_reduce.bias', 'encoder._blocks.16._se_expand.weight', 'encoder._blocks.16._se_expand.bias', 'encoder._blocks.16._project_conv.weight', 'encoder._blocks.16._bn2.weight', 'encoder._blocks.16._bn2.bias', 'encoder._blocks.16._bn2.running_mean', 'encoder._blocks.16._bn2.running_var', 'encoder._blocks.17._expand_conv.weight', 'encoder._blocks.17._bn0.weight', 'encoder._blocks.17._bn0.bias', 'encoder._blocks.17._bn0.running_mean', 'encoder._blocks.17._bn0.running_var', 'encoder._blocks.17._depthwise_conv.weight', 'encoder._blocks.17._bn1.weight', 'encoder._blocks.17._bn1.bias', 'encoder._blocks.17._bn1.running_mean', 'encoder._blocks.17._bn1.running_var', 'encoder._blocks.17._se_reduce.weight', 'encoder._blocks.17._se_reduce.bias', 'encoder._blocks.17._se_expand.weight', 'encoder._blocks.17._se_expand.bias', 'encoder._blocks.17._project_conv.weight', 'encoder._blocks.17._bn2.weight', 'encoder._blocks.17._bn2.bias', 'encoder._blocks.17._bn2.running_mean', 'encoder._blocks.17._bn2.running_var', 'encoder._blocks.18._expand_conv.weight', 'encoder._blocks.18._bn0.weight', 'encoder._blocks.18._bn0.bias', 'encoder._blocks.18._bn0.running_mean', 'encoder._blocks.18._bn0.running_var', 'encoder._blocks.18._depthwise_conv.weight', 'encoder._blocks.18._bn1.weight', 'encoder._blocks.18._bn1.bias', 'encoder._blocks.18._bn1.running_mean', 'encoder._blocks.18._bn1.running_var', 'encoder._blocks.18._se_reduce.weight', 'encoder._blocks.18._se_reduce.bias', 'encoder._blocks.18._se_expand.weight', 'encoder._blocks.18._se_expand.bias', 'encoder._blocks.18._project_conv.weight', 'encoder._blocks.18._bn2.weight', 'encoder._blocks.18._bn2.bias', 'encoder._blocks.18._bn2.running_mean', 'encoder._blocks.18._bn2.running_var', 'encoder._blocks.19._expand_conv.weight', 'encoder._blocks.19._bn0.weight', 'encoder._blocks.19._bn0.bias', 'encoder._blocks.19._bn0.running_mean', 'encoder._blocks.19._bn0.running_var', 'encoder._blocks.19._depthwise_conv.weight', 'encoder._blocks.19._bn1.weight', 'encoder._blocks.19._bn1.bias', 'encoder._blocks.19._bn1.running_mean', 'encoder._blocks.19._bn1.running_var', 'encoder._blocks.19._se_reduce.weight', 'encoder._blocks.19._se_reduce.bias', 'encoder._blocks.19._se_expand.weight', 'encoder._blocks.19._se_expand.bias', 'encoder._blocks.19._project_conv.weight', 'encoder._blocks.19._bn2.weight', 'encoder._blocks.19._bn2.bias', 'encoder._blocks.19._bn2.running_mean', 'encoder._blocks.19._bn2.running_var', 'encoder._blocks.20._expand_conv.weight', 'encoder._blocks.20._bn0.weight', 'encoder._blocks.20._bn0.bias', 'encoder._blocks.20._bn0.running_mean', 'encoder._blocks.20._bn0.running_var', 'encoder._blocks.20._depthwise_conv.weight', 'encoder._blocks.20._bn1.weight', 'encoder._blocks.20._bn1.bias', 'encoder._blocks.20._bn1.running_mean', 'encoder._blocks.20._bn1.running_var', 'encoder._blocks.20._se_reduce.weight', 'encoder._blocks.20._se_reduce.bias', 'encoder._blocks.20._se_expand.weight', 'encoder._blocks.20._se_expand.bias', 'encoder._blocks.20._project_conv.weight', 'encoder._blocks.20._bn2.weight', 'encoder._blocks.20._bn2.bias', 'encoder._blocks.20._bn2.running_mean', 'encoder._blocks.20._bn2.running_var', 'encoder._blocks.21._expand_conv.weight', 'encoder._blocks.21._bn0.weight', 'encoder._blocks.21._bn0.bias', 'encoder._blocks.21._bn0.running_mean', 'encoder._blocks.21._bn0.running_var', 'encoder._blocks.21._depthwise_conv.weight', 'encoder._blocks.21._bn1.weight', 'encoder._blocks.21._bn1.bias', 'encoder._blocks.21._bn1.running_mean', 'encoder._blocks.21._bn1.running_var', 'encoder._blocks.21._se_reduce.weight', 'encoder._blocks.21._se_reduce.bias', 'encoder._blocks.21._se_expand.weight', 'encoder._blocks.21._se_expand.bias', 'encoder._blocks.21._project_conv.weight', 'encoder._blocks.21._bn2.weight', 'encoder._blocks.21._bn2.bias', 'encoder._blocks.21._bn2.running_mean', 'encoder._blocks.21._bn2.running_var', 'encoder._blocks.22._expand_conv.weight', 'encoder._blocks.22._bn0.weight', 'encoder._blocks.22._bn0.bias', 'encoder._blocks.22._bn0.running_mean', 'encoder._blocks.22._bn0.running_var', 'encoder._blocks.22._depthwise_conv.weight', 'encoder._blocks.22._bn1.weight', 'encoder._blocks.22._bn1.bias', 'encoder._blocks.22._bn1.running_mean', 'encoder._blocks.22._bn1.running_var', 'encoder._blocks.22._se_reduce.weight', 'encoder._blocks.22._se_reduce.bias', 'encoder._blocks.22._se_expand.weight', 'encoder._blocks.22._se_expand.bias', 'encoder._blocks.22._project_conv.weight', 'encoder._blocks.22._bn2.weight', 'encoder._blocks.22._bn2.bias', 'encoder._blocks.22._bn2.running_mean', 'encoder._blocks.22._bn2.running_var', 'encoder._blocks.23._expand_conv.weight', 'encoder._blocks.23._bn0.weight', 'encoder._blocks.23._bn0.bias', 'encoder._blocks.23._bn0.running_mean', 'encoder._blocks.23._bn0.running_var', 'encoder._blocks.23._depthwise_conv.weight', 'encoder._blocks.23._bn1.weight', 'encoder._blocks.23._bn1.bias', 'encoder._blocks.23._bn1.running_mean', 'encoder._blocks.23._bn1.running_var', 'encoder._blocks.23._se_reduce.weight', 'encoder._blocks.23._se_reduce.bias', 'encoder._blocks.23._se_expand.weight', 'encoder._blocks.23._se_expand.bias', 'encoder._blocks.23._project_conv.weight', 'encoder._blocks.23._bn2.weight', 'encoder._blocks.23._bn2.bias', 'encoder._blocks.23._bn2.running_mean', 'encoder._blocks.23._bn2.running_var', 'encoder._blocks.24._expand_conv.weight', 'encoder._blocks.24._bn0.weight', 'encoder._blocks.24._bn0.bias', 'encoder._blocks.24._bn0.running_mean', 'encoder._blocks.24._bn0.running_var', 'encoder._blocks.24._depthwise_conv.weight', 'encoder._blocks.24._bn1.weight', 'encoder._blocks.24._bn1.bias', 'encoder._blocks.24._bn1.running_mean', 'encoder._blocks.24._bn1.running_var', 'encoder._blocks.24._se_reduce.weight', 'encoder._blocks.24._se_reduce.bias', 'encoder._blocks.24._se_expand.weight', 'encoder._blocks.24._se_expand.bias', 'encoder._blocks.24._project_conv.weight', 'encoder._blocks.24._bn2.weight', 'encoder._blocks.24._bn2.bias', 'encoder._blocks.24._bn2.running_mean', 'encoder._blocks.24._bn2.running_var', 'encoder._blocks.25._expand_conv.weight', 'encoder._blocks.25._bn0.weight', 'encoder._blocks.25._bn0.bias', 'encoder._blocks.25._bn0.running_mean', 'encoder._blocks.25._bn0.running_var', 'encoder._blocks.25._depthwise_conv.weight', 'encoder._blocks.25._bn1.weight', 'encoder._blocks.25._bn1.bias', 'encoder._blocks.25._bn1.running_mean', 'encoder._blocks.25._bn1.running_var', 'encoder._blocks.25._se_reduce.weight', 'encoder._blocks.25._se_reduce.bias', 'encoder._blocks.25._se_expand.weight', 'encoder._blocks.25._se_expand.bias', 'encoder._blocks.25._project_conv.weight', 'encoder._blocks.25._bn2.weight', 'encoder._blocks.25._bn2.bias', 'encoder._blocks.25._bn2.running_mean', 'encoder._blocks.25._bn2.running_var', 'encoder._blocks.26._expand_conv.weight', 'encoder._blocks.26._bn0.weight', 'encoder._blocks.26._bn0.bias', 'encoder._blocks.26._bn0.running_mean', 'encoder._blocks.26._bn0.running_var', 'encoder._blocks.26._depthwise_conv.weight', 'encoder._blocks.26._bn1.weight', 'encoder._blocks.26._bn1.bias', 'encoder._blocks.26._bn1.running_mean', 'encoder._blocks.26._bn1.running_var', 'encoder._blocks.26._se_reduce.weight', 'encoder._blocks.26._se_reduce.bias', 'encoder._blocks.26._se_expand.weight', 'encoder._blocks.26._se_expand.bias', 'encoder._blocks.26._project_conv.weight', 'encoder._blocks.26._bn2.weight', 'encoder._blocks.26._bn2.bias', 'encoder._blocks.26._bn2.running_mean', 'encoder._blocks.26._bn2.running_var', 'encoder._blocks.27._expand_conv.weight', 'encoder._blocks.27._bn0.weight', 'encoder._blocks.27._bn0.bias', 'encoder._blocks.27._bn0.running_mean', 'encoder._blocks.27._bn0.running_var', 'encoder._blocks.27._depthwise_conv.weight', 'encoder._blocks.27._bn1.weight', 'encoder._blocks.27._bn1.bias', 'encoder._blocks.27._bn1.running_mean', 'encoder._blocks.27._bn1.running_var', 'encoder._blocks.27._se_reduce.weight', 'encoder._blocks.27._se_reduce.bias', 'encoder._blocks.27._se_expand.weight', 'encoder._blocks.27._se_expand.bias', 'encoder._blocks.27._project_conv.weight', 'encoder._blocks.27._bn2.weight', 'encoder._blocks.27._bn2.bias', 'encoder._blocks.27._bn2.running_mean', 'encoder._blocks.27._bn2.running_var', 'encoder._blocks.28._expand_conv.weight', 'encoder._blocks.28._bn0.weight', 'encoder._blocks.28._bn0.bias', 'encoder._blocks.28._bn0.running_mean', 'encoder._blocks.28._bn0.running_var', 'encoder._blocks.28._depthwise_conv.weight', 'encoder._blocks.28._bn1.weight', 'encoder._blocks.28._bn1.bias', 'encoder._blocks.28._bn1.running_mean', 'encoder._blocks.28._bn1.running_var', 'encoder._blocks.28._se_reduce.weight', 'encoder._blocks.28._se_reduce.bias', 'encoder._blocks.28._se_expand.weight', 'encoder._blocks.28._se_expand.bias', 'encoder._blocks.28._project_conv.weight', 'encoder._blocks.28._bn2.weight', 'encoder._blocks.28._bn2.bias', 'encoder._blocks.28._bn2.running_mean', 'encoder._blocks.28._bn2.running_var', 'encoder._blocks.29._expand_conv.weight', 'encoder._blocks.29._bn0.weight', 'encoder._blocks.29._bn0.bias', 'encoder._blocks.29._bn0.running_mean', 'encoder._blocks.29._bn0.running_var', 'encoder._blocks.29._depthwise_conv.weight', 'encoder._blocks.29._bn1.weight', 'encoder._blocks.29._bn1.bias', 'encoder._blocks.29._bn1.running_mean', 'encoder._blocks.29._bn1.running_var', 'encoder._blocks.29._se_reduce.weight', 'encoder._blocks.29._se_reduce.bias', 'encoder._blocks.29._se_expand.weight', 'encoder._blocks.29._se_expand.bias', 'encoder._blocks.29._project_conv.weight', 'encoder._blocks.29._bn2.weight', 'encoder._blocks.29._bn2.bias', 'encoder._blocks.29._bn2.running_mean', 'encoder._blocks.29._bn2.running_var', 'encoder._blocks.30._expand_conv.weight', 'encoder._blocks.30._bn0.weight', 'encoder._blocks.30._bn0.bias', 'encoder._blocks.30._bn0.running_mean', 'encoder._blocks.30._bn0.running_var', 'encoder._blocks.30._depthwise_conv.weight', 'encoder._blocks.30._bn1.weight', 'encoder._blocks.30._bn1.bias', 'encoder._blocks.30._bn1.running_mean', 'encoder._blocks.30._bn1.running_var', 'encoder._blocks.30._se_reduce.weight', 'encoder._blocks.30._se_reduce.bias', 'encoder._blocks.30._se_expand.weight', 'encoder._blocks.30._se_expand.bias', 'encoder._blocks.30._project_conv.weight', 'encoder._blocks.30._bn2.weight', 'encoder._blocks.30._bn2.bias', 'encoder._blocks.30._bn2.running_mean', 'encoder._blocks.30._bn2.running_var', 'encoder._blocks.31._expand_conv.weight', 'encoder._blocks.31._bn0.weight', 'encoder._blocks.31._bn0.bias', 'encoder._blocks.31._bn0.running_mean', 'encoder._blocks.31._bn0.running_var', 'encoder._blocks.31._depthwise_conv.weight', 'encoder._blocks.31._bn1.weight', 'encoder._blocks.31._bn1.bias', 'encoder._blocks.31._bn1.running_mean', 'encoder._blocks.31._bn1.running_var', 'encoder._blocks.31._se_reduce.weight', 'encoder._blocks.31._se_reduce.bias', 'encoder._blocks.31._se_expand.weight', 'encoder._blocks.31._se_expand.bias', 'encoder._blocks.31._project_conv.weight', 'encoder._blocks.31._bn2.weight', 'encoder._blocks.31._bn2.bias', 'encoder._blocks.31._bn2.running_mean', 'encoder._blocks.31._bn2.running_var', 'encoder._conv_head.weight', 'encoder._bn1.weight', 'encoder._bn1.bias', 'encoder._bn1.running_mean', 'encoder._bn1.running_var', 'decoder.blocks.x_0_0.conv1.0.weight', 'decoder.blocks.x_0_0.conv1.1.weight', 'decoder.blocks.x_0_0.conv1.1.bias', 'decoder.blocks.x_0_0.conv1.1.running_mean', 'decoder.blocks.x_0_0.conv1.1.running_var', 'decoder.blocks.x_0_0.conv2.0.weight', 'decoder.blocks.x_0_0.conv2.1.weight', 'decoder.blocks.x_0_0.conv2.1.bias', 'decoder.blocks.x_0_0.conv2.1.running_mean', 'decoder.blocks.x_0_0.conv2.1.running_var', 'decoder.blocks.x_0_1.conv1.0.weight', 'decoder.blocks.x_0_1.conv1.1.weight', 'decoder.blocks.x_0_1.conv1.1.bias', 'decoder.blocks.x_0_1.conv1.1.running_mean', 'decoder.blocks.x_0_1.conv1.1.running_var', 'decoder.blocks.x_0_1.conv2.0.weight', 'decoder.blocks.x_0_1.conv2.1.weight', 'decoder.blocks.x_0_1.conv2.1.bias', 'decoder.blocks.x_0_1.conv2.1.running_mean', 'decoder.blocks.x_0_1.conv2.1.running_var', 'decoder.blocks.x_1_1.conv1.0.weight', 'decoder.blocks.x_1_1.conv1.1.weight', 'decoder.blocks.x_1_1.conv1.1.bias', 'decoder.blocks.x_1_1.conv1.1.running_mean', 'decoder.blocks.x_1_1.conv1.1.running_var', 'decoder.blocks.x_1_1.conv2.0.weight', 'decoder.blocks.x_1_1.conv2.1.weight', 'decoder.blocks.x_1_1.conv2.1.bias', 'decoder.blocks.x_1_1.conv2.1.running_mean', 'decoder.blocks.x_1_1.conv2.1.running_var', 'decoder.blocks.x_0_2.conv1.0.weight', 'decoder.blocks.x_0_2.conv1.1.weight', 'decoder.blocks.x_0_2.conv1.1.bias', 'decoder.blocks.x_0_2.conv1.1.running_mean', 'decoder.blocks.x_0_2.conv1.1.running_var', 'decoder.blocks.x_0_2.conv2.0.weight', 'decoder.blocks.x_0_2.conv2.1.weight', 'decoder.blocks.x_0_2.conv2.1.bias', 'decoder.blocks.x_0_2.conv2.1.running_mean', 'decoder.blocks.x_0_2.conv2.1.running_var', 'decoder.blocks.x_1_2.conv1.0.weight', 'decoder.blocks.x_1_2.conv1.1.weight', 'decoder.blocks.x_1_2.conv1.1.bias', 'decoder.blocks.x_1_2.conv1.1.running_mean', 'decoder.blocks.x_1_2.conv1.1.running_var', 'decoder.blocks.x_1_2.conv2.0.weight', 'decoder.blocks.x_1_2.conv2.1.weight', 'decoder.blocks.x_1_2.conv2.1.bias', 'decoder.blocks.x_1_2.conv2.1.running_mean', 'decoder.blocks.x_1_2.conv2.1.running_var', 'decoder.blocks.x_2_2.conv1.0.weight', 'decoder.blocks.x_2_2.conv1.1.weight', 'decoder.blocks.x_2_2.conv1.1.bias', 'decoder.blocks.x_2_2.conv1.1.running_mean', 'decoder.blocks.x_2_2.conv1.1.running_var', 'decoder.blocks.x_2_2.conv2.0.weight', 'decoder.blocks.x_2_2.conv2.1.weight', 'decoder.blocks.x_2_2.conv2.1.bias', 'decoder.blocks.x_2_2.conv2.1.running_mean', 'decoder.blocks.x_2_2.conv2.1.running_var', 'decoder.blocks.x_0_3.conv1.0.weight', 'decoder.blocks.x_0_3.conv1.1.weight', 'decoder.blocks.x_0_3.conv1.1.bias', 'decoder.blocks.x_0_3.conv1.1.running_mean', 'decoder.blocks.x_0_3.conv1.1.running_var', 'decoder.blocks.x_0_3.conv2.0.weight', 'decoder.blocks.x_0_3.conv2.1.weight', 'decoder.blocks.x_0_3.conv2.1.bias', 'decoder.blocks.x_0_3.conv2.1.running_mean', 'decoder.blocks.x_0_3.conv2.1.running_var', 'decoder.blocks.x_1_3.conv1.0.weight', 'decoder.blocks.x_1_3.conv1.1.weight', 'decoder.blocks.x_1_3.conv1.1.bias', 'decoder.blocks.x_1_3.conv1.1.running_mean', 'decoder.blocks.x_1_3.conv1.1.running_var', 'decoder.blocks.x_1_3.conv2.0.weight', 'decoder.blocks.x_1_3.conv2.1.weight', 'decoder.blocks.x_1_3.conv2.1.bias', 'decoder.blocks.x_1_3.conv2.1.running_mean', 'decoder.blocks.x_1_3.conv2.1.running_var', 'decoder.blocks.x_2_3.conv1.0.weight', 'decoder.blocks.x_2_3.conv1.1.weight', 'decoder.blocks.x_2_3.conv1.1.bias', 'decoder.blocks.x_2_3.conv1.1.running_mean', 'decoder.blocks.x_2_3.conv1.1.running_var', 'decoder.blocks.x_2_3.conv2.0.weight', 'decoder.blocks.x_2_3.conv2.1.weight', 'decoder.blocks.x_2_3.conv2.1.bias', 'decoder.blocks.x_2_3.conv2.1.running_mean', 'decoder.blocks.x_2_3.conv2.1.running_var', 'decoder.blocks.x_3_3.conv1.0.weight', 'decoder.blocks.x_3_3.conv1.1.weight', 'decoder.blocks.x_3_3.conv1.1.bias', 'decoder.blocks.x_3_3.conv1.1.running_mean', 'decoder.blocks.x_3_3.conv1.1.running_var', 'decoder.blocks.x_3_3.conv2.0.weight', 'decoder.blocks.x_3_3.conv2.1.weight', 'decoder.blocks.x_3_3.conv2.1.bias', 'decoder.blocks.x_3_3.conv2.1.running_mean', 'decoder.blocks.x_3_3.conv2.1.running_var', 'decoder.blocks.x_0_4.conv1.0.weight', 'decoder.blocks.x_0_4.conv1.1.weight', 'decoder.blocks.x_0_4.conv1.1.bias', 'decoder.blocks.x_0_4.conv1.1.running_mean', 'decoder.blocks.x_0_4.conv1.1.running_var', 'decoder.blocks.x_0_4.conv2.0.weight', 'decoder.blocks.x_0_4.conv2.1.weight', 'decoder.blocks.x_0_4.conv2.1.bias', 'decoder.blocks.x_0_4.conv2.1.running_mean', 'decoder.blocks.x_0_4.conv2.1.running_var', 'segmentation_head.0.weight', 'segmentation_head.0.bias'], unexpected_keys=['state_dict', 'logs', 'epoch'])"]},"metadata":{},"execution_count":24}],"source":["weight = torch.load('/content/gdrive/MyDrive/sw_contest/pretrained/models/stage3/effb4-f0/checkpoints/best.pth')\n","model2 = smp.UnetPlusPlus(encoder_name=\"efficientnet-b4\", encoder_weights=None, in_channels=3, classes=num_classes)\n","model2.to(device)\n","model2.load_state_dict(weight, strict=False)\n"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3378,"status":"ok","timestamp":1704927580512,"user":{"displayName":"조영민","userId":"10749653391039175177"},"user_tz":-540},"id":"goLTTltWK0u-","outputId":"0178a20a-4097-4a13-866f-d65a2cd75d57"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["_IncompatibleKeys(missing_keys=['encoder.conv2d_1a.conv.weight', 'encoder.conv2d_1a.bn.weight', 'encoder.conv2d_1a.bn.bias', 'encoder.conv2d_1a.bn.running_mean', 'encoder.conv2d_1a.bn.running_var', 'encoder.conv2d_2a.conv.weight', 'encoder.conv2d_2a.bn.weight', 'encoder.conv2d_2a.bn.bias', 'encoder.conv2d_2a.bn.running_mean', 'encoder.conv2d_2a.bn.running_var', 'encoder.conv2d_2b.conv.weight', 'encoder.conv2d_2b.bn.weight', 'encoder.conv2d_2b.bn.bias', 'encoder.conv2d_2b.bn.running_mean', 'encoder.conv2d_2b.bn.running_var', 'encoder.conv2d_3b.conv.weight', 'encoder.conv2d_3b.bn.weight', 'encoder.conv2d_3b.bn.bias', 'encoder.conv2d_3b.bn.running_mean', 'encoder.conv2d_3b.bn.running_var', 'encoder.conv2d_4a.conv.weight', 'encoder.conv2d_4a.bn.weight', 'encoder.conv2d_4a.bn.bias', 'encoder.conv2d_4a.bn.running_mean', 'encoder.conv2d_4a.bn.running_var', 'encoder.mixed_5b.branch0.conv.weight', 'encoder.mixed_5b.branch0.bn.weight', 'encoder.mixed_5b.branch0.bn.bias', 'encoder.mixed_5b.branch0.bn.running_mean', 'encoder.mixed_5b.branch0.bn.running_var', 'encoder.mixed_5b.branch1.0.conv.weight', 'encoder.mixed_5b.branch1.0.bn.weight', 'encoder.mixed_5b.branch1.0.bn.bias', 'encoder.mixed_5b.branch1.0.bn.running_mean', 'encoder.mixed_5b.branch1.0.bn.running_var', 'encoder.mixed_5b.branch1.1.conv.weight', 'encoder.mixed_5b.branch1.1.bn.weight', 'encoder.mixed_5b.branch1.1.bn.bias', 'encoder.mixed_5b.branch1.1.bn.running_mean', 'encoder.mixed_5b.branch1.1.bn.running_var', 'encoder.mixed_5b.branch2.0.conv.weight', 'encoder.mixed_5b.branch2.0.bn.weight', 'encoder.mixed_5b.branch2.0.bn.bias', 'encoder.mixed_5b.branch2.0.bn.running_mean', 'encoder.mixed_5b.branch2.0.bn.running_var', 'encoder.mixed_5b.branch2.1.conv.weight', 'encoder.mixed_5b.branch2.1.bn.weight', 'encoder.mixed_5b.branch2.1.bn.bias', 'encoder.mixed_5b.branch2.1.bn.running_mean', 'encoder.mixed_5b.branch2.1.bn.running_var', 'encoder.mixed_5b.branch2.2.conv.weight', 'encoder.mixed_5b.branch2.2.bn.weight', 'encoder.mixed_5b.branch2.2.bn.bias', 'encoder.mixed_5b.branch2.2.bn.running_mean', 'encoder.mixed_5b.branch2.2.bn.running_var', 'encoder.mixed_5b.branch3.1.conv.weight', 'encoder.mixed_5b.branch3.1.bn.weight', 'encoder.mixed_5b.branch3.1.bn.bias', 'encoder.mixed_5b.branch3.1.bn.running_mean', 'encoder.mixed_5b.branch3.1.bn.running_var', 'encoder.repeat.0.branch0.conv.weight', 'encoder.repeat.0.branch0.bn.weight', 'encoder.repeat.0.branch0.bn.bias', 'encoder.repeat.0.branch0.bn.running_mean', 'encoder.repeat.0.branch0.bn.running_var', 'encoder.repeat.0.branch1.0.conv.weight', 'encoder.repeat.0.branch1.0.bn.weight', 'encoder.repeat.0.branch1.0.bn.bias', 'encoder.repeat.0.branch1.0.bn.running_mean', 'encoder.repeat.0.branch1.0.bn.running_var', 'encoder.repeat.0.branch1.1.conv.weight', 'encoder.repeat.0.branch1.1.bn.weight', 'encoder.repeat.0.branch1.1.bn.bias', 'encoder.repeat.0.branch1.1.bn.running_mean', 'encoder.repeat.0.branch1.1.bn.running_var', 'encoder.repeat.0.branch2.0.conv.weight', 'encoder.repeat.0.branch2.0.bn.weight', 'encoder.repeat.0.branch2.0.bn.bias', 'encoder.repeat.0.branch2.0.bn.running_mean', 'encoder.repeat.0.branch2.0.bn.running_var', 'encoder.repeat.0.branch2.1.conv.weight', 'encoder.repeat.0.branch2.1.bn.weight', 'encoder.repeat.0.branch2.1.bn.bias', 'encoder.repeat.0.branch2.1.bn.running_mean', 'encoder.repeat.0.branch2.1.bn.running_var', 'encoder.repeat.0.branch2.2.conv.weight', 'encoder.repeat.0.branch2.2.bn.weight', 'encoder.repeat.0.branch2.2.bn.bias', 'encoder.repeat.0.branch2.2.bn.running_mean', 'encoder.repeat.0.branch2.2.bn.running_var', 'encoder.repeat.0.conv2d.weight', 'encoder.repeat.0.conv2d.bias', 'encoder.repeat.1.branch0.conv.weight', 'encoder.repeat.1.branch0.bn.weight', 'encoder.repeat.1.branch0.bn.bias', 'encoder.repeat.1.branch0.bn.running_mean', 'encoder.repeat.1.branch0.bn.running_var', 'encoder.repeat.1.branch1.0.conv.weight', 'encoder.repeat.1.branch1.0.bn.weight', 'encoder.repeat.1.branch1.0.bn.bias', 'encoder.repeat.1.branch1.0.bn.running_mean', 'encoder.repeat.1.branch1.0.bn.running_var', 'encoder.repeat.1.branch1.1.conv.weight', 'encoder.repeat.1.branch1.1.bn.weight', 'encoder.repeat.1.branch1.1.bn.bias', 'encoder.repeat.1.branch1.1.bn.running_mean', 'encoder.repeat.1.branch1.1.bn.running_var', 'encoder.repeat.1.branch2.0.conv.weight', 'encoder.repeat.1.branch2.0.bn.weight', 'encoder.repeat.1.branch2.0.bn.bias', 'encoder.repeat.1.branch2.0.bn.running_mean', 'encoder.repeat.1.branch2.0.bn.running_var', 'encoder.repeat.1.branch2.1.conv.weight', 'encoder.repeat.1.branch2.1.bn.weight', 'encoder.repeat.1.branch2.1.bn.bias', 'encoder.repeat.1.branch2.1.bn.running_mean', 'encoder.repeat.1.branch2.1.bn.running_var', 'encoder.repeat.1.branch2.2.conv.weight', 'encoder.repeat.1.branch2.2.bn.weight', 'encoder.repeat.1.branch2.2.bn.bias', 'encoder.repeat.1.branch2.2.bn.running_mean', 'encoder.repeat.1.branch2.2.bn.running_var', 'encoder.repeat.1.conv2d.weight', 'encoder.repeat.1.conv2d.bias', 'encoder.repeat.2.branch0.conv.weight', 'encoder.repeat.2.branch0.bn.weight', 'encoder.repeat.2.branch0.bn.bias', 'encoder.repeat.2.branch0.bn.running_mean', 'encoder.repeat.2.branch0.bn.running_var', 'encoder.repeat.2.branch1.0.conv.weight', 'encoder.repeat.2.branch1.0.bn.weight', 'encoder.repeat.2.branch1.0.bn.bias', 'encoder.repeat.2.branch1.0.bn.running_mean', 'encoder.repeat.2.branch1.0.bn.running_var', 'encoder.repeat.2.branch1.1.conv.weight', 'encoder.repeat.2.branch1.1.bn.weight', 'encoder.repeat.2.branch1.1.bn.bias', 'encoder.repeat.2.branch1.1.bn.running_mean', 'encoder.repeat.2.branch1.1.bn.running_var', 'encoder.repeat.2.branch2.0.conv.weight', 'encoder.repeat.2.branch2.0.bn.weight', 'encoder.repeat.2.branch2.0.bn.bias', 'encoder.repeat.2.branch2.0.bn.running_mean', 'encoder.repeat.2.branch2.0.bn.running_var', 'encoder.repeat.2.branch2.1.conv.weight', 'encoder.repeat.2.branch2.1.bn.weight', 'encoder.repeat.2.branch2.1.bn.bias', 'encoder.repeat.2.branch2.1.bn.running_mean', 'encoder.repeat.2.branch2.1.bn.running_var', 'encoder.repeat.2.branch2.2.conv.weight', 'encoder.repeat.2.branch2.2.bn.weight', 'encoder.repeat.2.branch2.2.bn.bias', 'encoder.repeat.2.branch2.2.bn.running_mean', 'encoder.repeat.2.branch2.2.bn.running_var', 'encoder.repeat.2.conv2d.weight', 'encoder.repeat.2.conv2d.bias', 'encoder.repeat.3.branch0.conv.weight', 'encoder.repeat.3.branch0.bn.weight', 'encoder.repeat.3.branch0.bn.bias', 'encoder.repeat.3.branch0.bn.running_mean', 'encoder.repeat.3.branch0.bn.running_var', 'encoder.repeat.3.branch1.0.conv.weight', 'encoder.repeat.3.branch1.0.bn.weight', 'encoder.repeat.3.branch1.0.bn.bias', 'encoder.repeat.3.branch1.0.bn.running_mean', 'encoder.repeat.3.branch1.0.bn.running_var', 'encoder.repeat.3.branch1.1.conv.weight', 'encoder.repeat.3.branch1.1.bn.weight', 'encoder.repeat.3.branch1.1.bn.bias', 'encoder.repeat.3.branch1.1.bn.running_mean', 'encoder.repeat.3.branch1.1.bn.running_var', 'encoder.repeat.3.branch2.0.conv.weight', 'encoder.repeat.3.branch2.0.bn.weight', 'encoder.repeat.3.branch2.0.bn.bias', 'encoder.repeat.3.branch2.0.bn.running_mean', 'encoder.repeat.3.branch2.0.bn.running_var', 'encoder.repeat.3.branch2.1.conv.weight', 'encoder.repeat.3.branch2.1.bn.weight', 'encoder.repeat.3.branch2.1.bn.bias', 'encoder.repeat.3.branch2.1.bn.running_mean', 'encoder.repeat.3.branch2.1.bn.running_var', 'encoder.repeat.3.branch2.2.conv.weight', 'encoder.repeat.3.branch2.2.bn.weight', 'encoder.repeat.3.branch2.2.bn.bias', 'encoder.repeat.3.branch2.2.bn.running_mean', 'encoder.repeat.3.branch2.2.bn.running_var', 'encoder.repeat.3.conv2d.weight', 'encoder.repeat.3.conv2d.bias', 'encoder.repeat.4.branch0.conv.weight', 'encoder.repeat.4.branch0.bn.weight', 'encoder.repeat.4.branch0.bn.bias', 'encoder.repeat.4.branch0.bn.running_mean', 'encoder.repeat.4.branch0.bn.running_var', 'encoder.repeat.4.branch1.0.conv.weight', 'encoder.repeat.4.branch1.0.bn.weight', 'encoder.repeat.4.branch1.0.bn.bias', 'encoder.repeat.4.branch1.0.bn.running_mean', 'encoder.repeat.4.branch1.0.bn.running_var', 'encoder.repeat.4.branch1.1.conv.weight', 'encoder.repeat.4.branch1.1.bn.weight', 'encoder.repeat.4.branch1.1.bn.bias', 'encoder.repeat.4.branch1.1.bn.running_mean', 'encoder.repeat.4.branch1.1.bn.running_var', 'encoder.repeat.4.branch2.0.conv.weight', 'encoder.repeat.4.branch2.0.bn.weight', 'encoder.repeat.4.branch2.0.bn.bias', 'encoder.repeat.4.branch2.0.bn.running_mean', 'encoder.repeat.4.branch2.0.bn.running_var', 'encoder.repeat.4.branch2.1.conv.weight', 'encoder.repeat.4.branch2.1.bn.weight', 'encoder.repeat.4.branch2.1.bn.bias', 'encoder.repeat.4.branch2.1.bn.running_mean', 'encoder.repeat.4.branch2.1.bn.running_var', 'encoder.repeat.4.branch2.2.conv.weight', 'encoder.repeat.4.branch2.2.bn.weight', 'encoder.repeat.4.branch2.2.bn.bias', 'encoder.repeat.4.branch2.2.bn.running_mean', 'encoder.repeat.4.branch2.2.bn.running_var', 'encoder.repeat.4.conv2d.weight', 'encoder.repeat.4.conv2d.bias', 'encoder.repeat.5.branch0.conv.weight', 'encoder.repeat.5.branch0.bn.weight', 'encoder.repeat.5.branch0.bn.bias', 'encoder.repeat.5.branch0.bn.running_mean', 'encoder.repeat.5.branch0.bn.running_var', 'encoder.repeat.5.branch1.0.conv.weight', 'encoder.repeat.5.branch1.0.bn.weight', 'encoder.repeat.5.branch1.0.bn.bias', 'encoder.repeat.5.branch1.0.bn.running_mean', 'encoder.repeat.5.branch1.0.bn.running_var', 'encoder.repeat.5.branch1.1.conv.weight', 'encoder.repeat.5.branch1.1.bn.weight', 'encoder.repeat.5.branch1.1.bn.bias', 'encoder.repeat.5.branch1.1.bn.running_mean', 'encoder.repeat.5.branch1.1.bn.running_var', 'encoder.repeat.5.branch2.0.conv.weight', 'encoder.repeat.5.branch2.0.bn.weight', 'encoder.repeat.5.branch2.0.bn.bias', 'encoder.repeat.5.branch2.0.bn.running_mean', 'encoder.repeat.5.branch2.0.bn.running_var', 'encoder.repeat.5.branch2.1.conv.weight', 'encoder.repeat.5.branch2.1.bn.weight', 'encoder.repeat.5.branch2.1.bn.bias', 'encoder.repeat.5.branch2.1.bn.running_mean', 'encoder.repeat.5.branch2.1.bn.running_var', 'encoder.repeat.5.branch2.2.conv.weight', 'encoder.repeat.5.branch2.2.bn.weight', 'encoder.repeat.5.branch2.2.bn.bias', 'encoder.repeat.5.branch2.2.bn.running_mean', 'encoder.repeat.5.branch2.2.bn.running_var', 'encoder.repeat.5.conv2d.weight', 'encoder.repeat.5.conv2d.bias', 'encoder.repeat.6.branch0.conv.weight', 'encoder.repeat.6.branch0.bn.weight', 'encoder.repeat.6.branch0.bn.bias', 'encoder.repeat.6.branch0.bn.running_mean', 'encoder.repeat.6.branch0.bn.running_var', 'encoder.repeat.6.branch1.0.conv.weight', 'encoder.repeat.6.branch1.0.bn.weight', 'encoder.repeat.6.branch1.0.bn.bias', 'encoder.repeat.6.branch1.0.bn.running_mean', 'encoder.repeat.6.branch1.0.bn.running_var', 'encoder.repeat.6.branch1.1.conv.weight', 'encoder.repeat.6.branch1.1.bn.weight', 'encoder.repeat.6.branch1.1.bn.bias', 'encoder.repeat.6.branch1.1.bn.running_mean', 'encoder.repeat.6.branch1.1.bn.running_var', 'encoder.repeat.6.branch2.0.conv.weight', 'encoder.repeat.6.branch2.0.bn.weight', 'encoder.repeat.6.branch2.0.bn.bias', 'encoder.repeat.6.branch2.0.bn.running_mean', 'encoder.repeat.6.branch2.0.bn.running_var', 'encoder.repeat.6.branch2.1.conv.weight', 'encoder.repeat.6.branch2.1.bn.weight', 'encoder.repeat.6.branch2.1.bn.bias', 'encoder.repeat.6.branch2.1.bn.running_mean', 'encoder.repeat.6.branch2.1.bn.running_var', 'encoder.repeat.6.branch2.2.conv.weight', 'encoder.repeat.6.branch2.2.bn.weight', 'encoder.repeat.6.branch2.2.bn.bias', 'encoder.repeat.6.branch2.2.bn.running_mean', 'encoder.repeat.6.branch2.2.bn.running_var', 'encoder.repeat.6.conv2d.weight', 'encoder.repeat.6.conv2d.bias', 'encoder.repeat.7.branch0.conv.weight', 'encoder.repeat.7.branch0.bn.weight', 'encoder.repeat.7.branch0.bn.bias', 'encoder.repeat.7.branch0.bn.running_mean', 'encoder.repeat.7.branch0.bn.running_var', 'encoder.repeat.7.branch1.0.conv.weight', 'encoder.repeat.7.branch1.0.bn.weight', 'encoder.repeat.7.branch1.0.bn.bias', 'encoder.repeat.7.branch1.0.bn.running_mean', 'encoder.repeat.7.branch1.0.bn.running_var', 'encoder.repeat.7.branch1.1.conv.weight', 'encoder.repeat.7.branch1.1.bn.weight', 'encoder.repeat.7.branch1.1.bn.bias', 'encoder.repeat.7.branch1.1.bn.running_mean', 'encoder.repeat.7.branch1.1.bn.running_var', 'encoder.repeat.7.branch2.0.conv.weight', 'encoder.repeat.7.branch2.0.bn.weight', 'encoder.repeat.7.branch2.0.bn.bias', 'encoder.repeat.7.branch2.0.bn.running_mean', 'encoder.repeat.7.branch2.0.bn.running_var', 'encoder.repeat.7.branch2.1.conv.weight', 'encoder.repeat.7.branch2.1.bn.weight', 'encoder.repeat.7.branch2.1.bn.bias', 'encoder.repeat.7.branch2.1.bn.running_mean', 'encoder.repeat.7.branch2.1.bn.running_var', 'encoder.repeat.7.branch2.2.conv.weight', 'encoder.repeat.7.branch2.2.bn.weight', 'encoder.repeat.7.branch2.2.bn.bias', 'encoder.repeat.7.branch2.2.bn.running_mean', 'encoder.repeat.7.branch2.2.bn.running_var', 'encoder.repeat.7.conv2d.weight', 'encoder.repeat.7.conv2d.bias', 'encoder.repeat.8.branch0.conv.weight', 'encoder.repeat.8.branch0.bn.weight', 'encoder.repeat.8.branch0.bn.bias', 'encoder.repeat.8.branch0.bn.running_mean', 'encoder.repeat.8.branch0.bn.running_var', 'encoder.repeat.8.branch1.0.conv.weight', 'encoder.repeat.8.branch1.0.bn.weight', 'encoder.repeat.8.branch1.0.bn.bias', 'encoder.repeat.8.branch1.0.bn.running_mean', 'encoder.repeat.8.branch1.0.bn.running_var', 'encoder.repeat.8.branch1.1.conv.weight', 'encoder.repeat.8.branch1.1.bn.weight', 'encoder.repeat.8.branch1.1.bn.bias', 'encoder.repeat.8.branch1.1.bn.running_mean', 'encoder.repeat.8.branch1.1.bn.running_var', 'encoder.repeat.8.branch2.0.conv.weight', 'encoder.repeat.8.branch2.0.bn.weight', 'encoder.repeat.8.branch2.0.bn.bias', 'encoder.repeat.8.branch2.0.bn.running_mean', 'encoder.repeat.8.branch2.0.bn.running_var', 'encoder.repeat.8.branch2.1.conv.weight', 'encoder.repeat.8.branch2.1.bn.weight', 'encoder.repeat.8.branch2.1.bn.bias', 'encoder.repeat.8.branch2.1.bn.running_mean', 'encoder.repeat.8.branch2.1.bn.running_var', 'encoder.repeat.8.branch2.2.conv.weight', 'encoder.repeat.8.branch2.2.bn.weight', 'encoder.repeat.8.branch2.2.bn.bias', 'encoder.repeat.8.branch2.2.bn.running_mean', 'encoder.repeat.8.branch2.2.bn.running_var', 'encoder.repeat.8.conv2d.weight', 'encoder.repeat.8.conv2d.bias', 'encoder.repeat.9.branch0.conv.weight', 'encoder.repeat.9.branch0.bn.weight', 'encoder.repeat.9.branch0.bn.bias', 'encoder.repeat.9.branch0.bn.running_mean', 'encoder.repeat.9.branch0.bn.running_var', 'encoder.repeat.9.branch1.0.conv.weight', 'encoder.repeat.9.branch1.0.bn.weight', 'encoder.repeat.9.branch1.0.bn.bias', 'encoder.repeat.9.branch1.0.bn.running_mean', 'encoder.repeat.9.branch1.0.bn.running_var', 'encoder.repeat.9.branch1.1.conv.weight', 'encoder.repeat.9.branch1.1.bn.weight', 'encoder.repeat.9.branch1.1.bn.bias', 'encoder.repeat.9.branch1.1.bn.running_mean', 'encoder.repeat.9.branch1.1.bn.running_var', 'encoder.repeat.9.branch2.0.conv.weight', 'encoder.repeat.9.branch2.0.bn.weight', 'encoder.repeat.9.branch2.0.bn.bias', 'encoder.repeat.9.branch2.0.bn.running_mean', 'encoder.repeat.9.branch2.0.bn.running_var', 'encoder.repeat.9.branch2.1.conv.weight', 'encoder.repeat.9.branch2.1.bn.weight', 'encoder.repeat.9.branch2.1.bn.bias', 'encoder.repeat.9.branch2.1.bn.running_mean', 'encoder.repeat.9.branch2.1.bn.running_var', 'encoder.repeat.9.branch2.2.conv.weight', 'encoder.repeat.9.branch2.2.bn.weight', 'encoder.repeat.9.branch2.2.bn.bias', 'encoder.repeat.9.branch2.2.bn.running_mean', 'encoder.repeat.9.branch2.2.bn.running_var', 'encoder.repeat.9.conv2d.weight', 'encoder.repeat.9.conv2d.bias', 'encoder.mixed_6a.branch0.conv.weight', 'encoder.mixed_6a.branch0.bn.weight', 'encoder.mixed_6a.branch0.bn.bias', 'encoder.mixed_6a.branch0.bn.running_mean', 'encoder.mixed_6a.branch0.bn.running_var', 'encoder.mixed_6a.branch1.0.conv.weight', 'encoder.mixed_6a.branch1.0.bn.weight', 'encoder.mixed_6a.branch1.0.bn.bias', 'encoder.mixed_6a.branch1.0.bn.running_mean', 'encoder.mixed_6a.branch1.0.bn.running_var', 'encoder.mixed_6a.branch1.1.conv.weight', 'encoder.mixed_6a.branch1.1.bn.weight', 'encoder.mixed_6a.branch1.1.bn.bias', 'encoder.mixed_6a.branch1.1.bn.running_mean', 'encoder.mixed_6a.branch1.1.bn.running_var', 'encoder.mixed_6a.branch1.2.conv.weight', 'encoder.mixed_6a.branch1.2.bn.weight', 'encoder.mixed_6a.branch1.2.bn.bias', 'encoder.mixed_6a.branch1.2.bn.running_mean', 'encoder.mixed_6a.branch1.2.bn.running_var', 'encoder.repeat_1.0.branch0.conv.weight', 'encoder.repeat_1.0.branch0.bn.weight', 'encoder.repeat_1.0.branch0.bn.bias', 'encoder.repeat_1.0.branch0.bn.running_mean', 'encoder.repeat_1.0.branch0.bn.running_var', 'encoder.repeat_1.0.branch1.0.conv.weight', 'encoder.repeat_1.0.branch1.0.bn.weight', 'encoder.repeat_1.0.branch1.0.bn.bias', 'encoder.repeat_1.0.branch1.0.bn.running_mean', 'encoder.repeat_1.0.branch1.0.bn.running_var', 'encoder.repeat_1.0.branch1.1.conv.weight', 'encoder.repeat_1.0.branch1.1.bn.weight', 'encoder.repeat_1.0.branch1.1.bn.bias', 'encoder.repeat_1.0.branch1.1.bn.running_mean', 'encoder.repeat_1.0.branch1.1.bn.running_var', 'encoder.repeat_1.0.branch1.2.conv.weight', 'encoder.repeat_1.0.branch1.2.bn.weight', 'encoder.repeat_1.0.branch1.2.bn.bias', 'encoder.repeat_1.0.branch1.2.bn.running_mean', 'encoder.repeat_1.0.branch1.2.bn.running_var', 'encoder.repeat_1.0.conv2d.weight', 'encoder.repeat_1.0.conv2d.bias', 'encoder.repeat_1.1.branch0.conv.weight', 'encoder.repeat_1.1.branch0.bn.weight', 'encoder.repeat_1.1.branch0.bn.bias', 'encoder.repeat_1.1.branch0.bn.running_mean', 'encoder.repeat_1.1.branch0.bn.running_var', 'encoder.repeat_1.1.branch1.0.conv.weight', 'encoder.repeat_1.1.branch1.0.bn.weight', 'encoder.repeat_1.1.branch1.0.bn.bias', 'encoder.repeat_1.1.branch1.0.bn.running_mean', 'encoder.repeat_1.1.branch1.0.bn.running_var', 'encoder.repeat_1.1.branch1.1.conv.weight', 'encoder.repeat_1.1.branch1.1.bn.weight', 'encoder.repeat_1.1.branch1.1.bn.bias', 'encoder.repeat_1.1.branch1.1.bn.running_mean', 'encoder.repeat_1.1.branch1.1.bn.running_var', 'encoder.repeat_1.1.branch1.2.conv.weight', 'encoder.repeat_1.1.branch1.2.bn.weight', 'encoder.repeat_1.1.branch1.2.bn.bias', 'encoder.repeat_1.1.branch1.2.bn.running_mean', 'encoder.repeat_1.1.branch1.2.bn.running_var', 'encoder.repeat_1.1.conv2d.weight', 'encoder.repeat_1.1.conv2d.bias', 'encoder.repeat_1.2.branch0.conv.weight', 'encoder.repeat_1.2.branch0.bn.weight', 'encoder.repeat_1.2.branch0.bn.bias', 'encoder.repeat_1.2.branch0.bn.running_mean', 'encoder.repeat_1.2.branch0.bn.running_var', 'encoder.repeat_1.2.branch1.0.conv.weight', 'encoder.repeat_1.2.branch1.0.bn.weight', 'encoder.repeat_1.2.branch1.0.bn.bias', 'encoder.repeat_1.2.branch1.0.bn.running_mean', 'encoder.repeat_1.2.branch1.0.bn.running_var', 'encoder.repeat_1.2.branch1.1.conv.weight', 'encoder.repeat_1.2.branch1.1.bn.weight', 'encoder.repeat_1.2.branch1.1.bn.bias', 'encoder.repeat_1.2.branch1.1.bn.running_mean', 'encoder.repeat_1.2.branch1.1.bn.running_var', 'encoder.repeat_1.2.branch1.2.conv.weight', 'encoder.repeat_1.2.branch1.2.bn.weight', 'encoder.repeat_1.2.branch1.2.bn.bias', 'encoder.repeat_1.2.branch1.2.bn.running_mean', 'encoder.repeat_1.2.branch1.2.bn.running_var', 'encoder.repeat_1.2.conv2d.weight', 'encoder.repeat_1.2.conv2d.bias', 'encoder.repeat_1.3.branch0.conv.weight', 'encoder.repeat_1.3.branch0.bn.weight', 'encoder.repeat_1.3.branch0.bn.bias', 'encoder.repeat_1.3.branch0.bn.running_mean', 'encoder.repeat_1.3.branch0.bn.running_var', 'encoder.repeat_1.3.branch1.0.conv.weight', 'encoder.repeat_1.3.branch1.0.bn.weight', 'encoder.repeat_1.3.branch1.0.bn.bias', 'encoder.repeat_1.3.branch1.0.bn.running_mean', 'encoder.repeat_1.3.branch1.0.bn.running_var', 'encoder.repeat_1.3.branch1.1.conv.weight', 'encoder.repeat_1.3.branch1.1.bn.weight', 'encoder.repeat_1.3.branch1.1.bn.bias', 'encoder.repeat_1.3.branch1.1.bn.running_mean', 'encoder.repeat_1.3.branch1.1.bn.running_var', 'encoder.repeat_1.3.branch1.2.conv.weight', 'encoder.repeat_1.3.branch1.2.bn.weight', 'encoder.repeat_1.3.branch1.2.bn.bias', 'encoder.repeat_1.3.branch1.2.bn.running_mean', 'encoder.repeat_1.3.branch1.2.bn.running_var', 'encoder.repeat_1.3.conv2d.weight', 'encoder.repeat_1.3.conv2d.bias', 'encoder.repeat_1.4.branch0.conv.weight', 'encoder.repeat_1.4.branch0.bn.weight', 'encoder.repeat_1.4.branch0.bn.bias', 'encoder.repeat_1.4.branch0.bn.running_mean', 'encoder.repeat_1.4.branch0.bn.running_var', 'encoder.repeat_1.4.branch1.0.conv.weight', 'encoder.repeat_1.4.branch1.0.bn.weight', 'encoder.repeat_1.4.branch1.0.bn.bias', 'encoder.repeat_1.4.branch1.0.bn.running_mean', 'encoder.repeat_1.4.branch1.0.bn.running_var', 'encoder.repeat_1.4.branch1.1.conv.weight', 'encoder.repeat_1.4.branch1.1.bn.weight', 'encoder.repeat_1.4.branch1.1.bn.bias', 'encoder.repeat_1.4.branch1.1.bn.running_mean', 'encoder.repeat_1.4.branch1.1.bn.running_var', 'encoder.repeat_1.4.branch1.2.conv.weight', 'encoder.repeat_1.4.branch1.2.bn.weight', 'encoder.repeat_1.4.branch1.2.bn.bias', 'encoder.repeat_1.4.branch1.2.bn.running_mean', 'encoder.repeat_1.4.branch1.2.bn.running_var', 'encoder.repeat_1.4.conv2d.weight', 'encoder.repeat_1.4.conv2d.bias', 'encoder.repeat_1.5.branch0.conv.weight', 'encoder.repeat_1.5.branch0.bn.weight', 'encoder.repeat_1.5.branch0.bn.bias', 'encoder.repeat_1.5.branch0.bn.running_mean', 'encoder.repeat_1.5.branch0.bn.running_var', 'encoder.repeat_1.5.branch1.0.conv.weight', 'encoder.repeat_1.5.branch1.0.bn.weight', 'encoder.repeat_1.5.branch1.0.bn.bias', 'encoder.repeat_1.5.branch1.0.bn.running_mean', 'encoder.repeat_1.5.branch1.0.bn.running_var', 'encoder.repeat_1.5.branch1.1.conv.weight', 'encoder.repeat_1.5.branch1.1.bn.weight', 'encoder.repeat_1.5.branch1.1.bn.bias', 'encoder.repeat_1.5.branch1.1.bn.running_mean', 'encoder.repeat_1.5.branch1.1.bn.running_var', 'encoder.repeat_1.5.branch1.2.conv.weight', 'encoder.repeat_1.5.branch1.2.bn.weight', 'encoder.repeat_1.5.branch1.2.bn.bias', 'encoder.repeat_1.5.branch1.2.bn.running_mean', 'encoder.repeat_1.5.branch1.2.bn.running_var', 'encoder.repeat_1.5.conv2d.weight', 'encoder.repeat_1.5.conv2d.bias', 'encoder.repeat_1.6.branch0.conv.weight', 'encoder.repeat_1.6.branch0.bn.weight', 'encoder.repeat_1.6.branch0.bn.bias', 'encoder.repeat_1.6.branch0.bn.running_mean', 'encoder.repeat_1.6.branch0.bn.running_var', 'encoder.repeat_1.6.branch1.0.conv.weight', 'encoder.repeat_1.6.branch1.0.bn.weight', 'encoder.repeat_1.6.branch1.0.bn.bias', 'encoder.repeat_1.6.branch1.0.bn.running_mean', 'encoder.repeat_1.6.branch1.0.bn.running_var', 'encoder.repeat_1.6.branch1.1.conv.weight', 'encoder.repeat_1.6.branch1.1.bn.weight', 'encoder.repeat_1.6.branch1.1.bn.bias', 'encoder.repeat_1.6.branch1.1.bn.running_mean', 'encoder.repeat_1.6.branch1.1.bn.running_var', 'encoder.repeat_1.6.branch1.2.conv.weight', 'encoder.repeat_1.6.branch1.2.bn.weight', 'encoder.repeat_1.6.branch1.2.bn.bias', 'encoder.repeat_1.6.branch1.2.bn.running_mean', 'encoder.repeat_1.6.branch1.2.bn.running_var', 'encoder.repeat_1.6.conv2d.weight', 'encoder.repeat_1.6.conv2d.bias', 'encoder.repeat_1.7.branch0.conv.weight', 'encoder.repeat_1.7.branch0.bn.weight', 'encoder.repeat_1.7.branch0.bn.bias', 'encoder.repeat_1.7.branch0.bn.running_mean', 'encoder.repeat_1.7.branch0.bn.running_var', 'encoder.repeat_1.7.branch1.0.conv.weight', 'encoder.repeat_1.7.branch1.0.bn.weight', 'encoder.repeat_1.7.branch1.0.bn.bias', 'encoder.repeat_1.7.branch1.0.bn.running_mean', 'encoder.repeat_1.7.branch1.0.bn.running_var', 'encoder.repeat_1.7.branch1.1.conv.weight', 'encoder.repeat_1.7.branch1.1.bn.weight', 'encoder.repeat_1.7.branch1.1.bn.bias', 'encoder.repeat_1.7.branch1.1.bn.running_mean', 'encoder.repeat_1.7.branch1.1.bn.running_var', 'encoder.repeat_1.7.branch1.2.conv.weight', 'encoder.repeat_1.7.branch1.2.bn.weight', 'encoder.repeat_1.7.branch1.2.bn.bias', 'encoder.repeat_1.7.branch1.2.bn.running_mean', 'encoder.repeat_1.7.branch1.2.bn.running_var', 'encoder.repeat_1.7.conv2d.weight', 'encoder.repeat_1.7.conv2d.bias', 'encoder.repeat_1.8.branch0.conv.weight', 'encoder.repeat_1.8.branch0.bn.weight', 'encoder.repeat_1.8.branch0.bn.bias', 'encoder.repeat_1.8.branch0.bn.running_mean', 'encoder.repeat_1.8.branch0.bn.running_var', 'encoder.repeat_1.8.branch1.0.conv.weight', 'encoder.repeat_1.8.branch1.0.bn.weight', 'encoder.repeat_1.8.branch1.0.bn.bias', 'encoder.repeat_1.8.branch1.0.bn.running_mean', 'encoder.repeat_1.8.branch1.0.bn.running_var', 'encoder.repeat_1.8.branch1.1.conv.weight', 'encoder.repeat_1.8.branch1.1.bn.weight', 'encoder.repeat_1.8.branch1.1.bn.bias', 'encoder.repeat_1.8.branch1.1.bn.running_mean', 'encoder.repeat_1.8.branch1.1.bn.running_var', 'encoder.repeat_1.8.branch1.2.conv.weight', 'encoder.repeat_1.8.branch1.2.bn.weight', 'encoder.repeat_1.8.branch1.2.bn.bias', 'encoder.repeat_1.8.branch1.2.bn.running_mean', 'encoder.repeat_1.8.branch1.2.bn.running_var', 'encoder.repeat_1.8.conv2d.weight', 'encoder.repeat_1.8.conv2d.bias', 'encoder.repeat_1.9.branch0.conv.weight', 'encoder.repeat_1.9.branch0.bn.weight', 'encoder.repeat_1.9.branch0.bn.bias', 'encoder.repeat_1.9.branch0.bn.running_mean', 'encoder.repeat_1.9.branch0.bn.running_var', 'encoder.repeat_1.9.branch1.0.conv.weight', 'encoder.repeat_1.9.branch1.0.bn.weight', 'encoder.repeat_1.9.branch1.0.bn.bias', 'encoder.repeat_1.9.branch1.0.bn.running_mean', 'encoder.repeat_1.9.branch1.0.bn.running_var', 'encoder.repeat_1.9.branch1.1.conv.weight', 'encoder.repeat_1.9.branch1.1.bn.weight', 'encoder.repeat_1.9.branch1.1.bn.bias', 'encoder.repeat_1.9.branch1.1.bn.running_mean', 'encoder.repeat_1.9.branch1.1.bn.running_var', 'encoder.repeat_1.9.branch1.2.conv.weight', 'encoder.repeat_1.9.branch1.2.bn.weight', 'encoder.repeat_1.9.branch1.2.bn.bias', 'encoder.repeat_1.9.branch1.2.bn.running_mean', 'encoder.repeat_1.9.branch1.2.bn.running_var', 'encoder.repeat_1.9.conv2d.weight', 'encoder.repeat_1.9.conv2d.bias', 'encoder.repeat_1.10.branch0.conv.weight', 'encoder.repeat_1.10.branch0.bn.weight', 'encoder.repeat_1.10.branch0.bn.bias', 'encoder.repeat_1.10.branch0.bn.running_mean', 'encoder.repeat_1.10.branch0.bn.running_var', 'encoder.repeat_1.10.branch1.0.conv.weight', 'encoder.repeat_1.10.branch1.0.bn.weight', 'encoder.repeat_1.10.branch1.0.bn.bias', 'encoder.repeat_1.10.branch1.0.bn.running_mean', 'encoder.repeat_1.10.branch1.0.bn.running_var', 'encoder.repeat_1.10.branch1.1.conv.weight', 'encoder.repeat_1.10.branch1.1.bn.weight', 'encoder.repeat_1.10.branch1.1.bn.bias', 'encoder.repeat_1.10.branch1.1.bn.running_mean', 'encoder.repeat_1.10.branch1.1.bn.running_var', 'encoder.repeat_1.10.branch1.2.conv.weight', 'encoder.repeat_1.10.branch1.2.bn.weight', 'encoder.repeat_1.10.branch1.2.bn.bias', 'encoder.repeat_1.10.branch1.2.bn.running_mean', 'encoder.repeat_1.10.branch1.2.bn.running_var', 'encoder.repeat_1.10.conv2d.weight', 'encoder.repeat_1.10.conv2d.bias', 'encoder.repeat_1.11.branch0.conv.weight', 'encoder.repeat_1.11.branch0.bn.weight', 'encoder.repeat_1.11.branch0.bn.bias', 'encoder.repeat_1.11.branch0.bn.running_mean', 'encoder.repeat_1.11.branch0.bn.running_var', 'encoder.repeat_1.11.branch1.0.conv.weight', 'encoder.repeat_1.11.branch1.0.bn.weight', 'encoder.repeat_1.11.branch1.0.bn.bias', 'encoder.repeat_1.11.branch1.0.bn.running_mean', 'encoder.repeat_1.11.branch1.0.bn.running_var', 'encoder.repeat_1.11.branch1.1.conv.weight', 'encoder.repeat_1.11.branch1.1.bn.weight', 'encoder.repeat_1.11.branch1.1.bn.bias', 'encoder.repeat_1.11.branch1.1.bn.running_mean', 'encoder.repeat_1.11.branch1.1.bn.running_var', 'encoder.repeat_1.11.branch1.2.conv.weight', 'encoder.repeat_1.11.branch1.2.bn.weight', 'encoder.repeat_1.11.branch1.2.bn.bias', 'encoder.repeat_1.11.branch1.2.bn.running_mean', 'encoder.repeat_1.11.branch1.2.bn.running_var', 'encoder.repeat_1.11.conv2d.weight', 'encoder.repeat_1.11.conv2d.bias', 'encoder.repeat_1.12.branch0.conv.weight', 'encoder.repeat_1.12.branch0.bn.weight', 'encoder.repeat_1.12.branch0.bn.bias', 'encoder.repeat_1.12.branch0.bn.running_mean', 'encoder.repeat_1.12.branch0.bn.running_var', 'encoder.repeat_1.12.branch1.0.conv.weight', 'encoder.repeat_1.12.branch1.0.bn.weight', 'encoder.repeat_1.12.branch1.0.bn.bias', 'encoder.repeat_1.12.branch1.0.bn.running_mean', 'encoder.repeat_1.12.branch1.0.bn.running_var', 'encoder.repeat_1.12.branch1.1.conv.weight', 'encoder.repeat_1.12.branch1.1.bn.weight', 'encoder.repeat_1.12.branch1.1.bn.bias', 'encoder.repeat_1.12.branch1.1.bn.running_mean', 'encoder.repeat_1.12.branch1.1.bn.running_var', 'encoder.repeat_1.12.branch1.2.conv.weight', 'encoder.repeat_1.12.branch1.2.bn.weight', 'encoder.repeat_1.12.branch1.2.bn.bias', 'encoder.repeat_1.12.branch1.2.bn.running_mean', 'encoder.repeat_1.12.branch1.2.bn.running_var', 'encoder.repeat_1.12.conv2d.weight', 'encoder.repeat_1.12.conv2d.bias', 'encoder.repeat_1.13.branch0.conv.weight', 'encoder.repeat_1.13.branch0.bn.weight', 'encoder.repeat_1.13.branch0.bn.bias', 'encoder.repeat_1.13.branch0.bn.running_mean', 'encoder.repeat_1.13.branch0.bn.running_var', 'encoder.repeat_1.13.branch1.0.conv.weight', 'encoder.repeat_1.13.branch1.0.bn.weight', 'encoder.repeat_1.13.branch1.0.bn.bias', 'encoder.repeat_1.13.branch1.0.bn.running_mean', 'encoder.repeat_1.13.branch1.0.bn.running_var', 'encoder.repeat_1.13.branch1.1.conv.weight', 'encoder.repeat_1.13.branch1.1.bn.weight', 'encoder.repeat_1.13.branch1.1.bn.bias', 'encoder.repeat_1.13.branch1.1.bn.running_mean', 'encoder.repeat_1.13.branch1.1.bn.running_var', 'encoder.repeat_1.13.branch1.2.conv.weight', 'encoder.repeat_1.13.branch1.2.bn.weight', 'encoder.repeat_1.13.branch1.2.bn.bias', 'encoder.repeat_1.13.branch1.2.bn.running_mean', 'encoder.repeat_1.13.branch1.2.bn.running_var', 'encoder.repeat_1.13.conv2d.weight', 'encoder.repeat_1.13.conv2d.bias', 'encoder.repeat_1.14.branch0.conv.weight', 'encoder.repeat_1.14.branch0.bn.weight', 'encoder.repeat_1.14.branch0.bn.bias', 'encoder.repeat_1.14.branch0.bn.running_mean', 'encoder.repeat_1.14.branch0.bn.running_var', 'encoder.repeat_1.14.branch1.0.conv.weight', 'encoder.repeat_1.14.branch1.0.bn.weight', 'encoder.repeat_1.14.branch1.0.bn.bias', 'encoder.repeat_1.14.branch1.0.bn.running_mean', 'encoder.repeat_1.14.branch1.0.bn.running_var', 'encoder.repeat_1.14.branch1.1.conv.weight', 'encoder.repeat_1.14.branch1.1.bn.weight', 'encoder.repeat_1.14.branch1.1.bn.bias', 'encoder.repeat_1.14.branch1.1.bn.running_mean', 'encoder.repeat_1.14.branch1.1.bn.running_var', 'encoder.repeat_1.14.branch1.2.conv.weight', 'encoder.repeat_1.14.branch1.2.bn.weight', 'encoder.repeat_1.14.branch1.2.bn.bias', 'encoder.repeat_1.14.branch1.2.bn.running_mean', 'encoder.repeat_1.14.branch1.2.bn.running_var', 'encoder.repeat_1.14.conv2d.weight', 'encoder.repeat_1.14.conv2d.bias', 'encoder.repeat_1.15.branch0.conv.weight', 'encoder.repeat_1.15.branch0.bn.weight', 'encoder.repeat_1.15.branch0.bn.bias', 'encoder.repeat_1.15.branch0.bn.running_mean', 'encoder.repeat_1.15.branch0.bn.running_var', 'encoder.repeat_1.15.branch1.0.conv.weight', 'encoder.repeat_1.15.branch1.0.bn.weight', 'encoder.repeat_1.15.branch1.0.bn.bias', 'encoder.repeat_1.15.branch1.0.bn.running_mean', 'encoder.repeat_1.15.branch1.0.bn.running_var', 'encoder.repeat_1.15.branch1.1.conv.weight', 'encoder.repeat_1.15.branch1.1.bn.weight', 'encoder.repeat_1.15.branch1.1.bn.bias', 'encoder.repeat_1.15.branch1.1.bn.running_mean', 'encoder.repeat_1.15.branch1.1.bn.running_var', 'encoder.repeat_1.15.branch1.2.conv.weight', 'encoder.repeat_1.15.branch1.2.bn.weight', 'encoder.repeat_1.15.branch1.2.bn.bias', 'encoder.repeat_1.15.branch1.2.bn.running_mean', 'encoder.repeat_1.15.branch1.2.bn.running_var', 'encoder.repeat_1.15.conv2d.weight', 'encoder.repeat_1.15.conv2d.bias', 'encoder.repeat_1.16.branch0.conv.weight', 'encoder.repeat_1.16.branch0.bn.weight', 'encoder.repeat_1.16.branch0.bn.bias', 'encoder.repeat_1.16.branch0.bn.running_mean', 'encoder.repeat_1.16.branch0.bn.running_var', 'encoder.repeat_1.16.branch1.0.conv.weight', 'encoder.repeat_1.16.branch1.0.bn.weight', 'encoder.repeat_1.16.branch1.0.bn.bias', 'encoder.repeat_1.16.branch1.0.bn.running_mean', 'encoder.repeat_1.16.branch1.0.bn.running_var', 'encoder.repeat_1.16.branch1.1.conv.weight', 'encoder.repeat_1.16.branch1.1.bn.weight', 'encoder.repeat_1.16.branch1.1.bn.bias', 'encoder.repeat_1.16.branch1.1.bn.running_mean', 'encoder.repeat_1.16.branch1.1.bn.running_var', 'encoder.repeat_1.16.branch1.2.conv.weight', 'encoder.repeat_1.16.branch1.2.bn.weight', 'encoder.repeat_1.16.branch1.2.bn.bias', 'encoder.repeat_1.16.branch1.2.bn.running_mean', 'encoder.repeat_1.16.branch1.2.bn.running_var', 'encoder.repeat_1.16.conv2d.weight', 'encoder.repeat_1.16.conv2d.bias', 'encoder.repeat_1.17.branch0.conv.weight', 'encoder.repeat_1.17.branch0.bn.weight', 'encoder.repeat_1.17.branch0.bn.bias', 'encoder.repeat_1.17.branch0.bn.running_mean', 'encoder.repeat_1.17.branch0.bn.running_var', 'encoder.repeat_1.17.branch1.0.conv.weight', 'encoder.repeat_1.17.branch1.0.bn.weight', 'encoder.repeat_1.17.branch1.0.bn.bias', 'encoder.repeat_1.17.branch1.0.bn.running_mean', 'encoder.repeat_1.17.branch1.0.bn.running_var', 'encoder.repeat_1.17.branch1.1.conv.weight', 'encoder.repeat_1.17.branch1.1.bn.weight', 'encoder.repeat_1.17.branch1.1.bn.bias', 'encoder.repeat_1.17.branch1.1.bn.running_mean', 'encoder.repeat_1.17.branch1.1.bn.running_var', 'encoder.repeat_1.17.branch1.2.conv.weight', 'encoder.repeat_1.17.branch1.2.bn.weight', 'encoder.repeat_1.17.branch1.2.bn.bias', 'encoder.repeat_1.17.branch1.2.bn.running_mean', 'encoder.repeat_1.17.branch1.2.bn.running_var', 'encoder.repeat_1.17.conv2d.weight', 'encoder.repeat_1.17.conv2d.bias', 'encoder.repeat_1.18.branch0.conv.weight', 'encoder.repeat_1.18.branch0.bn.weight', 'encoder.repeat_1.18.branch0.bn.bias', 'encoder.repeat_1.18.branch0.bn.running_mean', 'encoder.repeat_1.18.branch0.bn.running_var', 'encoder.repeat_1.18.branch1.0.conv.weight', 'encoder.repeat_1.18.branch1.0.bn.weight', 'encoder.repeat_1.18.branch1.0.bn.bias', 'encoder.repeat_1.18.branch1.0.bn.running_mean', 'encoder.repeat_1.18.branch1.0.bn.running_var', 'encoder.repeat_1.18.branch1.1.conv.weight', 'encoder.repeat_1.18.branch1.1.bn.weight', 'encoder.repeat_1.18.branch1.1.bn.bias', 'encoder.repeat_1.18.branch1.1.bn.running_mean', 'encoder.repeat_1.18.branch1.1.bn.running_var', 'encoder.repeat_1.18.branch1.2.conv.weight', 'encoder.repeat_1.18.branch1.2.bn.weight', 'encoder.repeat_1.18.branch1.2.bn.bias', 'encoder.repeat_1.18.branch1.2.bn.running_mean', 'encoder.repeat_1.18.branch1.2.bn.running_var', 'encoder.repeat_1.18.conv2d.weight', 'encoder.repeat_1.18.conv2d.bias', 'encoder.repeat_1.19.branch0.conv.weight', 'encoder.repeat_1.19.branch0.bn.weight', 'encoder.repeat_1.19.branch0.bn.bias', 'encoder.repeat_1.19.branch0.bn.running_mean', 'encoder.repeat_1.19.branch0.bn.running_var', 'encoder.repeat_1.19.branch1.0.conv.weight', 'encoder.repeat_1.19.branch1.0.bn.weight', 'encoder.repeat_1.19.branch1.0.bn.bias', 'encoder.repeat_1.19.branch1.0.bn.running_mean', 'encoder.repeat_1.19.branch1.0.bn.running_var', 'encoder.repeat_1.19.branch1.1.conv.weight', 'encoder.repeat_1.19.branch1.1.bn.weight', 'encoder.repeat_1.19.branch1.1.bn.bias', 'encoder.repeat_1.19.branch1.1.bn.running_mean', 'encoder.repeat_1.19.branch1.1.bn.running_var', 'encoder.repeat_1.19.branch1.2.conv.weight', 'encoder.repeat_1.19.branch1.2.bn.weight', 'encoder.repeat_1.19.branch1.2.bn.bias', 'encoder.repeat_1.19.branch1.2.bn.running_mean', 'encoder.repeat_1.19.branch1.2.bn.running_var', 'encoder.repeat_1.19.conv2d.weight', 'encoder.repeat_1.19.conv2d.bias', 'encoder.mixed_7a.branch0.0.conv.weight', 'encoder.mixed_7a.branch0.0.bn.weight', 'encoder.mixed_7a.branch0.0.bn.bias', 'encoder.mixed_7a.branch0.0.bn.running_mean', 'encoder.mixed_7a.branch0.0.bn.running_var', 'encoder.mixed_7a.branch0.1.conv.weight', 'encoder.mixed_7a.branch0.1.bn.weight', 'encoder.mixed_7a.branch0.1.bn.bias', 'encoder.mixed_7a.branch0.1.bn.running_mean', 'encoder.mixed_7a.branch0.1.bn.running_var', 'encoder.mixed_7a.branch1.0.conv.weight', 'encoder.mixed_7a.branch1.0.bn.weight', 'encoder.mixed_7a.branch1.0.bn.bias', 'encoder.mixed_7a.branch1.0.bn.running_mean', 'encoder.mixed_7a.branch1.0.bn.running_var', 'encoder.mixed_7a.branch1.1.conv.weight', 'encoder.mixed_7a.branch1.1.bn.weight', 'encoder.mixed_7a.branch1.1.bn.bias', 'encoder.mixed_7a.branch1.1.bn.running_mean', 'encoder.mixed_7a.branch1.1.bn.running_var', 'encoder.mixed_7a.branch2.0.conv.weight', 'encoder.mixed_7a.branch2.0.bn.weight', 'encoder.mixed_7a.branch2.0.bn.bias', 'encoder.mixed_7a.branch2.0.bn.running_mean', 'encoder.mixed_7a.branch2.0.bn.running_var', 'encoder.mixed_7a.branch2.1.conv.weight', 'encoder.mixed_7a.branch2.1.bn.weight', 'encoder.mixed_7a.branch2.1.bn.bias', 'encoder.mixed_7a.branch2.1.bn.running_mean', 'encoder.mixed_7a.branch2.1.bn.running_var', 'encoder.mixed_7a.branch2.2.conv.weight', 'encoder.mixed_7a.branch2.2.bn.weight', 'encoder.mixed_7a.branch2.2.bn.bias', 'encoder.mixed_7a.branch2.2.bn.running_mean', 'encoder.mixed_7a.branch2.2.bn.running_var', 'encoder.repeat_2.0.branch0.conv.weight', 'encoder.repeat_2.0.branch0.bn.weight', 'encoder.repeat_2.0.branch0.bn.bias', 'encoder.repeat_2.0.branch0.bn.running_mean', 'encoder.repeat_2.0.branch0.bn.running_var', 'encoder.repeat_2.0.branch1.0.conv.weight', 'encoder.repeat_2.0.branch1.0.bn.weight', 'encoder.repeat_2.0.branch1.0.bn.bias', 'encoder.repeat_2.0.branch1.0.bn.running_mean', 'encoder.repeat_2.0.branch1.0.bn.running_var', 'encoder.repeat_2.0.branch1.1.conv.weight', 'encoder.repeat_2.0.branch1.1.bn.weight', 'encoder.repeat_2.0.branch1.1.bn.bias', 'encoder.repeat_2.0.branch1.1.bn.running_mean', 'encoder.repeat_2.0.branch1.1.bn.running_var', 'encoder.repeat_2.0.branch1.2.conv.weight', 'encoder.repeat_2.0.branch1.2.bn.weight', 'encoder.repeat_2.0.branch1.2.bn.bias', 'encoder.repeat_2.0.branch1.2.bn.running_mean', 'encoder.repeat_2.0.branch1.2.bn.running_var', 'encoder.repeat_2.0.conv2d.weight', 'encoder.repeat_2.0.conv2d.bias', 'encoder.repeat_2.1.branch0.conv.weight', 'encoder.repeat_2.1.branch0.bn.weight', 'encoder.repeat_2.1.branch0.bn.bias', 'encoder.repeat_2.1.branch0.bn.running_mean', 'encoder.repeat_2.1.branch0.bn.running_var', 'encoder.repeat_2.1.branch1.0.conv.weight', 'encoder.repeat_2.1.branch1.0.bn.weight', 'encoder.repeat_2.1.branch1.0.bn.bias', 'encoder.repeat_2.1.branch1.0.bn.running_mean', 'encoder.repeat_2.1.branch1.0.bn.running_var', 'encoder.repeat_2.1.branch1.1.conv.weight', 'encoder.repeat_2.1.branch1.1.bn.weight', 'encoder.repeat_2.1.branch1.1.bn.bias', 'encoder.repeat_2.1.branch1.1.bn.running_mean', 'encoder.repeat_2.1.branch1.1.bn.running_var', 'encoder.repeat_2.1.branch1.2.conv.weight', 'encoder.repeat_2.1.branch1.2.bn.weight', 'encoder.repeat_2.1.branch1.2.bn.bias', 'encoder.repeat_2.1.branch1.2.bn.running_mean', 'encoder.repeat_2.1.branch1.2.bn.running_var', 'encoder.repeat_2.1.conv2d.weight', 'encoder.repeat_2.1.conv2d.bias', 'encoder.repeat_2.2.branch0.conv.weight', 'encoder.repeat_2.2.branch0.bn.weight', 'encoder.repeat_2.2.branch0.bn.bias', 'encoder.repeat_2.2.branch0.bn.running_mean', 'encoder.repeat_2.2.branch0.bn.running_var', 'encoder.repeat_2.2.branch1.0.conv.weight', 'encoder.repeat_2.2.branch1.0.bn.weight', 'encoder.repeat_2.2.branch1.0.bn.bias', 'encoder.repeat_2.2.branch1.0.bn.running_mean', 'encoder.repeat_2.2.branch1.0.bn.running_var', 'encoder.repeat_2.2.branch1.1.conv.weight', 'encoder.repeat_2.2.branch1.1.bn.weight', 'encoder.repeat_2.2.branch1.1.bn.bias', 'encoder.repeat_2.2.branch1.1.bn.running_mean', 'encoder.repeat_2.2.branch1.1.bn.running_var', 'encoder.repeat_2.2.branch1.2.conv.weight', 'encoder.repeat_2.2.branch1.2.bn.weight', 'encoder.repeat_2.2.branch1.2.bn.bias', 'encoder.repeat_2.2.branch1.2.bn.running_mean', 'encoder.repeat_2.2.branch1.2.bn.running_var', 'encoder.repeat_2.2.conv2d.weight', 'encoder.repeat_2.2.conv2d.bias', 'encoder.repeat_2.3.branch0.conv.weight', 'encoder.repeat_2.3.branch0.bn.weight', 'encoder.repeat_2.3.branch0.bn.bias', 'encoder.repeat_2.3.branch0.bn.running_mean', 'encoder.repeat_2.3.branch0.bn.running_var', 'encoder.repeat_2.3.branch1.0.conv.weight', 'encoder.repeat_2.3.branch1.0.bn.weight', 'encoder.repeat_2.3.branch1.0.bn.bias', 'encoder.repeat_2.3.branch1.0.bn.running_mean', 'encoder.repeat_2.3.branch1.0.bn.running_var', 'encoder.repeat_2.3.branch1.1.conv.weight', 'encoder.repeat_2.3.branch1.1.bn.weight', 'encoder.repeat_2.3.branch1.1.bn.bias', 'encoder.repeat_2.3.branch1.1.bn.running_mean', 'encoder.repeat_2.3.branch1.1.bn.running_var', 'encoder.repeat_2.3.branch1.2.conv.weight', 'encoder.repeat_2.3.branch1.2.bn.weight', 'encoder.repeat_2.3.branch1.2.bn.bias', 'encoder.repeat_2.3.branch1.2.bn.running_mean', 'encoder.repeat_2.3.branch1.2.bn.running_var', 'encoder.repeat_2.3.conv2d.weight', 'encoder.repeat_2.3.conv2d.bias', 'encoder.repeat_2.4.branch0.conv.weight', 'encoder.repeat_2.4.branch0.bn.weight', 'encoder.repeat_2.4.branch0.bn.bias', 'encoder.repeat_2.4.branch0.bn.running_mean', 'encoder.repeat_2.4.branch0.bn.running_var', 'encoder.repeat_2.4.branch1.0.conv.weight', 'encoder.repeat_2.4.branch1.0.bn.weight', 'encoder.repeat_2.4.branch1.0.bn.bias', 'encoder.repeat_2.4.branch1.0.bn.running_mean', 'encoder.repeat_2.4.branch1.0.bn.running_var', 'encoder.repeat_2.4.branch1.1.conv.weight', 'encoder.repeat_2.4.branch1.1.bn.weight', 'encoder.repeat_2.4.branch1.1.bn.bias', 'encoder.repeat_2.4.branch1.1.bn.running_mean', 'encoder.repeat_2.4.branch1.1.bn.running_var', 'encoder.repeat_2.4.branch1.2.conv.weight', 'encoder.repeat_2.4.branch1.2.bn.weight', 'encoder.repeat_2.4.branch1.2.bn.bias', 'encoder.repeat_2.4.branch1.2.bn.running_mean', 'encoder.repeat_2.4.branch1.2.bn.running_var', 'encoder.repeat_2.4.conv2d.weight', 'encoder.repeat_2.4.conv2d.bias', 'encoder.repeat_2.5.branch0.conv.weight', 'encoder.repeat_2.5.branch0.bn.weight', 'encoder.repeat_2.5.branch0.bn.bias', 'encoder.repeat_2.5.branch0.bn.running_mean', 'encoder.repeat_2.5.branch0.bn.running_var', 'encoder.repeat_2.5.branch1.0.conv.weight', 'encoder.repeat_2.5.branch1.0.bn.weight', 'encoder.repeat_2.5.branch1.0.bn.bias', 'encoder.repeat_2.5.branch1.0.bn.running_mean', 'encoder.repeat_2.5.branch1.0.bn.running_var', 'encoder.repeat_2.5.branch1.1.conv.weight', 'encoder.repeat_2.5.branch1.1.bn.weight', 'encoder.repeat_2.5.branch1.1.bn.bias', 'encoder.repeat_2.5.branch1.1.bn.running_mean', 'encoder.repeat_2.5.branch1.1.bn.running_var', 'encoder.repeat_2.5.branch1.2.conv.weight', 'encoder.repeat_2.5.branch1.2.bn.weight', 'encoder.repeat_2.5.branch1.2.bn.bias', 'encoder.repeat_2.5.branch1.2.bn.running_mean', 'encoder.repeat_2.5.branch1.2.bn.running_var', 'encoder.repeat_2.5.conv2d.weight', 'encoder.repeat_2.5.conv2d.bias', 'encoder.repeat_2.6.branch0.conv.weight', 'encoder.repeat_2.6.branch0.bn.weight', 'encoder.repeat_2.6.branch0.bn.bias', 'encoder.repeat_2.6.branch0.bn.running_mean', 'encoder.repeat_2.6.branch0.bn.running_var', 'encoder.repeat_2.6.branch1.0.conv.weight', 'encoder.repeat_2.6.branch1.0.bn.weight', 'encoder.repeat_2.6.branch1.0.bn.bias', 'encoder.repeat_2.6.branch1.0.bn.running_mean', 'encoder.repeat_2.6.branch1.0.bn.running_var', 'encoder.repeat_2.6.branch1.1.conv.weight', 'encoder.repeat_2.6.branch1.1.bn.weight', 'encoder.repeat_2.6.branch1.1.bn.bias', 'encoder.repeat_2.6.branch1.1.bn.running_mean', 'encoder.repeat_2.6.branch1.1.bn.running_var', 'encoder.repeat_2.6.branch1.2.conv.weight', 'encoder.repeat_2.6.branch1.2.bn.weight', 'encoder.repeat_2.6.branch1.2.bn.bias', 'encoder.repeat_2.6.branch1.2.bn.running_mean', 'encoder.repeat_2.6.branch1.2.bn.running_var', 'encoder.repeat_2.6.conv2d.weight', 'encoder.repeat_2.6.conv2d.bias', 'encoder.repeat_2.7.branch0.conv.weight', 'encoder.repeat_2.7.branch0.bn.weight', 'encoder.repeat_2.7.branch0.bn.bias', 'encoder.repeat_2.7.branch0.bn.running_mean', 'encoder.repeat_2.7.branch0.bn.running_var', 'encoder.repeat_2.7.branch1.0.conv.weight', 'encoder.repeat_2.7.branch1.0.bn.weight', 'encoder.repeat_2.7.branch1.0.bn.bias', 'encoder.repeat_2.7.branch1.0.bn.running_mean', 'encoder.repeat_2.7.branch1.0.bn.running_var', 'encoder.repeat_2.7.branch1.1.conv.weight', 'encoder.repeat_2.7.branch1.1.bn.weight', 'encoder.repeat_2.7.branch1.1.bn.bias', 'encoder.repeat_2.7.branch1.1.bn.running_mean', 'encoder.repeat_2.7.branch1.1.bn.running_var', 'encoder.repeat_2.7.branch1.2.conv.weight', 'encoder.repeat_2.7.branch1.2.bn.weight', 'encoder.repeat_2.7.branch1.2.bn.bias', 'encoder.repeat_2.7.branch1.2.bn.running_mean', 'encoder.repeat_2.7.branch1.2.bn.running_var', 'encoder.repeat_2.7.conv2d.weight', 'encoder.repeat_2.7.conv2d.bias', 'encoder.repeat_2.8.branch0.conv.weight', 'encoder.repeat_2.8.branch0.bn.weight', 'encoder.repeat_2.8.branch0.bn.bias', 'encoder.repeat_2.8.branch0.bn.running_mean', 'encoder.repeat_2.8.branch0.bn.running_var', 'encoder.repeat_2.8.branch1.0.conv.weight', 'encoder.repeat_2.8.branch1.0.bn.weight', 'encoder.repeat_2.8.branch1.0.bn.bias', 'encoder.repeat_2.8.branch1.0.bn.running_mean', 'encoder.repeat_2.8.branch1.0.bn.running_var', 'encoder.repeat_2.8.branch1.1.conv.weight', 'encoder.repeat_2.8.branch1.1.bn.weight', 'encoder.repeat_2.8.branch1.1.bn.bias', 'encoder.repeat_2.8.branch1.1.bn.running_mean', 'encoder.repeat_2.8.branch1.1.bn.running_var', 'encoder.repeat_2.8.branch1.2.conv.weight', 'encoder.repeat_2.8.branch1.2.bn.weight', 'encoder.repeat_2.8.branch1.2.bn.bias', 'encoder.repeat_2.8.branch1.2.bn.running_mean', 'encoder.repeat_2.8.branch1.2.bn.running_var', 'encoder.repeat_2.8.conv2d.weight', 'encoder.repeat_2.8.conv2d.bias', 'encoder.block8.branch0.conv.weight', 'encoder.block8.branch0.bn.weight', 'encoder.block8.branch0.bn.bias', 'encoder.block8.branch0.bn.running_mean', 'encoder.block8.branch0.bn.running_var', 'encoder.block8.branch1.0.conv.weight', 'encoder.block8.branch1.0.bn.weight', 'encoder.block8.branch1.0.bn.bias', 'encoder.block8.branch1.0.bn.running_mean', 'encoder.block8.branch1.0.bn.running_var', 'encoder.block8.branch1.1.conv.weight', 'encoder.block8.branch1.1.bn.weight', 'encoder.block8.branch1.1.bn.bias', 'encoder.block8.branch1.1.bn.running_mean', 'encoder.block8.branch1.1.bn.running_var', 'encoder.block8.branch1.2.conv.weight', 'encoder.block8.branch1.2.bn.weight', 'encoder.block8.branch1.2.bn.bias', 'encoder.block8.branch1.2.bn.running_mean', 'encoder.block8.branch1.2.bn.running_var', 'encoder.block8.conv2d.weight', 'encoder.block8.conv2d.bias', 'encoder.conv2d_7b.conv.weight', 'encoder.conv2d_7b.bn.weight', 'encoder.conv2d_7b.bn.bias', 'encoder.conv2d_7b.bn.running_mean', 'encoder.conv2d_7b.bn.running_var', 'decoder.blocks.x_0_0.conv1.0.weight', 'decoder.blocks.x_0_0.conv1.1.weight', 'decoder.blocks.x_0_0.conv1.1.bias', 'decoder.blocks.x_0_0.conv1.1.running_mean', 'decoder.blocks.x_0_0.conv1.1.running_var', 'decoder.blocks.x_0_0.conv2.0.weight', 'decoder.blocks.x_0_0.conv2.1.weight', 'decoder.blocks.x_0_0.conv2.1.bias', 'decoder.blocks.x_0_0.conv2.1.running_mean', 'decoder.blocks.x_0_0.conv2.1.running_var', 'decoder.blocks.x_0_1.conv1.0.weight', 'decoder.blocks.x_0_1.conv1.1.weight', 'decoder.blocks.x_0_1.conv1.1.bias', 'decoder.blocks.x_0_1.conv1.1.running_mean', 'decoder.blocks.x_0_1.conv1.1.running_var', 'decoder.blocks.x_0_1.conv2.0.weight', 'decoder.blocks.x_0_1.conv2.1.weight', 'decoder.blocks.x_0_1.conv2.1.bias', 'decoder.blocks.x_0_1.conv2.1.running_mean', 'decoder.blocks.x_0_1.conv2.1.running_var', 'decoder.blocks.x_1_1.conv1.0.weight', 'decoder.blocks.x_1_1.conv1.1.weight', 'decoder.blocks.x_1_1.conv1.1.bias', 'decoder.blocks.x_1_1.conv1.1.running_mean', 'decoder.blocks.x_1_1.conv1.1.running_var', 'decoder.blocks.x_1_1.conv2.0.weight', 'decoder.blocks.x_1_1.conv2.1.weight', 'decoder.blocks.x_1_1.conv2.1.bias', 'decoder.blocks.x_1_1.conv2.1.running_mean', 'decoder.blocks.x_1_1.conv2.1.running_var', 'decoder.blocks.x_0_2.conv1.0.weight', 'decoder.blocks.x_0_2.conv1.1.weight', 'decoder.blocks.x_0_2.conv1.1.bias', 'decoder.blocks.x_0_2.conv1.1.running_mean', 'decoder.blocks.x_0_2.conv1.1.running_var', 'decoder.blocks.x_0_2.conv2.0.weight', 'decoder.blocks.x_0_2.conv2.1.weight', 'decoder.blocks.x_0_2.conv2.1.bias', 'decoder.blocks.x_0_2.conv2.1.running_mean', 'decoder.blocks.x_0_2.conv2.1.running_var', 'decoder.blocks.x_1_2.conv1.0.weight', 'decoder.blocks.x_1_2.conv1.1.weight', 'decoder.blocks.x_1_2.conv1.1.bias', 'decoder.blocks.x_1_2.conv1.1.running_mean', 'decoder.blocks.x_1_2.conv1.1.running_var', 'decoder.blocks.x_1_2.conv2.0.weight', 'decoder.blocks.x_1_2.conv2.1.weight', 'decoder.blocks.x_1_2.conv2.1.bias', 'decoder.blocks.x_1_2.conv2.1.running_mean', 'decoder.blocks.x_1_2.conv2.1.running_var', 'decoder.blocks.x_2_2.conv1.0.weight', 'decoder.blocks.x_2_2.conv1.1.weight', 'decoder.blocks.x_2_2.conv1.1.bias', 'decoder.blocks.x_2_2.conv1.1.running_mean', 'decoder.blocks.x_2_2.conv1.1.running_var', 'decoder.blocks.x_2_2.conv2.0.weight', 'decoder.blocks.x_2_2.conv2.1.weight', 'decoder.blocks.x_2_2.conv2.1.bias', 'decoder.blocks.x_2_2.conv2.1.running_mean', 'decoder.blocks.x_2_2.conv2.1.running_var', 'decoder.blocks.x_0_3.conv1.0.weight', 'decoder.blocks.x_0_3.conv1.1.weight', 'decoder.blocks.x_0_3.conv1.1.bias', 'decoder.blocks.x_0_3.conv1.1.running_mean', 'decoder.blocks.x_0_3.conv1.1.running_var', 'decoder.blocks.x_0_3.conv2.0.weight', 'decoder.blocks.x_0_3.conv2.1.weight', 'decoder.blocks.x_0_3.conv2.1.bias', 'decoder.blocks.x_0_3.conv2.1.running_mean', 'decoder.blocks.x_0_3.conv2.1.running_var', 'decoder.blocks.x_1_3.conv1.0.weight', 'decoder.blocks.x_1_3.conv1.1.weight', 'decoder.blocks.x_1_3.conv1.1.bias', 'decoder.blocks.x_1_3.conv1.1.running_mean', 'decoder.blocks.x_1_3.conv1.1.running_var', 'decoder.blocks.x_1_3.conv2.0.weight', 'decoder.blocks.x_1_3.conv2.1.weight', 'decoder.blocks.x_1_3.conv2.1.bias', 'decoder.blocks.x_1_3.conv2.1.running_mean', 'decoder.blocks.x_1_3.conv2.1.running_var', 'decoder.blocks.x_2_3.conv1.0.weight', 'decoder.blocks.x_2_3.conv1.1.weight', 'decoder.blocks.x_2_3.conv1.1.bias', 'decoder.blocks.x_2_3.conv1.1.running_mean', 'decoder.blocks.x_2_3.conv1.1.running_var', 'decoder.blocks.x_2_3.conv2.0.weight', 'decoder.blocks.x_2_3.conv2.1.weight', 'decoder.blocks.x_2_3.conv2.1.bias', 'decoder.blocks.x_2_3.conv2.1.running_mean', 'decoder.blocks.x_2_3.conv2.1.running_var', 'decoder.blocks.x_3_3.conv1.0.weight', 'decoder.blocks.x_3_3.conv1.1.weight', 'decoder.blocks.x_3_3.conv1.1.bias', 'decoder.blocks.x_3_3.conv1.1.running_mean', 'decoder.blocks.x_3_3.conv1.1.running_var', 'decoder.blocks.x_3_3.conv2.0.weight', 'decoder.blocks.x_3_3.conv2.1.weight', 'decoder.blocks.x_3_3.conv2.1.bias', 'decoder.blocks.x_3_3.conv2.1.running_mean', 'decoder.blocks.x_3_3.conv2.1.running_var', 'decoder.blocks.x_0_4.conv1.0.weight', 'decoder.blocks.x_0_4.conv1.1.weight', 'decoder.blocks.x_0_4.conv1.1.bias', 'decoder.blocks.x_0_4.conv1.1.running_mean', 'decoder.blocks.x_0_4.conv1.1.running_var', 'decoder.blocks.x_0_4.conv2.0.weight', 'decoder.blocks.x_0_4.conv2.1.weight', 'decoder.blocks.x_0_4.conv2.1.bias', 'decoder.blocks.x_0_4.conv2.1.running_mean', 'decoder.blocks.x_0_4.conv2.1.running_var', 'segmentation_head.0.weight', 'segmentation_head.0.bias'], unexpected_keys=['state_dict', 'logs', 'epoch'])"]},"metadata":{},"execution_count":25}],"source":["weight = torch.load('/content/gdrive/MyDrive/sw_contest/pretrained/models/stage3/inrv2-f0/checkpoints/best.pth')\n","model3 = smp.UnetPlusPlus(encoder_name=\"inceptionresnetv2\", encoder_weights=None, in_channels=3, classes=num_classes)\n","model3.to(device)\n","model3.load_state_dict(weight, strict=False)\n"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1906,"status":"ok","timestamp":1704927582416,"user":{"displayName":"조영민","userId":"10749653391039175177"},"user_tz":-540},"id":"8SQY-tq_K0r6","outputId":"0519fdda-965d-466a-9df2-2209cba94eb2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["_IncompatibleKeys(missing_keys=['encoder.layer0.conv1.weight', 'encoder.layer0.bn1.weight', 'encoder.layer0.bn1.bias', 'encoder.layer0.bn1.running_mean', 'encoder.layer0.bn1.running_var', 'encoder.layer1.0.conv1.weight', 'encoder.layer1.0.bn1.weight', 'encoder.layer1.0.bn1.bias', 'encoder.layer1.0.bn1.running_mean', 'encoder.layer1.0.bn1.running_var', 'encoder.layer1.0.conv2.weight', 'encoder.layer1.0.bn2.weight', 'encoder.layer1.0.bn2.bias', 'encoder.layer1.0.bn2.running_mean', 'encoder.layer1.0.bn2.running_var', 'encoder.layer1.0.conv3.weight', 'encoder.layer1.0.bn3.weight', 'encoder.layer1.0.bn3.bias', 'encoder.layer1.0.bn3.running_mean', 'encoder.layer1.0.bn3.running_var', 'encoder.layer1.0.se_module.fc1.weight', 'encoder.layer1.0.se_module.fc1.bias', 'encoder.layer1.0.se_module.fc2.weight', 'encoder.layer1.0.se_module.fc2.bias', 'encoder.layer1.0.downsample.0.weight', 'encoder.layer1.0.downsample.1.weight', 'encoder.layer1.0.downsample.1.bias', 'encoder.layer1.0.downsample.1.running_mean', 'encoder.layer1.0.downsample.1.running_var', 'encoder.layer1.1.conv1.weight', 'encoder.layer1.1.bn1.weight', 'encoder.layer1.1.bn1.bias', 'encoder.layer1.1.bn1.running_mean', 'encoder.layer1.1.bn1.running_var', 'encoder.layer1.1.conv2.weight', 'encoder.layer1.1.bn2.weight', 'encoder.layer1.1.bn2.bias', 'encoder.layer1.1.bn2.running_mean', 'encoder.layer1.1.bn2.running_var', 'encoder.layer1.1.conv3.weight', 'encoder.layer1.1.bn3.weight', 'encoder.layer1.1.bn3.bias', 'encoder.layer1.1.bn3.running_mean', 'encoder.layer1.1.bn3.running_var', 'encoder.layer1.1.se_module.fc1.weight', 'encoder.layer1.1.se_module.fc1.bias', 'encoder.layer1.1.se_module.fc2.weight', 'encoder.layer1.1.se_module.fc2.bias', 'encoder.layer1.2.conv1.weight', 'encoder.layer1.2.bn1.weight', 'encoder.layer1.2.bn1.bias', 'encoder.layer1.2.bn1.running_mean', 'encoder.layer1.2.bn1.running_var', 'encoder.layer1.2.conv2.weight', 'encoder.layer1.2.bn2.weight', 'encoder.layer1.2.bn2.bias', 'encoder.layer1.2.bn2.running_mean', 'encoder.layer1.2.bn2.running_var', 'encoder.layer1.2.conv3.weight', 'encoder.layer1.2.bn3.weight', 'encoder.layer1.2.bn3.bias', 'encoder.layer1.2.bn3.running_mean', 'encoder.layer1.2.bn3.running_var', 'encoder.layer1.2.se_module.fc1.weight', 'encoder.layer1.2.se_module.fc1.bias', 'encoder.layer1.2.se_module.fc2.weight', 'encoder.layer1.2.se_module.fc2.bias', 'encoder.layer2.0.conv1.weight', 'encoder.layer2.0.bn1.weight', 'encoder.layer2.0.bn1.bias', 'encoder.layer2.0.bn1.running_mean', 'encoder.layer2.0.bn1.running_var', 'encoder.layer2.0.conv2.weight', 'encoder.layer2.0.bn2.weight', 'encoder.layer2.0.bn2.bias', 'encoder.layer2.0.bn2.running_mean', 'encoder.layer2.0.bn2.running_var', 'encoder.layer2.0.conv3.weight', 'encoder.layer2.0.bn3.weight', 'encoder.layer2.0.bn3.bias', 'encoder.layer2.0.bn3.running_mean', 'encoder.layer2.0.bn3.running_var', 'encoder.layer2.0.se_module.fc1.weight', 'encoder.layer2.0.se_module.fc1.bias', 'encoder.layer2.0.se_module.fc2.weight', 'encoder.layer2.0.se_module.fc2.bias', 'encoder.layer2.0.downsample.0.weight', 'encoder.layer2.0.downsample.1.weight', 'encoder.layer2.0.downsample.1.bias', 'encoder.layer2.0.downsample.1.running_mean', 'encoder.layer2.0.downsample.1.running_var', 'encoder.layer2.1.conv1.weight', 'encoder.layer2.1.bn1.weight', 'encoder.layer2.1.bn1.bias', 'encoder.layer2.1.bn1.running_mean', 'encoder.layer2.1.bn1.running_var', 'encoder.layer2.1.conv2.weight', 'encoder.layer2.1.bn2.weight', 'encoder.layer2.1.bn2.bias', 'encoder.layer2.1.bn2.running_mean', 'encoder.layer2.1.bn2.running_var', 'encoder.layer2.1.conv3.weight', 'encoder.layer2.1.bn3.weight', 'encoder.layer2.1.bn3.bias', 'encoder.layer2.1.bn3.running_mean', 'encoder.layer2.1.bn3.running_var', 'encoder.layer2.1.se_module.fc1.weight', 'encoder.layer2.1.se_module.fc1.bias', 'encoder.layer2.1.se_module.fc2.weight', 'encoder.layer2.1.se_module.fc2.bias', 'encoder.layer2.2.conv1.weight', 'encoder.layer2.2.bn1.weight', 'encoder.layer2.2.bn1.bias', 'encoder.layer2.2.bn1.running_mean', 'encoder.layer2.2.bn1.running_var', 'encoder.layer2.2.conv2.weight', 'encoder.layer2.2.bn2.weight', 'encoder.layer2.2.bn2.bias', 'encoder.layer2.2.bn2.running_mean', 'encoder.layer2.2.bn2.running_var', 'encoder.layer2.2.conv3.weight', 'encoder.layer2.2.bn3.weight', 'encoder.layer2.2.bn3.bias', 'encoder.layer2.2.bn3.running_mean', 'encoder.layer2.2.bn3.running_var', 'encoder.layer2.2.se_module.fc1.weight', 'encoder.layer2.2.se_module.fc1.bias', 'encoder.layer2.2.se_module.fc2.weight', 'encoder.layer2.2.se_module.fc2.bias', 'encoder.layer2.3.conv1.weight', 'encoder.layer2.3.bn1.weight', 'encoder.layer2.3.bn1.bias', 'encoder.layer2.3.bn1.running_mean', 'encoder.layer2.3.bn1.running_var', 'encoder.layer2.3.conv2.weight', 'encoder.layer2.3.bn2.weight', 'encoder.layer2.3.bn2.bias', 'encoder.layer2.3.bn2.running_mean', 'encoder.layer2.3.bn2.running_var', 'encoder.layer2.3.conv3.weight', 'encoder.layer2.3.bn3.weight', 'encoder.layer2.3.bn3.bias', 'encoder.layer2.3.bn3.running_mean', 'encoder.layer2.3.bn3.running_var', 'encoder.layer2.3.se_module.fc1.weight', 'encoder.layer2.3.se_module.fc1.bias', 'encoder.layer2.3.se_module.fc2.weight', 'encoder.layer2.3.se_module.fc2.bias', 'encoder.layer3.0.conv1.weight', 'encoder.layer3.0.bn1.weight', 'encoder.layer3.0.bn1.bias', 'encoder.layer3.0.bn1.running_mean', 'encoder.layer3.0.bn1.running_var', 'encoder.layer3.0.conv2.weight', 'encoder.layer3.0.bn2.weight', 'encoder.layer3.0.bn2.bias', 'encoder.layer3.0.bn2.running_mean', 'encoder.layer3.0.bn2.running_var', 'encoder.layer3.0.conv3.weight', 'encoder.layer3.0.bn3.weight', 'encoder.layer3.0.bn3.bias', 'encoder.layer3.0.bn3.running_mean', 'encoder.layer3.0.bn3.running_var', 'encoder.layer3.0.se_module.fc1.weight', 'encoder.layer3.0.se_module.fc1.bias', 'encoder.layer3.0.se_module.fc2.weight', 'encoder.layer3.0.se_module.fc2.bias', 'encoder.layer3.0.downsample.0.weight', 'encoder.layer3.0.downsample.1.weight', 'encoder.layer3.0.downsample.1.bias', 'encoder.layer3.0.downsample.1.running_mean', 'encoder.layer3.0.downsample.1.running_var', 'encoder.layer3.1.conv1.weight', 'encoder.layer3.1.bn1.weight', 'encoder.layer3.1.bn1.bias', 'encoder.layer3.1.bn1.running_mean', 'encoder.layer3.1.bn1.running_var', 'encoder.layer3.1.conv2.weight', 'encoder.layer3.1.bn2.weight', 'encoder.layer3.1.bn2.bias', 'encoder.layer3.1.bn2.running_mean', 'encoder.layer3.1.bn2.running_var', 'encoder.layer3.1.conv3.weight', 'encoder.layer3.1.bn3.weight', 'encoder.layer3.1.bn3.bias', 'encoder.layer3.1.bn3.running_mean', 'encoder.layer3.1.bn3.running_var', 'encoder.layer3.1.se_module.fc1.weight', 'encoder.layer3.1.se_module.fc1.bias', 'encoder.layer3.1.se_module.fc2.weight', 'encoder.layer3.1.se_module.fc2.bias', 'encoder.layer3.2.conv1.weight', 'encoder.layer3.2.bn1.weight', 'encoder.layer3.2.bn1.bias', 'encoder.layer3.2.bn1.running_mean', 'encoder.layer3.2.bn1.running_var', 'encoder.layer3.2.conv2.weight', 'encoder.layer3.2.bn2.weight', 'encoder.layer3.2.bn2.bias', 'encoder.layer3.2.bn2.running_mean', 'encoder.layer3.2.bn2.running_var', 'encoder.layer3.2.conv3.weight', 'encoder.layer3.2.bn3.weight', 'encoder.layer3.2.bn3.bias', 'encoder.layer3.2.bn3.running_mean', 'encoder.layer3.2.bn3.running_var', 'encoder.layer3.2.se_module.fc1.weight', 'encoder.layer3.2.se_module.fc1.bias', 'encoder.layer3.2.se_module.fc2.weight', 'encoder.layer3.2.se_module.fc2.bias', 'encoder.layer3.3.conv1.weight', 'encoder.layer3.3.bn1.weight', 'encoder.layer3.3.bn1.bias', 'encoder.layer3.3.bn1.running_mean', 'encoder.layer3.3.bn1.running_var', 'encoder.layer3.3.conv2.weight', 'encoder.layer3.3.bn2.weight', 'encoder.layer3.3.bn2.bias', 'encoder.layer3.3.bn2.running_mean', 'encoder.layer3.3.bn2.running_var', 'encoder.layer3.3.conv3.weight', 'encoder.layer3.3.bn3.weight', 'encoder.layer3.3.bn3.bias', 'encoder.layer3.3.bn3.running_mean', 'encoder.layer3.3.bn3.running_var', 'encoder.layer3.3.se_module.fc1.weight', 'encoder.layer3.3.se_module.fc1.bias', 'encoder.layer3.3.se_module.fc2.weight', 'encoder.layer3.3.se_module.fc2.bias', 'encoder.layer3.4.conv1.weight', 'encoder.layer3.4.bn1.weight', 'encoder.layer3.4.bn1.bias', 'encoder.layer3.4.bn1.running_mean', 'encoder.layer3.4.bn1.running_var', 'encoder.layer3.4.conv2.weight', 'encoder.layer3.4.bn2.weight', 'encoder.layer3.4.bn2.bias', 'encoder.layer3.4.bn2.running_mean', 'encoder.layer3.4.bn2.running_var', 'encoder.layer3.4.conv3.weight', 'encoder.layer3.4.bn3.weight', 'encoder.layer3.4.bn3.bias', 'encoder.layer3.4.bn3.running_mean', 'encoder.layer3.4.bn3.running_var', 'encoder.layer3.4.se_module.fc1.weight', 'encoder.layer3.4.se_module.fc1.bias', 'encoder.layer3.4.se_module.fc2.weight', 'encoder.layer3.4.se_module.fc2.bias', 'encoder.layer3.5.conv1.weight', 'encoder.layer3.5.bn1.weight', 'encoder.layer3.5.bn1.bias', 'encoder.layer3.5.bn1.running_mean', 'encoder.layer3.5.bn1.running_var', 'encoder.layer3.5.conv2.weight', 'encoder.layer3.5.bn2.weight', 'encoder.layer3.5.bn2.bias', 'encoder.layer3.5.bn2.running_mean', 'encoder.layer3.5.bn2.running_var', 'encoder.layer3.5.conv3.weight', 'encoder.layer3.5.bn3.weight', 'encoder.layer3.5.bn3.bias', 'encoder.layer3.5.bn3.running_mean', 'encoder.layer3.5.bn3.running_var', 'encoder.layer3.5.se_module.fc1.weight', 'encoder.layer3.5.se_module.fc1.bias', 'encoder.layer3.5.se_module.fc2.weight', 'encoder.layer3.5.se_module.fc2.bias', 'encoder.layer4.0.conv1.weight', 'encoder.layer4.0.bn1.weight', 'encoder.layer4.0.bn1.bias', 'encoder.layer4.0.bn1.running_mean', 'encoder.layer4.0.bn1.running_var', 'encoder.layer4.0.conv2.weight', 'encoder.layer4.0.bn2.weight', 'encoder.layer4.0.bn2.bias', 'encoder.layer4.0.bn2.running_mean', 'encoder.layer4.0.bn2.running_var', 'encoder.layer4.0.conv3.weight', 'encoder.layer4.0.bn3.weight', 'encoder.layer4.0.bn3.bias', 'encoder.layer4.0.bn3.running_mean', 'encoder.layer4.0.bn3.running_var', 'encoder.layer4.0.se_module.fc1.weight', 'encoder.layer4.0.se_module.fc1.bias', 'encoder.layer4.0.se_module.fc2.weight', 'encoder.layer4.0.se_module.fc2.bias', 'encoder.layer4.0.downsample.0.weight', 'encoder.layer4.0.downsample.1.weight', 'encoder.layer4.0.downsample.1.bias', 'encoder.layer4.0.downsample.1.running_mean', 'encoder.layer4.0.downsample.1.running_var', 'encoder.layer4.1.conv1.weight', 'encoder.layer4.1.bn1.weight', 'encoder.layer4.1.bn1.bias', 'encoder.layer4.1.bn1.running_mean', 'encoder.layer4.1.bn1.running_var', 'encoder.layer4.1.conv2.weight', 'encoder.layer4.1.bn2.weight', 'encoder.layer4.1.bn2.bias', 'encoder.layer4.1.bn2.running_mean', 'encoder.layer4.1.bn2.running_var', 'encoder.layer4.1.conv3.weight', 'encoder.layer4.1.bn3.weight', 'encoder.layer4.1.bn3.bias', 'encoder.layer4.1.bn3.running_mean', 'encoder.layer4.1.bn3.running_var', 'encoder.layer4.1.se_module.fc1.weight', 'encoder.layer4.1.se_module.fc1.bias', 'encoder.layer4.1.se_module.fc2.weight', 'encoder.layer4.1.se_module.fc2.bias', 'encoder.layer4.2.conv1.weight', 'encoder.layer4.2.bn1.weight', 'encoder.layer4.2.bn1.bias', 'encoder.layer4.2.bn1.running_mean', 'encoder.layer4.2.bn1.running_var', 'encoder.layer4.2.conv2.weight', 'encoder.layer4.2.bn2.weight', 'encoder.layer4.2.bn2.bias', 'encoder.layer4.2.bn2.running_mean', 'encoder.layer4.2.bn2.running_var', 'encoder.layer4.2.conv3.weight', 'encoder.layer4.2.bn3.weight', 'encoder.layer4.2.bn3.bias', 'encoder.layer4.2.bn3.running_mean', 'encoder.layer4.2.bn3.running_var', 'encoder.layer4.2.se_module.fc1.weight', 'encoder.layer4.2.se_module.fc1.bias', 'encoder.layer4.2.se_module.fc2.weight', 'encoder.layer4.2.se_module.fc2.bias', 'decoder.blocks.x_0_0.conv1.0.weight', 'decoder.blocks.x_0_0.conv1.1.weight', 'decoder.blocks.x_0_0.conv1.1.bias', 'decoder.blocks.x_0_0.conv1.1.running_mean', 'decoder.blocks.x_0_0.conv1.1.running_var', 'decoder.blocks.x_0_0.conv2.0.weight', 'decoder.blocks.x_0_0.conv2.1.weight', 'decoder.blocks.x_0_0.conv2.1.bias', 'decoder.blocks.x_0_0.conv2.1.running_mean', 'decoder.blocks.x_0_0.conv2.1.running_var', 'decoder.blocks.x_0_1.conv1.0.weight', 'decoder.blocks.x_0_1.conv1.1.weight', 'decoder.blocks.x_0_1.conv1.1.bias', 'decoder.blocks.x_0_1.conv1.1.running_mean', 'decoder.blocks.x_0_1.conv1.1.running_var', 'decoder.blocks.x_0_1.conv2.0.weight', 'decoder.blocks.x_0_1.conv2.1.weight', 'decoder.blocks.x_0_1.conv2.1.bias', 'decoder.blocks.x_0_1.conv2.1.running_mean', 'decoder.blocks.x_0_1.conv2.1.running_var', 'decoder.blocks.x_1_1.conv1.0.weight', 'decoder.blocks.x_1_1.conv1.1.weight', 'decoder.blocks.x_1_1.conv1.1.bias', 'decoder.blocks.x_1_1.conv1.1.running_mean', 'decoder.blocks.x_1_1.conv1.1.running_var', 'decoder.blocks.x_1_1.conv2.0.weight', 'decoder.blocks.x_1_1.conv2.1.weight', 'decoder.blocks.x_1_1.conv2.1.bias', 'decoder.blocks.x_1_1.conv2.1.running_mean', 'decoder.blocks.x_1_1.conv2.1.running_var', 'decoder.blocks.x_0_2.conv1.0.weight', 'decoder.blocks.x_0_2.conv1.1.weight', 'decoder.blocks.x_0_2.conv1.1.bias', 'decoder.blocks.x_0_2.conv1.1.running_mean', 'decoder.blocks.x_0_2.conv1.1.running_var', 'decoder.blocks.x_0_2.conv2.0.weight', 'decoder.blocks.x_0_2.conv2.1.weight', 'decoder.blocks.x_0_2.conv2.1.bias', 'decoder.blocks.x_0_2.conv2.1.running_mean', 'decoder.blocks.x_0_2.conv2.1.running_var', 'decoder.blocks.x_1_2.conv1.0.weight', 'decoder.blocks.x_1_2.conv1.1.weight', 'decoder.blocks.x_1_2.conv1.1.bias', 'decoder.blocks.x_1_2.conv1.1.running_mean', 'decoder.blocks.x_1_2.conv1.1.running_var', 'decoder.blocks.x_1_2.conv2.0.weight', 'decoder.blocks.x_1_2.conv2.1.weight', 'decoder.blocks.x_1_2.conv2.1.bias', 'decoder.blocks.x_1_2.conv2.1.running_mean', 'decoder.blocks.x_1_2.conv2.1.running_var', 'decoder.blocks.x_2_2.conv1.0.weight', 'decoder.blocks.x_2_2.conv1.1.weight', 'decoder.blocks.x_2_2.conv1.1.bias', 'decoder.blocks.x_2_2.conv1.1.running_mean', 'decoder.blocks.x_2_2.conv1.1.running_var', 'decoder.blocks.x_2_2.conv2.0.weight', 'decoder.blocks.x_2_2.conv2.1.weight', 'decoder.blocks.x_2_2.conv2.1.bias', 'decoder.blocks.x_2_2.conv2.1.running_mean', 'decoder.blocks.x_2_2.conv2.1.running_var', 'decoder.blocks.x_0_3.conv1.0.weight', 'decoder.blocks.x_0_3.conv1.1.weight', 'decoder.blocks.x_0_3.conv1.1.bias', 'decoder.blocks.x_0_3.conv1.1.running_mean', 'decoder.blocks.x_0_3.conv1.1.running_var', 'decoder.blocks.x_0_3.conv2.0.weight', 'decoder.blocks.x_0_3.conv2.1.weight', 'decoder.blocks.x_0_3.conv2.1.bias', 'decoder.blocks.x_0_3.conv2.1.running_mean', 'decoder.blocks.x_0_3.conv2.1.running_var', 'decoder.blocks.x_1_3.conv1.0.weight', 'decoder.blocks.x_1_3.conv1.1.weight', 'decoder.blocks.x_1_3.conv1.1.bias', 'decoder.blocks.x_1_3.conv1.1.running_mean', 'decoder.blocks.x_1_3.conv1.1.running_var', 'decoder.blocks.x_1_3.conv2.0.weight', 'decoder.blocks.x_1_3.conv2.1.weight', 'decoder.blocks.x_1_3.conv2.1.bias', 'decoder.blocks.x_1_3.conv2.1.running_mean', 'decoder.blocks.x_1_3.conv2.1.running_var', 'decoder.blocks.x_2_3.conv1.0.weight', 'decoder.blocks.x_2_3.conv1.1.weight', 'decoder.blocks.x_2_3.conv1.1.bias', 'decoder.blocks.x_2_3.conv1.1.running_mean', 'decoder.blocks.x_2_3.conv1.1.running_var', 'decoder.blocks.x_2_3.conv2.0.weight', 'decoder.blocks.x_2_3.conv2.1.weight', 'decoder.blocks.x_2_3.conv2.1.bias', 'decoder.blocks.x_2_3.conv2.1.running_mean', 'decoder.blocks.x_2_3.conv2.1.running_var', 'decoder.blocks.x_3_3.conv1.0.weight', 'decoder.blocks.x_3_3.conv1.1.weight', 'decoder.blocks.x_3_3.conv1.1.bias', 'decoder.blocks.x_3_3.conv1.1.running_mean', 'decoder.blocks.x_3_3.conv1.1.running_var', 'decoder.blocks.x_3_3.conv2.0.weight', 'decoder.blocks.x_3_3.conv2.1.weight', 'decoder.blocks.x_3_3.conv2.1.bias', 'decoder.blocks.x_3_3.conv2.1.running_mean', 'decoder.blocks.x_3_3.conv2.1.running_var', 'decoder.blocks.x_0_4.conv1.0.weight', 'decoder.blocks.x_0_4.conv1.1.weight', 'decoder.blocks.x_0_4.conv1.1.bias', 'decoder.blocks.x_0_4.conv1.1.running_mean', 'decoder.blocks.x_0_4.conv1.1.running_var', 'decoder.blocks.x_0_4.conv2.0.weight', 'decoder.blocks.x_0_4.conv2.1.weight', 'decoder.blocks.x_0_4.conv2.1.bias', 'decoder.blocks.x_0_4.conv2.1.running_mean', 'decoder.blocks.x_0_4.conv2.1.running_var', 'segmentation_head.0.weight', 'segmentation_head.0.bias'], unexpected_keys=['state_dict', 'logs', 'epoch'])"]},"metadata":{},"execution_count":26}],"source":["weight = torch.load('/content/gdrive/MyDrive/sw_contest/pretrained/models/stage3/srx50-2-f0/checkpoints/best.pth')\n","model4 = smp.UnetPlusPlus(encoder_name=\"se_resnext50_32x4d\", encoder_weights=None, in_channels=3, classes=num_classes)\n","model4.to(device)\n","model4.load_state_dict(weight, strict=False)\n","\n"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1350,"status":"ok","timestamp":1704927583763,"user":{"displayName":"조영민","userId":"10749653391039175177"},"user_tz":-540},"id":"xW5_hRreM4Po","outputId":"ac0a60b4-8f67-4469-820b-750c11d5c9e7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["_IncompatibleKeys(missing_keys=['encoder.layer0.conv1.weight', 'encoder.layer0.bn1.weight', 'encoder.layer0.bn1.bias', 'encoder.layer0.bn1.running_mean', 'encoder.layer0.bn1.running_var', 'encoder.layer1.0.conv1.weight', 'encoder.layer1.0.bn1.weight', 'encoder.layer1.0.bn1.bias', 'encoder.layer1.0.bn1.running_mean', 'encoder.layer1.0.bn1.running_var', 'encoder.layer1.0.conv2.weight', 'encoder.layer1.0.bn2.weight', 'encoder.layer1.0.bn2.bias', 'encoder.layer1.0.bn2.running_mean', 'encoder.layer1.0.bn2.running_var', 'encoder.layer1.0.conv3.weight', 'encoder.layer1.0.bn3.weight', 'encoder.layer1.0.bn3.bias', 'encoder.layer1.0.bn3.running_mean', 'encoder.layer1.0.bn3.running_var', 'encoder.layer1.0.se_module.fc1.weight', 'encoder.layer1.0.se_module.fc1.bias', 'encoder.layer1.0.se_module.fc2.weight', 'encoder.layer1.0.se_module.fc2.bias', 'encoder.layer1.0.downsample.0.weight', 'encoder.layer1.0.downsample.1.weight', 'encoder.layer1.0.downsample.1.bias', 'encoder.layer1.0.downsample.1.running_mean', 'encoder.layer1.0.downsample.1.running_var', 'encoder.layer1.1.conv1.weight', 'encoder.layer1.1.bn1.weight', 'encoder.layer1.1.bn1.bias', 'encoder.layer1.1.bn1.running_mean', 'encoder.layer1.1.bn1.running_var', 'encoder.layer1.1.conv2.weight', 'encoder.layer1.1.bn2.weight', 'encoder.layer1.1.bn2.bias', 'encoder.layer1.1.bn2.running_mean', 'encoder.layer1.1.bn2.running_var', 'encoder.layer1.1.conv3.weight', 'encoder.layer1.1.bn3.weight', 'encoder.layer1.1.bn3.bias', 'encoder.layer1.1.bn3.running_mean', 'encoder.layer1.1.bn3.running_var', 'encoder.layer1.1.se_module.fc1.weight', 'encoder.layer1.1.se_module.fc1.bias', 'encoder.layer1.1.se_module.fc2.weight', 'encoder.layer1.1.se_module.fc2.bias', 'encoder.layer1.2.conv1.weight', 'encoder.layer1.2.bn1.weight', 'encoder.layer1.2.bn1.bias', 'encoder.layer1.2.bn1.running_mean', 'encoder.layer1.2.bn1.running_var', 'encoder.layer1.2.conv2.weight', 'encoder.layer1.2.bn2.weight', 'encoder.layer1.2.bn2.bias', 'encoder.layer1.2.bn2.running_mean', 'encoder.layer1.2.bn2.running_var', 'encoder.layer1.2.conv3.weight', 'encoder.layer1.2.bn3.weight', 'encoder.layer1.2.bn3.bias', 'encoder.layer1.2.bn3.running_mean', 'encoder.layer1.2.bn3.running_var', 'encoder.layer1.2.se_module.fc1.weight', 'encoder.layer1.2.se_module.fc1.bias', 'encoder.layer1.2.se_module.fc2.weight', 'encoder.layer1.2.se_module.fc2.bias', 'encoder.layer2.0.conv1.weight', 'encoder.layer2.0.bn1.weight', 'encoder.layer2.0.bn1.bias', 'encoder.layer2.0.bn1.running_mean', 'encoder.layer2.0.bn1.running_var', 'encoder.layer2.0.conv2.weight', 'encoder.layer2.0.bn2.weight', 'encoder.layer2.0.bn2.bias', 'encoder.layer2.0.bn2.running_mean', 'encoder.layer2.0.bn2.running_var', 'encoder.layer2.0.conv3.weight', 'encoder.layer2.0.bn3.weight', 'encoder.layer2.0.bn3.bias', 'encoder.layer2.0.bn3.running_mean', 'encoder.layer2.0.bn3.running_var', 'encoder.layer2.0.se_module.fc1.weight', 'encoder.layer2.0.se_module.fc1.bias', 'encoder.layer2.0.se_module.fc2.weight', 'encoder.layer2.0.se_module.fc2.bias', 'encoder.layer2.0.downsample.0.weight', 'encoder.layer2.0.downsample.1.weight', 'encoder.layer2.0.downsample.1.bias', 'encoder.layer2.0.downsample.1.running_mean', 'encoder.layer2.0.downsample.1.running_var', 'encoder.layer2.1.conv1.weight', 'encoder.layer2.1.bn1.weight', 'encoder.layer2.1.bn1.bias', 'encoder.layer2.1.bn1.running_mean', 'encoder.layer2.1.bn1.running_var', 'encoder.layer2.1.conv2.weight', 'encoder.layer2.1.bn2.weight', 'encoder.layer2.1.bn2.bias', 'encoder.layer2.1.bn2.running_mean', 'encoder.layer2.1.bn2.running_var', 'encoder.layer2.1.conv3.weight', 'encoder.layer2.1.bn3.weight', 'encoder.layer2.1.bn3.bias', 'encoder.layer2.1.bn3.running_mean', 'encoder.layer2.1.bn3.running_var', 'encoder.layer2.1.se_module.fc1.weight', 'encoder.layer2.1.se_module.fc1.bias', 'encoder.layer2.1.se_module.fc2.weight', 'encoder.layer2.1.se_module.fc2.bias', 'encoder.layer2.2.conv1.weight', 'encoder.layer2.2.bn1.weight', 'encoder.layer2.2.bn1.bias', 'encoder.layer2.2.bn1.running_mean', 'encoder.layer2.2.bn1.running_var', 'encoder.layer2.2.conv2.weight', 'encoder.layer2.2.bn2.weight', 'encoder.layer2.2.bn2.bias', 'encoder.layer2.2.bn2.running_mean', 'encoder.layer2.2.bn2.running_var', 'encoder.layer2.2.conv3.weight', 'encoder.layer2.2.bn3.weight', 'encoder.layer2.2.bn3.bias', 'encoder.layer2.2.bn3.running_mean', 'encoder.layer2.2.bn3.running_var', 'encoder.layer2.2.se_module.fc1.weight', 'encoder.layer2.2.se_module.fc1.bias', 'encoder.layer2.2.se_module.fc2.weight', 'encoder.layer2.2.se_module.fc2.bias', 'encoder.layer2.3.conv1.weight', 'encoder.layer2.3.bn1.weight', 'encoder.layer2.3.bn1.bias', 'encoder.layer2.3.bn1.running_mean', 'encoder.layer2.3.bn1.running_var', 'encoder.layer2.3.conv2.weight', 'encoder.layer2.3.bn2.weight', 'encoder.layer2.3.bn2.bias', 'encoder.layer2.3.bn2.running_mean', 'encoder.layer2.3.bn2.running_var', 'encoder.layer2.3.conv3.weight', 'encoder.layer2.3.bn3.weight', 'encoder.layer2.3.bn3.bias', 'encoder.layer2.3.bn3.running_mean', 'encoder.layer2.3.bn3.running_var', 'encoder.layer2.3.se_module.fc1.weight', 'encoder.layer2.3.se_module.fc1.bias', 'encoder.layer2.3.se_module.fc2.weight', 'encoder.layer2.3.se_module.fc2.bias', 'encoder.layer3.0.conv1.weight', 'encoder.layer3.0.bn1.weight', 'encoder.layer3.0.bn1.bias', 'encoder.layer3.0.bn1.running_mean', 'encoder.layer3.0.bn1.running_var', 'encoder.layer3.0.conv2.weight', 'encoder.layer3.0.bn2.weight', 'encoder.layer3.0.bn2.bias', 'encoder.layer3.0.bn2.running_mean', 'encoder.layer3.0.bn2.running_var', 'encoder.layer3.0.conv3.weight', 'encoder.layer3.0.bn3.weight', 'encoder.layer3.0.bn3.bias', 'encoder.layer3.0.bn3.running_mean', 'encoder.layer3.0.bn3.running_var', 'encoder.layer3.0.se_module.fc1.weight', 'encoder.layer3.0.se_module.fc1.bias', 'encoder.layer3.0.se_module.fc2.weight', 'encoder.layer3.0.se_module.fc2.bias', 'encoder.layer3.0.downsample.0.weight', 'encoder.layer3.0.downsample.1.weight', 'encoder.layer3.0.downsample.1.bias', 'encoder.layer3.0.downsample.1.running_mean', 'encoder.layer3.0.downsample.1.running_var', 'encoder.layer3.1.conv1.weight', 'encoder.layer3.1.bn1.weight', 'encoder.layer3.1.bn1.bias', 'encoder.layer3.1.bn1.running_mean', 'encoder.layer3.1.bn1.running_var', 'encoder.layer3.1.conv2.weight', 'encoder.layer3.1.bn2.weight', 'encoder.layer3.1.bn2.bias', 'encoder.layer3.1.bn2.running_mean', 'encoder.layer3.1.bn2.running_var', 'encoder.layer3.1.conv3.weight', 'encoder.layer3.1.bn3.weight', 'encoder.layer3.1.bn3.bias', 'encoder.layer3.1.bn3.running_mean', 'encoder.layer3.1.bn3.running_var', 'encoder.layer3.1.se_module.fc1.weight', 'encoder.layer3.1.se_module.fc1.bias', 'encoder.layer3.1.se_module.fc2.weight', 'encoder.layer3.1.se_module.fc2.bias', 'encoder.layer3.2.conv1.weight', 'encoder.layer3.2.bn1.weight', 'encoder.layer3.2.bn1.bias', 'encoder.layer3.2.bn1.running_mean', 'encoder.layer3.2.bn1.running_var', 'encoder.layer3.2.conv2.weight', 'encoder.layer3.2.bn2.weight', 'encoder.layer3.2.bn2.bias', 'encoder.layer3.2.bn2.running_mean', 'encoder.layer3.2.bn2.running_var', 'encoder.layer3.2.conv3.weight', 'encoder.layer3.2.bn3.weight', 'encoder.layer3.2.bn3.bias', 'encoder.layer3.2.bn3.running_mean', 'encoder.layer3.2.bn3.running_var', 'encoder.layer3.2.se_module.fc1.weight', 'encoder.layer3.2.se_module.fc1.bias', 'encoder.layer3.2.se_module.fc2.weight', 'encoder.layer3.2.se_module.fc2.bias', 'encoder.layer3.3.conv1.weight', 'encoder.layer3.3.bn1.weight', 'encoder.layer3.3.bn1.bias', 'encoder.layer3.3.bn1.running_mean', 'encoder.layer3.3.bn1.running_var', 'encoder.layer3.3.conv2.weight', 'encoder.layer3.3.bn2.weight', 'encoder.layer3.3.bn2.bias', 'encoder.layer3.3.bn2.running_mean', 'encoder.layer3.3.bn2.running_var', 'encoder.layer3.3.conv3.weight', 'encoder.layer3.3.bn3.weight', 'encoder.layer3.3.bn3.bias', 'encoder.layer3.3.bn3.running_mean', 'encoder.layer3.3.bn3.running_var', 'encoder.layer3.3.se_module.fc1.weight', 'encoder.layer3.3.se_module.fc1.bias', 'encoder.layer3.3.se_module.fc2.weight', 'encoder.layer3.3.se_module.fc2.bias', 'encoder.layer3.4.conv1.weight', 'encoder.layer3.4.bn1.weight', 'encoder.layer3.4.bn1.bias', 'encoder.layer3.4.bn1.running_mean', 'encoder.layer3.4.bn1.running_var', 'encoder.layer3.4.conv2.weight', 'encoder.layer3.4.bn2.weight', 'encoder.layer3.4.bn2.bias', 'encoder.layer3.4.bn2.running_mean', 'encoder.layer3.4.bn2.running_var', 'encoder.layer3.4.conv3.weight', 'encoder.layer3.4.bn3.weight', 'encoder.layer3.4.bn3.bias', 'encoder.layer3.4.bn3.running_mean', 'encoder.layer3.4.bn3.running_var', 'encoder.layer3.4.se_module.fc1.weight', 'encoder.layer3.4.se_module.fc1.bias', 'encoder.layer3.4.se_module.fc2.weight', 'encoder.layer3.4.se_module.fc2.bias', 'encoder.layer3.5.conv1.weight', 'encoder.layer3.5.bn1.weight', 'encoder.layer3.5.bn1.bias', 'encoder.layer3.5.bn1.running_mean', 'encoder.layer3.5.bn1.running_var', 'encoder.layer3.5.conv2.weight', 'encoder.layer3.5.bn2.weight', 'encoder.layer3.5.bn2.bias', 'encoder.layer3.5.bn2.running_mean', 'encoder.layer3.5.bn2.running_var', 'encoder.layer3.5.conv3.weight', 'encoder.layer3.5.bn3.weight', 'encoder.layer3.5.bn3.bias', 'encoder.layer3.5.bn3.running_mean', 'encoder.layer3.5.bn3.running_var', 'encoder.layer3.5.se_module.fc1.weight', 'encoder.layer3.5.se_module.fc1.bias', 'encoder.layer3.5.se_module.fc2.weight', 'encoder.layer3.5.se_module.fc2.bias', 'encoder.layer4.0.conv1.weight', 'encoder.layer4.0.bn1.weight', 'encoder.layer4.0.bn1.bias', 'encoder.layer4.0.bn1.running_mean', 'encoder.layer4.0.bn1.running_var', 'encoder.layer4.0.conv2.weight', 'encoder.layer4.0.bn2.weight', 'encoder.layer4.0.bn2.bias', 'encoder.layer4.0.bn2.running_mean', 'encoder.layer4.0.bn2.running_var', 'encoder.layer4.0.conv3.weight', 'encoder.layer4.0.bn3.weight', 'encoder.layer4.0.bn3.bias', 'encoder.layer4.0.bn3.running_mean', 'encoder.layer4.0.bn3.running_var', 'encoder.layer4.0.se_module.fc1.weight', 'encoder.layer4.0.se_module.fc1.bias', 'encoder.layer4.0.se_module.fc2.weight', 'encoder.layer4.0.se_module.fc2.bias', 'encoder.layer4.0.downsample.0.weight', 'encoder.layer4.0.downsample.1.weight', 'encoder.layer4.0.downsample.1.bias', 'encoder.layer4.0.downsample.1.running_mean', 'encoder.layer4.0.downsample.1.running_var', 'encoder.layer4.1.conv1.weight', 'encoder.layer4.1.bn1.weight', 'encoder.layer4.1.bn1.bias', 'encoder.layer4.1.bn1.running_mean', 'encoder.layer4.1.bn1.running_var', 'encoder.layer4.1.conv2.weight', 'encoder.layer4.1.bn2.weight', 'encoder.layer4.1.bn2.bias', 'encoder.layer4.1.bn2.running_mean', 'encoder.layer4.1.bn2.running_var', 'encoder.layer4.1.conv3.weight', 'encoder.layer4.1.bn3.weight', 'encoder.layer4.1.bn3.bias', 'encoder.layer4.1.bn3.running_mean', 'encoder.layer4.1.bn3.running_var', 'encoder.layer4.1.se_module.fc1.weight', 'encoder.layer4.1.se_module.fc1.bias', 'encoder.layer4.1.se_module.fc2.weight', 'encoder.layer4.1.se_module.fc2.bias', 'encoder.layer4.2.conv1.weight', 'encoder.layer4.2.bn1.weight', 'encoder.layer4.2.bn1.bias', 'encoder.layer4.2.bn1.running_mean', 'encoder.layer4.2.bn1.running_var', 'encoder.layer4.2.conv2.weight', 'encoder.layer4.2.bn2.weight', 'encoder.layer4.2.bn2.bias', 'encoder.layer4.2.bn2.running_mean', 'encoder.layer4.2.bn2.running_var', 'encoder.layer4.2.conv3.weight', 'encoder.layer4.2.bn3.weight', 'encoder.layer4.2.bn3.bias', 'encoder.layer4.2.bn3.running_mean', 'encoder.layer4.2.bn3.running_var', 'encoder.layer4.2.se_module.fc1.weight', 'encoder.layer4.2.se_module.fc1.bias', 'encoder.layer4.2.se_module.fc2.weight', 'encoder.layer4.2.se_module.fc2.bias', 'decoder.blocks.x_0_0.conv1.0.weight', 'decoder.blocks.x_0_0.conv1.1.weight', 'decoder.blocks.x_0_0.conv1.1.bias', 'decoder.blocks.x_0_0.conv1.1.running_mean', 'decoder.blocks.x_0_0.conv1.1.running_var', 'decoder.blocks.x_0_0.conv2.0.weight', 'decoder.blocks.x_0_0.conv2.1.weight', 'decoder.blocks.x_0_0.conv2.1.bias', 'decoder.blocks.x_0_0.conv2.1.running_mean', 'decoder.blocks.x_0_0.conv2.1.running_var', 'decoder.blocks.x_0_1.conv1.0.weight', 'decoder.blocks.x_0_1.conv1.1.weight', 'decoder.blocks.x_0_1.conv1.1.bias', 'decoder.blocks.x_0_1.conv1.1.running_mean', 'decoder.blocks.x_0_1.conv1.1.running_var', 'decoder.blocks.x_0_1.conv2.0.weight', 'decoder.blocks.x_0_1.conv2.1.weight', 'decoder.blocks.x_0_1.conv2.1.bias', 'decoder.blocks.x_0_1.conv2.1.running_mean', 'decoder.blocks.x_0_1.conv2.1.running_var', 'decoder.blocks.x_1_1.conv1.0.weight', 'decoder.blocks.x_1_1.conv1.1.weight', 'decoder.blocks.x_1_1.conv1.1.bias', 'decoder.blocks.x_1_1.conv1.1.running_mean', 'decoder.blocks.x_1_1.conv1.1.running_var', 'decoder.blocks.x_1_1.conv2.0.weight', 'decoder.blocks.x_1_1.conv2.1.weight', 'decoder.blocks.x_1_1.conv2.1.bias', 'decoder.blocks.x_1_1.conv2.1.running_mean', 'decoder.blocks.x_1_1.conv2.1.running_var', 'decoder.blocks.x_0_2.conv1.0.weight', 'decoder.blocks.x_0_2.conv1.1.weight', 'decoder.blocks.x_0_2.conv1.1.bias', 'decoder.blocks.x_0_2.conv1.1.running_mean', 'decoder.blocks.x_0_2.conv1.1.running_var', 'decoder.blocks.x_0_2.conv2.0.weight', 'decoder.blocks.x_0_2.conv2.1.weight', 'decoder.blocks.x_0_2.conv2.1.bias', 'decoder.blocks.x_0_2.conv2.1.running_mean', 'decoder.blocks.x_0_2.conv2.1.running_var', 'decoder.blocks.x_1_2.conv1.0.weight', 'decoder.blocks.x_1_2.conv1.1.weight', 'decoder.blocks.x_1_2.conv1.1.bias', 'decoder.blocks.x_1_2.conv1.1.running_mean', 'decoder.blocks.x_1_2.conv1.1.running_var', 'decoder.blocks.x_1_2.conv2.0.weight', 'decoder.blocks.x_1_2.conv2.1.weight', 'decoder.blocks.x_1_2.conv2.1.bias', 'decoder.blocks.x_1_2.conv2.1.running_mean', 'decoder.blocks.x_1_2.conv2.1.running_var', 'decoder.blocks.x_2_2.conv1.0.weight', 'decoder.blocks.x_2_2.conv1.1.weight', 'decoder.blocks.x_2_2.conv1.1.bias', 'decoder.blocks.x_2_2.conv1.1.running_mean', 'decoder.blocks.x_2_2.conv1.1.running_var', 'decoder.blocks.x_2_2.conv2.0.weight', 'decoder.blocks.x_2_2.conv2.1.weight', 'decoder.blocks.x_2_2.conv2.1.bias', 'decoder.blocks.x_2_2.conv2.1.running_mean', 'decoder.blocks.x_2_2.conv2.1.running_var', 'decoder.blocks.x_0_3.conv1.0.weight', 'decoder.blocks.x_0_3.conv1.1.weight', 'decoder.blocks.x_0_3.conv1.1.bias', 'decoder.blocks.x_0_3.conv1.1.running_mean', 'decoder.blocks.x_0_3.conv1.1.running_var', 'decoder.blocks.x_0_3.conv2.0.weight', 'decoder.blocks.x_0_3.conv2.1.weight', 'decoder.blocks.x_0_3.conv2.1.bias', 'decoder.blocks.x_0_3.conv2.1.running_mean', 'decoder.blocks.x_0_3.conv2.1.running_var', 'decoder.blocks.x_1_3.conv1.0.weight', 'decoder.blocks.x_1_3.conv1.1.weight', 'decoder.blocks.x_1_3.conv1.1.bias', 'decoder.blocks.x_1_3.conv1.1.running_mean', 'decoder.blocks.x_1_3.conv1.1.running_var', 'decoder.blocks.x_1_3.conv2.0.weight', 'decoder.blocks.x_1_3.conv2.1.weight', 'decoder.blocks.x_1_3.conv2.1.bias', 'decoder.blocks.x_1_3.conv2.1.running_mean', 'decoder.blocks.x_1_3.conv2.1.running_var', 'decoder.blocks.x_2_3.conv1.0.weight', 'decoder.blocks.x_2_3.conv1.1.weight', 'decoder.blocks.x_2_3.conv1.1.bias', 'decoder.blocks.x_2_3.conv1.1.running_mean', 'decoder.blocks.x_2_3.conv1.1.running_var', 'decoder.blocks.x_2_3.conv2.0.weight', 'decoder.blocks.x_2_3.conv2.1.weight', 'decoder.blocks.x_2_3.conv2.1.bias', 'decoder.blocks.x_2_3.conv2.1.running_mean', 'decoder.blocks.x_2_3.conv2.1.running_var', 'decoder.blocks.x_3_3.conv1.0.weight', 'decoder.blocks.x_3_3.conv1.1.weight', 'decoder.blocks.x_3_3.conv1.1.bias', 'decoder.blocks.x_3_3.conv1.1.running_mean', 'decoder.blocks.x_3_3.conv1.1.running_var', 'decoder.blocks.x_3_3.conv2.0.weight', 'decoder.blocks.x_3_3.conv2.1.weight', 'decoder.blocks.x_3_3.conv2.1.bias', 'decoder.blocks.x_3_3.conv2.1.running_mean', 'decoder.blocks.x_3_3.conv2.1.running_var', 'decoder.blocks.x_0_4.conv1.0.weight', 'decoder.blocks.x_0_4.conv1.1.weight', 'decoder.blocks.x_0_4.conv1.1.bias', 'decoder.blocks.x_0_4.conv1.1.running_mean', 'decoder.blocks.x_0_4.conv1.1.running_var', 'decoder.blocks.x_0_4.conv2.0.weight', 'decoder.blocks.x_0_4.conv2.1.weight', 'decoder.blocks.x_0_4.conv2.1.bias', 'decoder.blocks.x_0_4.conv2.1.running_mean', 'decoder.blocks.x_0_4.conv2.1.running_var', 'segmentation_head.0.weight', 'segmentation_head.0.bias'], unexpected_keys=['state_dict', 'logs', 'epoch'])"]},"metadata":{},"execution_count":27}],"source":["weight = torch.load('/content/gdrive/MyDrive/sw_contest/pretrained/models/stage3/srx50-f0/checkpoints/best.pth')\n","model5 = smp.UnetPlusPlus(encoder_name=\"se_resnext50_32x4d\", encoder_weights=None, in_channels=3, classes=num_classes)\n","model5.to(device)\n","model5.load_state_dict(weight, strict=False)\n"]},{"cell_type":"markdown","metadata":{"id":"FlOIJsvVXzY_"},"source":["## Ensemble & Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ht4x3zvDktnS","outputId":"f586c14d-4e85-4ec5-ab3e-216224c48fb3"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"," 74%|███████▍  | 5612/7580 [58:43<20:11,  1.62it/s]"]}],"source":["test_dataset = SatelliteDataset(csv_file='./test.csv', transform=test_transform, infer=True)\n","test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=4)\n","\n","\n","model1.eval()\n","model2.eval()\n","model3.eval()\n","model4.eval()\n","model5.eval()\n","\n","with torch.no_grad():\n","    result = []\n","    for images in tqdm(test_dataloader):  # test_dataloader는 테스트 데이터셋을 위한 DataLoader\n","        images = images.float().to(device)\n","\n","        # 두 모델의 예측 수행\n","        masks1 = torch.sigmoid(model1(images)).cpu().numpy() # torch.sigmoid는 로짓을 확률로 변환\n","        masks1 = np.squeeze(masks1, axis=1)\n","\n","        masks2 = torch.sigmoid(model2(images)).cpu().numpy() # torch.sigmoid는 로짓을 확률로 변환\n","        masks2 = np.squeeze(masks2, axis=1)\n","\n","        masks3 = torch.sigmoid(model3(images)).cpu().numpy() # torch.sigmoid는 로짓을 확률로 변환\n","        masks3 = np.squeeze(masks3, axis=1)\n","\n","        masks4 = torch.sigmoid(model4(images)).cpu().numpy() # torch.sigmoid는 로짓을 확률로 변환\n","        masks4 = np.squeeze(masks4, axis=1)\n","\n","        masks5 = torch.sigmoid(model5(images)).cpu().numpy() # torch.sigmoid는 로짓을 확률로 변환\n","        masks5 = np.squeeze(masks5, axis=1)\n","\n","\n","        # 예측 평균화\n","        averaged_outputs = (masks1 + masks2 + masks3 + masks4 + masks5) / 2\n","\n","        # 임계값 적용하여 최종 이진 마스크 생성\n","        final_predictions = (averaged_outputs > 0.5).astype(np.uint8) # 0.5 이상이면 1, 그렇지 않으면 0\n","\n","        for i in range(len(images)):\n","          mask_rle = rle_encode(final_predictions[i])\n","          if mask_rle == '': # 예측된 건물 픽셀이 없는 경우 -1\n","              result.append(-1)\n","          else:\n","              result.append(mask_rle)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BJoytbsiizwo"},"outputs":[],"source":["submit = pd.read_csv('./sample_submission.csv')\n","submit['mask_rle'] = result\n","submit.to_csv(\"./submission/stage1-3.csv\", index=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i8JAbnJYjfwp"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"collapsed_sections":["D5y90GLVXafO","20x7C0trXd_p","qThRcQtbXhpr","8Yv1vnO0sFg4","VCmW1EPa_0D0"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}